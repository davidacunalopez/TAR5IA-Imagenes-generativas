# Checklist de VerificaciÃ³n - Proyecto II

Este documento actÃºa como memoria y trazabilidad de los cambios realizados en el proyecto segÃºn el enunciado.

---

## VerificaciÃ³n: SecciÃ³n I. OBJETIVO y II. MODELO DE DETECCIÃ“N DE ANOMALÃAS (LÃ­neas 9-19)

**Fecha de verificaciÃ³n:** 2025-01-27

### âœ… I. OBJETIVO (LÃ­neas 9-11)

**Requisito del enunciado:**
> Aplicar un experimento que permita validar la hipÃ³tesis de que al aplicar tÃ©cnicas de destilado de modelos de grandes volÃºmenes de parÃ¡metros en modelos mÃ¡s pequeÃ±os se pueden resolver tareas igual de complejas pero con modelos mÃ¡s eficientes.

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 9-22: Objetivo claramente definido
- ImplementaciÃ³n de 3 modelos (A, B, C) para validar la hipÃ³tesis:
  - **Modelo A**: CNN clasificador desde cero
  - **Modelo B**: CNN clasificador con destilaciÃ³n teacher-student
  - **Modelo C**: Autoencoder U-Net para reconstrucciÃ³n

**Notas:**
- El objetivo estÃ¡ correctamente documentado en el notebook
- La estructura del proyecto permite validar la hipÃ³tesis mediante comparaciÃ³n de modelos

---

### âœ… II. MODELO DE DETECCIÃ“N DE ANOMALÃAS (LÃ­neas 13-19)

#### 2.1. Dataset MVTec AD

**Requisito del enunciado:**
> Para el desarrollo de este proyecto debe usar el dataset propuesto en **MVTec AD â€” A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection**. Un dataset de escenarios industriales reales con diferentes tipos de anomalÃ­as en la forma de detecciÃ³n de defectos en objetos o texturas.

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 1248: DocumentaciÃ³n sobre carga del dataset MVTec AD
- LÃ­nea 1346-1474: ImplementaciÃ³n de `MVTecDataModule` (hereda de `pl.LightningDataModule`)
- ConfiguraciÃ³n en `conf/config.yaml`: Ruta del dataset configurada

**Notas:**
- El dataset estÃ¡ correctamente configurado y cargado
- Se implementa un DataModule siguiendo las mejores prÃ¡cticas de PyTorch Lightning

---

#### 2.2. SelecciÃ³n de 10 Clases

**Requisito del enunciado:**
> Seleccione **10 clases del dataset** las que usted mÃ¡s prefiera, y con este subconjunto vamos a entrenar distintos modelos para resolver un problema de detecciÃ³n de anomalÃ­as.

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 1248: Menciona "10 clases"
- ConfiguraciÃ³n en `conf/config.yaml` (lÃ­nea 10):
  ```yaml
  categories: ["bottle", "cable", "capsule", "grid", "metal_nut", "pill", "screw", "tile", "transistor", "zipper"]
  ```

**Clases seleccionadas:**
1. bottle
2. cable
3. capsule
4. grid
5. metal_nut
6. pill
7. screw
8. tile
9. transistor
10. zipper

**Notas:**
- Las 10 clases estÃ¡n correctamente configuradas
- El DataModule carga datos de todas las categorÃ­as especificadas

---

#### 2.3. Nota sobre DetecciÃ³n de AnomalÃ­as (LÃ­nea 19)

**Requisito del enunciado:**
> **Nota:** Detectar anomalÃ­as desde una reconstrucciÃ³n de datos, cuando construimos un embedding. Detectar las anomalÃ­as partiendo de la pregunta: Â¿Existen diferencias a partir de las imÃ¡genes que estamos haciendo con las originales?

**Estado:** âœ… **IMPLEMENTADO** (parcialmente documentado)

**UbicaciÃ³n en notebook:**
- LÃ­nea 4097-4117: SecciÃ³n "6. EvaluaciÃ³n de AnomalÃ­as"
- ImplementaciÃ³n de mÃ©todo `reconstruction_loss` en funciÃ³n `evaluate_anomaly_detection`:
  - LÃ­nea 4421-4440: CÃ¡lculo de reconstruction loss comparando imÃ¡genes reconstruidas vs originales
  - Se calcula: `np.mean((test_reconstructions - test_originals) ** 2, axis=(1, 2, 3))`

**MÃ©todos implementados:**
1. âœ… **Distancia de Mahalanobis**: Usando embeddings
2. âœ… **Distancia Euclidiana**: Usando embeddings
3. âœ… **Reconstruction Loss**: Comparando imÃ¡genes reconstruidas vs originales

**Notas:**
- La funcionalidad estÃ¡ implementada correctamente
- El mÃ©todo `reconstruction_loss` responde a la pregunta: "Â¿Existen diferencias entre las imÃ¡genes que estamos haciendo con las originales?"
- **Sugerencia de mejora**: PodrÃ­a aÃ±adirse una nota explÃ­cita en la documentaciÃ³n del notebook mencionando esta pregunta filosÃ³fica del profesor

---

## VerificaciÃ³n: SecciÃ³n III. MODELOS - Estructura con Hydra (LÃ­neas 21-43)

**Fecha de verificaciÃ³n:** 2025-01-27

### âœ… III.1. GestiÃ³n Modular con Hydra (LÃ­nea 23)

**Requisito del enunciado:**
> Cada modelo debe estructurar el proyecto utilizando la librerÃ­a **Hydra**(el mismo que en la tarea) para la gestiÃ³n modular de configuraciones, asegurando la correcta separaciÃ³n de hiper parÃ¡metros entre el modelo, el entrenamiento y los registros experimentales.

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 938-944: SecciÃ³n "3. ConfiguraciÃ³n con Hydra"
- LÃ­nea 1152-1171: InicializaciÃ³n de Hydra con `hydra.initialize()` y `hydra.compose()`
- LÃ­nea 296: ImportaciÃ³n de `OmegaConf` para manejo de configuraciones

**Notas:**
- Hydra estÃ¡ correctamente inicializado y configurado
- Se maneja el caso donde no existe configuraciÃ³n (valores por defecto)
- Las clases del notebook estÃ¡n registradas en el resolver de Hydra

---

### âœ… III.2. Estructura del Proyecto (LÃ­neas 25-38)

**Requisito del enunciado:**
> La estructura mÃ­nima recomendada del proyecto es la siguiente:
> ```
> conf/
> - config.yaml
> - model/
>     - vae.yaml
> - trainer/
>     - default.yaml
> - logger/
>     - wandb.yaml
> ```

**Estado:** âœ… **IMPLEMENTADO** (con variaciÃ³n aceptable)

**Estructura actual:**
```
conf/
- config.yaml âœ…
- model/
    - cnn_classifier_scratch.yaml âœ… (Modelo A)
    - cnn_classifier_distilled.yaml âœ… (Modelo B)
    - unet_autoencoder.yaml âœ… (Modelo C - equivalente a vae.yaml)
- trainer/
    - default.yaml âœ…
- logger/
    - wandb.yaml âœ…
```

**UbicaciÃ³n:**
- Directorio `conf/` existe en el proyecto
- Todos los archivos YAML requeridos estÃ¡n presentes
- El notebook crea la estructura automÃ¡ticamente si no existe (lÃ­neas 1118-1125)

**Notas:**
- âœ… El requisito menciona `vae.yaml` pero el proyecto usa `unet_autoencoder.yaml` (equivalente funcional para autoencoder)
- âœ… Se tienen 3 archivos de modelo (A, B, C) en lugar de solo uno, lo cual es correcto para este proyecto
- âœ… La estructura cumple con el requisito de separaciÃ³n modular

---

### âœ… III.3. ConfiguraciÃ³n de HiperparÃ¡metros (LÃ­neas 40-43)

**Requisito del enunciado:**
> Cada mÃ³dulo de configuraciÃ³n deberÃ¡ permitir la ejecuciÃ³n de experimentos con distintos parÃ¡metros del modelo, tales como:
> - DimensiÃ³n del espacio latente $(z)$.
> - Cantidad de Ã©pocas, tamaÃ±o de batch, o cualquier hiperparÃ¡metro que requiera.

**Estado:** âœ… **IMPLEMENTADO**

#### 3.3.1. DimensiÃ³n del Espacio Latente (z)

**UbicaciÃ³n:**
- `conf/model/unet_autoencoder.yaml` (lÃ­nea 7): `latent_dim: 128`
- `conf/model/cnn_classifier_scratch.yaml` (lÃ­nea 19): `embedding_dim: 256`
- `conf/model/cnn_classifier_distilled.yaml` (lÃ­nea 19): `embedding_dim: 256`

**Notas:**
- âœ… Configurable en archivos YAML
- âœ… Diferentes modelos pueden tener diferentes dimensiones

#### 3.3.2. Cantidad de Ã‰pocas

**UbicaciÃ³n:**
- `conf/trainer/default.yaml` (lÃ­nea 2): `max_epochs: 50`

**Notas:**
- âœ… Configurable en archivo de entrenamiento
- âœ… Permite variar entre experimentos

#### 3.3.3. TamaÃ±o de Batch

**UbicaciÃ³n:**
- `conf/config.yaml` (lÃ­nea 13): `batch_size: 32`

**Notas:**
- âœ… Configurable en configuraciÃ³n principal
- âœ… FÃ¡cil de modificar para diferentes experimentos

#### 3.3.4. Otros HiperparÃ¡metros Configurables

**En `conf/trainer/default.yaml`:**
- âœ… `learning_rate: 0.001`
- âœ… `weight_decay: 1e-5`
- âœ… `optimizer: "adam"` (adam, sgd)
- âœ… `momentum: 0.9` (para SGD)
- âœ… `scheduler`: ConfiguraciÃ³n completa (step, cosine, plateau)
- âœ… `early_stopping`: ConfiguraciÃ³n completa
- âœ… `checkpoint`: ConfiguraciÃ³n de guardado

**En `conf/model/`:**
- âœ… `conv1_channels`, `conv2_channels`, `conv3_channels` (arquitectura CNN)
- âœ… `num_classes: 10`
- âœ… `fc_hidden: 512`
- âœ… `dropout: 0.5`
- âœ… `encoder_channels`, `decoder_channels` (para U-Net)
- âœ… `embedding_dim` (para detecciÃ³n de anomalÃ­as)

**En `conf/config.yaml`:**
- âœ… `image_size: 128`
- âœ… `num_workers: 2`
- âœ… `train_split: 0.8`

**Notas:**
- âœ… Todos los hiperparÃ¡metros importantes son configurables
- âœ… La separaciÃ³n modular permite modificar parÃ¡metros sin tocar cÃ³digo
- âœ… Se pueden ejecutar mÃºltiples experimentos cambiando solo los archivos YAML

---

## VerificaciÃ³n: SecciÃ³n III.4. PyTorch Lightning (LÃ­neas 45-56)

**Fecha de verificaciÃ³n:** 2025-01-27

### âœ… III.4.1. Uso de PyTorch Lightning (LÃ­nea 47)

**Requisito del enunciado:**
> AdemÃ¡s debe utilizar **PyTorch Lightning** para las personalizaciones de los entrenamientos y creaciÃ³n de los modelos basado en las mejores prÃ¡cticas de diseÃ±o de software que permita un correcto diseÃ±o escalable.

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 290: ImportaciÃ³n de `pytorch_lightning` y callbacks
- LÃ­nea 691: `CNNClassifierLightning(pl.LightningModule)`
- LÃ­nea 838: `AutoencoderLightning(pl.LightningModule)`
- LÃ­nea 1346: `MVTecDataModule(pl.LightningDataModule)`

**Notas:**
- âœ… Todos los modelos y mÃ³dulos de datos heredan de clases Lightning
- âœ… Estructura escalable y modular implementada

---

### âœ… III.4.2. LightningDataModule (LÃ­nea 47)

**Requisito del enunciado:**
> Debe crear su propia clase de carga de datos utilizando `LightningDataModule`

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 1346-1474: ImplementaciÃ³n de `MVTecDataModule(pl.LightningDataModule)`
- MÃ©todos implementados:
  - âœ… `setup()`: Carga y prepara los datos
  - âœ… `train_dataloader()`: Retorna DataLoader de entrenamiento
  - âœ… `val_dataloader()`: Retorna DataLoader de validaciÃ³n
  - âœ… `test_dataloader()`: Retorna DataLoader de prueba

**Notas:**
- âœ… ImplementaciÃ³n completa siguiendo mejores prÃ¡cticas
- âœ… Solo usa datos 'good' para entrenamiento (lÃ­nea 1323-1330)
- âœ… Test incluye normales y anomalÃ­as (lÃ­nea 1332-1341)

---

### âœ… III.4.3. LightningModule con MÃ©todos MÃ­nimos (LÃ­nea 47-48)

**Requisito del enunciado:**
> Debe crear su propia clase de carga de datos utilizando `LightningDataModule` y su modelo utilizando `LightningModule`, acÃ¡ debe redefinir como mÃ­nimo los mÃ©todos de `training_step`, `test_step`, `configure_optimizers`.  
> **Nota:** El parrafo anterior es lo minimo que debe de estar.

**Estado:** âœ… **IMPLEMENTADO**

#### Modelo A y B: CNNClassifierLightning

**UbicaciÃ³n en notebook:**
- LÃ­nea 691: DefiniciÃ³n de clase `CNNClassifierLightning(pl.LightningModule)`
- LÃ­nea 736: âœ… `training_step(self, batch, batch_idx)` - Implementado
- LÃ­nea 768: âœ… `validation_step(self, batch, batch_idx)` - Implementado (adicional)
- LÃ­nea 779: âœ… `test_step(self, batch, batch_idx)` - Implementado
- LÃ­nea 790: âœ… `configure_optimizers(self)` - Implementado

**Notas:**
- âœ… Todos los mÃ©todos mÃ­nimos requeridos estÃ¡n implementados
- âœ… Incluye `validation_step` adicional (buena prÃ¡ctica)
- âœ… Soporta destilaciÃ³n teacher-student en `training_step` (Modelo B)

#### Modelo C: AutoencoderLightning

**UbicaciÃ³n en notebook:**
- LÃ­nea 838: DefiniciÃ³n de clase `AutoencoderLightning(pl.LightningModule)`
- LÃ­nea 869: âœ… `training_step(self, batch, batch_idx)` - Implementado
- LÃ­nea 880: âœ… `validation_step(self, batch, batch_idx)` - Implementado (adicional)
- LÃ­nea 894: âœ… `test_step(self, batch, batch_idx)` - Implementado
- LÃ­nea 907: âœ… `configure_optimizers(self)` - Implementado

**Notas:**
- âœ… Todos los mÃ©todos mÃ­nimos requeridos estÃ¡n implementados
- âœ… Incluye `validation_step` adicional (buena prÃ¡ctica)
- âœ… Soporta mÃºltiples funciones de pÃ©rdida (L1, L2, SSIM)

---

### âœ… III.4.4. Callback de EarlyStopping (LÃ­nea 50-51)

**Requisito del enunciado:**
> Adicionalmente utilice el **Callback de EarlyStopping** durante el proceso de entrenamiento para evitar Overfitting del modelo.  
> **Nota:** Este callback va a estar monitoreando el comportamiento de mis metricas para definir cuando ya no hay una mejora y detener el entrenamiento con el objetivo de no gastar recursos computacionales en entrenar un modelo que no va a mejorar.

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 290: ImportaciÃ³n de `EarlyStopping` desde `pytorch_lightning.callbacks`
- LÃ­nea 1642-1646: ConfiguraciÃ³n de `EarlyStopping`:
  ```python
  early_stopping = EarlyStopping(
      monitor="val/loss",
      mode="min",
      patience=10,
      min_delta=0.001
  )
  ```
- LÃ­nea 1666: Callback aÃ±adido al Trainer: `callbacks=[early_stopping, checkpoint_callback, lr_monitor]`
- LÃ­nea 1015: ConfiguraciÃ³n en `conf/trainer/default.yaml`:
  ```yaml
  early_stopping:
    enabled: true
    monitor: "val/loss"
    mode: "min"
    patience: 10
    min_delta: 0.001
  ```

**Notas:**
- âœ… EarlyStopping correctamente implementado y configurado
- âœ… Monitorea mÃ©trica `val/loss` para detectar mejoras
- âœ… Configurable mediante archivo YAML

---

### âœ… III.4.5. Callback de ReducciÃ³n de Learning Rate (LÃ­nea 51)

**Requisito del enunciado:**
> El profe tambien menciona que se pueden usar otros callbacks como el de reduccion del learning rate que es cuando no se ha mejorado en 5 itereaciones en el set de validacion

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 290: ImportaciÃ³n de `LearningRateMonitor` desde `pytorch_lightning.callbacks`
- LÃ­nea 1658: `LearningRateMonitor(logging_interval='step')` - Callback de monitoreo
- LÃ­nea 806-811: `ReduceLROnPlateau` scheduler implementado en `configure_optimizers`:
  ```python
  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
      optimizer,
      mode='min',
      factor=0.5,
      patience=5  # Reduce LR si no mejora en 5 iteraciones
  )
  ```
- LÃ­nea 1011-1016: ConfiguraciÃ³n en `conf/trainer/default.yaml`:
  ```yaml
  scheduler:
    name: "plateau"  # o "step", "cosine"
    patience: 5
    factor: 0.5
  ```

**Notas:**
- âœ… `LearningRateMonitor`: Callback que registra el learning rate en WandB
- âœ… `ReduceLROnPlateau`: Scheduler que reduce LR cuando no hay mejora (patience=5)
- âœ… Ambos callbacks estÃ¡n implementados y funcionando

---

### âœ… III.4.6. Entrenamiento Solo con Datos Sin Defectos (LÃ­nea 53)

**Requisito del enunciado:**
> **Importante:** Cada modelo debe de ser entrenado Ãºnicamente con datos de la clase sin defectos, no incluir clases anÃ³malas en el entrenamiento de los modelos.

**Estado:** âœ… **IMPLEMENTADO** (ya verificado en secciÃ³n II)

**UbicaciÃ³n en notebook:**
- LÃ­nea 1323-1330: En `load_dataset_paths()`, cuando `split == 'train' and only_good`:
  ```python
  if split == 'train' and only_good:
      # Solo imÃ¡genes 'good' en entrenamiento
      good_path = os.path.join(split_path, 'good')
  ```

**Notas:**
- âœ… Solo se cargan imÃ¡genes 'good' para entrenamiento
- âœ… Las anomalÃ­as solo se usan en el conjunto de prueba

---

### âš ï¸ III.4.7. Scripts Externos (LÃ­neas 55-56)

**Requisito del enunciado:**
> Como tener todo en un mismo Jupyter Notebook puede ser complicado y extenso, pueden crear la jerarquÃ­a de archivos que requieran y utilizarlas como archivos auxiliares en formato script para el diseÃ±o y control de los experimentos, sin embargo, la ejecuciÃ³n del entrenamiento debe estar en un Jupyter Notebook. Todos los archivos utilizados deben estar dentro del entregable.  
> **Nota:** Para esta parte puedes separar lo que ya hay en el notebook y crear scripsts por aparte en una carpeta llamada "scripts" para que no sea tan pesado. Documenta los los scripts utilizados de forma basica en el notebook para ver que se utiizaron externo al notebook. La ruta para usar correr en google coolab es "/content/drive/MyDrive/Colab Notebooks/Proyecto2-IA/scripts"

**Estado:** âš ï¸ **PARCIALMENTE IMPLEMENTADO**

**Observaciones:**
- âœ… El cÃ³digo estÃ¡ todo en el notebook (cumple con "la ejecuciÃ³n del entrenamiento debe estar en un Jupyter Notebook")
- âš ï¸ No existe carpeta "scripts" con archivos externos
- âš ï¸ Hay comentarios que mencionan scripts externos:
  - LÃ­nea 665: `"# MÃ³dulos Lightning - Copiamos el contenido de lightning_modules.py"`
  - LÃ­nea 1292: `"# DataModule para MVTec AD - Copiamos el contenido de data_module.py"`
- âš ï¸ No hay documentaciÃ³n explÃ­cita en el notebook sobre scripts utilizados externamente

**Notas:**
- El enfoque actual (todo en el notebook) es vÃ¡lido segÃºn el enunciado
- La nota del profesor sugiere usar scripts externos para reducir tamaÃ±o, pero no es obligatorio
- **Sugerencia opcional**: Si se desea seguir la nota del profesor, se podrÃ­an crear scripts en carpeta "scripts" y documentarlos en el notebook

---

## VerificaciÃ³n: SecciÃ³n III.A. Modelo Clasificador CNN (Scratch y DestilaciÃ³n) (LÃ­neas 58-76)

**Fecha de verificaciÃ³n:** 2025-01-27

### âœ… III.A.1. Estructura Basada en ResNet-18 (LÃ­neas 60-61)

**Requisito del enunciado:**
> Para el siguiente modelo debe de crear una estructura base siguiendo la estructura de **RESNET-18** para las primeras 3 convoluciones (`conv1`, `conv2_x`, `conv3_x`) (ver Figura 1), de acÃ¡ en adelante coloque un clasificador (FC layer) a su gusto para crear un clasificador entre las distintas clases.  
> **Nota:** Lo que quiero es que el extractor de caracteriristicas de esa red convolucional, tengan la mismas entradas de la figura(para las 3 primeras entradas conv1, conv2 y conv3), esto porque vamos a hacer dos variantes de entrenamientos explicado mas adelante. Apartir de esas 3 convoluciones podemos extender la arquitectura como nosotros queramos para hacer un mejor modelo

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 369-395: ImplementaciÃ³n de `BasicBlock` (bloques residuales de ResNet)
- LÃ­nea 396-433: ImplementaciÃ³n de `CNNClassifier` con estructura ResNet-18

#### ComparaciÃ³n detallada con Figura 1 (ResNet-18):

**conv1 (Figura 1, lÃ­nea 92-93):**
- Requerido: $7 \times 7,64$, stride 2, seguido de $3 \times 3$ max pool, stride 2
- Implementado (lÃ­nea 410-412):
  - âœ… `nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)`
  - âœ… `nn.MaxPool2d(kernel_size=3, stride=2, padding=1)`
- **Estado:** âœ… **COINCIDE EXACTAMENTE**

**conv2_x (Figura 1, lÃ­nea 94):**
- Requerido: $\left[\begin{array}{l}3 \times 3,64 \\ 3 \times 3,64\end{array}\right] \times 2$ (output $56 \times 56$)
- Implementado (lÃ­nea 415):
  - âœ… `self._make_layer(64, 64, 64, num_blocks=2, stride=1)`
  - âœ… `BasicBlock` usa: `3x3, 64` y `3x3, 64` (lÃ­neas 374-376)
  - âœ… `num_blocks=2` crea 2 bloques residuales
- **Estado:** âœ… **COINCIDE EXACTAMENTE**

**conv3_x (Figura 1, lÃ­nea 95):**
- Requerido: $\left[\begin{array}{l}3 \times 3,128 \\ 3 \times 3,128\end{array}\right] \times 2$ (output $28 \times 28$)
- Implementado (lÃ­nea 418):
  - âœ… `self._make_layer(64, 128, 128, num_blocks=2, stride=2)`
  - âœ… `BasicBlock` usa: `3x3, 128` y `3x3, 128`
  - âœ… `num_blocks=2` crea 2 bloques residuales
  - âœ… `stride=2` reduce tamaÃ±o de $56 \times 56$ a $28 \times 28$
- **Estado:** âœ… **COINCIDE EXACTAMENTE**

**BasicBlock (verificaciÃ³n de estructura):**
- LÃ­nea 374: âœ… `nn.Conv2d(..., kernel_size=3, ...)` - Primera convoluciÃ³n $3 \times 3$
- LÃ­nea 376: âœ… `nn.Conv2d(..., kernel_size=3, ...)` - Segunda convoluciÃ³n $3 \times 3$
- âœ… Skip connection implementada (lÃ­neas 382-386, 390)

**ConfiguraciÃ³n en YAML:**
- `conf/model/cnn_classifier_scratch.yaml` (lÃ­neas 9-11):
  ```yaml
  conv1_channels: 64        # âœ… Coincide con Figura 1
  conv2_channels: [64, 64]   # âœ… Coincide con Figura 1 (2 bloques de 64)
  conv3_channels: [128, 128] # âœ… Coincide con Figura 1 (2 bloques de 128)
  ```

**Notas:**
- âœ… Estructura **EXACTA** de ResNet-18 para las primeras 3 convoluciones segÃºn Figura 1
- âœ… Todos los parÃ¡metros (kernel size, stride, canales, nÃºmero de bloques) coinciden
- âœ… Clasificador FC personalizado despuÃ©s de conv3_x (como permite el enunciado)
- âœ… Arquitectura extensible manteniendo las 3 primeras convoluciones iguales

---

### âœ… III.A.2. Modelo A - Entrenado desde 0 (LÃ­neas 65-66)

**Requisito del enunciado:**
> **El modelo A** serÃ¡ entrenado desde 0, es decir al inicio tendrÃ¡ pesos colocados aleatoriamente y comenzarÃ¡ su proceso de entrenamiento.

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 403: `model_type="scratch"` en `CNNClassifier.__init__()`
- LÃ­nea 3505: `model_type="cnn_scratch"` en funciÃ³n de entrenamiento
- LÃ­nea 3440-3498: 3 configuraciones de hiperparÃ¡metros para Modelo A

**Notas:**
- âœ… Modelo A se inicializa con pesos aleatorios (comportamiento por defecto de PyTorch)
- âœ… No se usa ningÃºn modelo pre-entrenado
- âœ… Entrenamiento completamente desde cero

---

### âœ… III.A.3. Modelo B - DestilaciÃ³n Teacher-Student (LÃ­neas 67-71)

**Requisito del enunciado:**
> **El modelo B** serÃ¡ entrenado siguiendo un proceso de destilado del modelo RESNET-18 siguiendo la tÃ©cnica **teacher-student** donde el modelo RESNET-18 sirve como teacher y el modelo B como student.  
> **Nota:** Vamos a aprovechar ya entrenamiento que existen en las primeras 3 capas(conv1, conv2 y conv3 de RESNET), y utilizar la tecnica de teacher-student

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 287: ImportaciÃ³n de `resnet18` desde `torchvision.models`
- LÃ­nea 708-720: Carga de ResNet-18 pre-entrenado como teacher:
  ```python
  self.teacher_model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)
  self.teacher_model.fc = nn.Linear(..., num_classes)
  self.teacher_model.eval()
  for param in self.teacher_model.parameters():
      param.requires_grad = False
  ```
- LÃ­nea 743-759: ImplementaciÃ³n de destilaciÃ³n en `training_step`:
  - LÃ­nea 748-749: ExtracciÃ³n de logits del teacher
  - LÃ­nea 751-752: Softmax con temperatura
  - LÃ­nea 755-756: PÃ©rdida de destilaciÃ³n (KL divergence)
  - LÃ­nea 759: CombinaciÃ³n de pÃ©rdidas: `alpha * distillation_loss + (1-alpha) * classification_loss`
- LÃ­nea 3606-3668: 3 configuraciones de hiperparÃ¡metros para Modelo B con destilaciÃ³n

**ConfiguraciÃ³n de destilaciÃ³n:**
- `conf/model/cnn_classifier_distilled.yaml` (lÃ­neas 22-25):
  ```yaml
  distillation:
    teacher_model: "resnet18"
    temperature: 4.0
    alpha: 0.7
  ```

**Notas:**
- âœ… ResNet-18 pre-entrenado en ImageNet se usa como teacher
- âœ… Teacher se congela (no se entrena)
- âœ… DestilaciÃ³n implementada con temperatura y alpha
- âœ… Las primeras 3 capas del teacher (conv1, conv2, conv3) estÃ¡n pre-entrenadas y se aprovechan mediante la tÃ©cnica teacher-student

---

### âœ… III.A.4. ExtracciÃ³n de Embeddings (LÃ­nea 73)

**Requisito del enunciado:**
> **Importante:** Es importante un buen diseÃ±o de modelo que permita obtener el vector de embeddings de salida de las capas convolucionales. Pues son los que luego permitirÃ¡n diseÃ±ar el detector de anomalÃ­as.

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 432: `self.embedding_layer = nn.Linear(conv3_channels[-1], embedding_dim)`
- LÃ­nea 458-459: ExtracciÃ³n de embeddings en `forward()`:
  ```python
  embedding = self.embedding_layer(x)  # x viene de conv3_x
  ```
- LÃ­nea 466-477: MÃ©todo `get_embedding()` para extraer solo embeddings:
  ```python
  def get_embedding(self, x):
      # Pasa por conv1, conv2_x, conv3_x
      # Extrae embedding de la salida de conv3_x
      embedding = self.embedding_layer(x)
      return embedding
  ```

**Notas:**
- âœ… Embeddings extraÃ­dos de la salida de las capas convolucionales (despuÃ©s de conv3_x)
- âœ… MÃ©todo `get_embedding()` implementado para facilitar extracciÃ³n
- âœ… Embeddings usados para detecciÃ³n de anomalÃ­as (ver secciÃ³n IV)

---

### âœ… III.A.5. 3 HiperparÃ¡metros Distintos por Modelo (LÃ­neas 75-76)

**Requisito del enunciado:**
> Cada modelo debe ser entrenado al menos con **3 hiperparÃ¡metros distintos** para obtener buenos modelos y no solamente la primera combinaciÃ³n que obtengan.  
> **Nota:** Tenemos 3 distintos entrenamientos por cada modelo(A, B C), en total 9 entrenamientos.

**Estado:** âœ… **IMPLEMENTADO**

#### Modelo A - 3 Configuraciones

**UbicaciÃ³n en notebook:**
- LÃ­nea 3439-3498: DefiniciÃ³n de 3 configuraciones (`model_a_configs`)
- LÃ­nea 3500-3521: Entrenamiento de las 3 configuraciones
- Variaciones en hiperparÃ¡metros:
  - **Config 1**: `fc_hidden=512`, `dropout=0.5`, `embedding_dim=256`, `lr=0.001`, scheduler `step`
  - **Config 2**: `fc_hidden=256`, `dropout=0.3`, `embedding_dim=128`, `lr=0.0005`, scheduler `cosine`
  - **Config 3**: `fc_hidden=1024`, `dropout=0.7`, `embedding_dim=512`, `lr=0.002`, scheduler `plateau`

#### Modelo B - 3 Configuraciones

**UbicaciÃ³n en notebook:**
- LÃ­nea 3606-3668: DefiniciÃ³n de 3 configuraciones (`model_b_configs`)
- LÃ­nea 3670-3744: Entrenamiento de las 3 configuraciones
- Variaciones en hiperparÃ¡metros:
  - **Config 1**: Misma estructura que A, `temperature=4.0`, `alpha=0.7`
  - **Config 2**: Misma estructura que A, `temperature=5.0`, `alpha=0.8`
  - **Config 3**: Misma estructura que A, `temperature=3.0`, `alpha=0.6`
  - AdemÃ¡s incluye variaciones en parÃ¡metros de destilaciÃ³n

#### Modelo C - 3 Configuraciones

**UbicaciÃ³n en notebook:**
- LÃ­nea 4009-4088: DefiniciÃ³n de 3 configuraciones (`model_c_configs`)
- LÃ­nea 4068-4088: Entrenamiento de las 3 configuraciones
- Variaciones en hiperparÃ¡metros:
  - **Config 1**: `latent_dim=128`, `embedding_dim=128`, loss `L2`
  - **Config 2**: `latent_dim=256`, `embedding_dim=256`, loss `SSIM_L1`
  - **Config 3**: (verificar en notebook)

**Resumen:**
- âœ… Modelo A: 3 configuraciones entrenadas
- âœ… Modelo B: 3 configuraciones entrenadas
- âœ… Modelo C: 3 configuraciones entrenadas
- âœ… **Total: 9 entrenamientos** (cumple con el requisito)

**Notas:**
- âœ… Cada modelo tiene al menos 3 configuraciones distintas
- âœ… Las configuraciones varÃ­an hiperparÃ¡metros importantes (learning rate, dropout, embedding_dim, etc.)
- âœ… Para Modelo B, tambiÃ©n varÃ­an parÃ¡metros de destilaciÃ³n (temperature, alpha)

---

## VerificaciÃ³n: SecciÃ³n III.B. Modelo C - Embedding de un Autoencoder (LÃ­neas 78-84)

**Fecha de verificaciÃ³n:** 2025-01-27

### âœ… III.B.1. Autoencoder Basado en U-Net (LÃ­neas 80, 84)

**Requisito del enunciado:**
> DiseÃ±e un modelo de autoencoder basado en **U-Net** que reconstruya las imÃ¡genes del set de entrenamiento seleccionado y tambiÃ©n permita obtener el embedding correspondiente.  
> **Nota:** Recordar que buscamos probar diferentes arquitecturas que me construyan embbedins y hacer comparaciones. Arquitectura A es un CNN tradicional entrenado desde 0. Modelo B es el mismo CNN pero aplicado con un proceso de destilado desde el modelo RESNET. Y el modelo C va a ser un autoecoder, vamos a reconstruir la imagen. Este autoencoder esta basado en el modelo U-Net(esto lo podemos ver con la Tarea 5 ya realizada)

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 480-618: ImplementaciÃ³n de `UNetAutoencoder(nn.Module)`
- LÃ­nea 482: DocumentaciÃ³n: "Autoencoder U-Net con skip connections (Modelo C)"
- LÃ­nea 483: Nota: "Reutilizado de Tarea05"

#### Estructura U-Net:

**Encoder (LÃ­neas 497-509):**
- âœ… Bloques de encoder con convoluciones `4x4, stride=2`
- âœ… Canales: `[64, 128, 256, 512]` (configurable)
- âœ… BatchNorm y ReLU despuÃ©s de cada convoluciÃ³n

**Bottleneck (LÃ­neas 511-515):**
- âœ… Capa bottleneck que reduce a `latent_dim`

**Decoder con Skip Connections (LÃ­neas 517-546):**
- âœ… Bloques de decoder con transposed convoluciones
- âœ… **Skip connections implementadas** (lÃ­neas 575-593):
  - LÃ­nea 567-570: Guarda skip connections durante encoding
  - LÃ­nea 579-592: Usa skip connections durante decoding con `torch.cat([x, skip], dim=1)`
  - LÃ­nea 595-608: Usa skip connection en capa final
- âœ… Canales: `[512, 256, 128, 64]` (configurable)
- âœ… Capa final con `Tanh()` para normalizar salida

**Notas:**
- âœ… Skip connections correctamente implementadas (caracterÃ­stica clave de U-Net)
- âœ… Similar a implementaciÃ³n de Tarea 5 (como menciona la nota)
- âœ… Arquitectura permite reconstrucciÃ³n de imÃ¡genes

---

### âœ… III.B.2. ReconstrucciÃ³n de ImÃ¡genes (LÃ­nea 80)

**Requisito del enunciado:**
> que reconstruya las imÃ¡genes del set de entrenamiento seleccionado

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 565-610: MÃ©todo `forward()` que reconstruye imÃ¡genes:
  ```python
  def forward(self, x):
      # Encoder
      # Bottleneck
      # Decoder con skip connections
      x = self.final_layer(x)  # ReconstrucciÃ³n
      return x
  ```
- LÃ­nea 869-877: `training_step()` en `AutoencoderLightning`:
  ```python
  x_recon = self(x)  # ReconstrucciÃ³n
  loss = self.criterion(x_recon, x)  # Compara reconstrucciÃ³n vs original
  ```
- LÃ­nea 880-891: `validation_step()` tambiÃ©n reconstruye y calcula pÃ©rdida
- LÃ­nea 894-905: `test_step()` reconstruye y extrae embeddings

**Notas:**
- âœ… El modelo reconstruye imÃ¡genes de entrada
- âœ… La pÃ©rdida se calcula comparando reconstrucciÃ³n vs original
- âœ… Soporta mÃºltiples funciones de pÃ©rdida: L1, L2, SSIM, SSIM_L1

---

### âœ… III.B.3. ExtracciÃ³n de Embeddings (LÃ­nea 80)

**Requisito del enunciado:**
> y tambiÃ©n permita obtener el embedding correspondiente

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 548-553: Capa de embeddings:
  ```python
  self.embedding_layer = nn.Sequential(
      nn.AdaptiveAvgPool2d((1, 1)),
      nn.Flatten(),
      nn.Linear(latent_dim, embedding_dim)
  )
  ```
- LÃ­nea 555-563: MÃ©todo `encode()` que extrae el vector latente
- LÃ­nea 612-616: MÃ©todo `get_embedding()`:
  ```python
  def get_embedding(self, x):
      latent, _ = self.encode(x)
      embedding = self.embedding_layer(latent)
      return embedding
  ```
- LÃ­nea 900: Uso en `test_step()`: `embeddings = self.model.get_embedding(x)`

**Notas:**
- âœ… Embeddings extraÃ­dos del espacio latente (bottleneck)
- âœ… MÃ©todo `get_embedding()` implementado para facilitar extracciÃ³n
- âœ… Embeddings usados para detecciÃ³n de anomalÃ­as (ver secciÃ³n IV)

---

### âœ… III.B.4. Entrenamiento desde 0 (LÃ­nea 82)

**Requisito del enunciado:**
> Este serÃ¡ entrenado completamente desde 0.

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 1615: CreaciÃ³n del modelo: `base_model = UNetAutoencoder(**model_config)`
- No hay carga de pesos pre-entrenados
- No hay uso de modelos pre-entrenados como base
- InicializaciÃ³n con pesos aleatorios (comportamiento por defecto de PyTorch)

**Notas:**
- âœ… Modelo se inicializa desde cero
- âœ… No se usa ningÃºn modelo pre-entrenado
- âœ… Entrenamiento completamente desde 0 (a diferencia del Modelo B que usa destilaciÃ³n)

---

### âœ… III.B.5. ComparaciÃ³n con Otros Modelos (LÃ­nea 84 - Nota)

**Requisito del enunciado:**
> **Nota:** Recordar que buscamos probar diferentes arquitecturas que me construyan embbedins y hacer comparaciones. Arquitectura A es un CNN tradicional entrenado desde 0. Modelo B es el mismo CNN pero aplicado con un proceso de destilado desde el modelo RESNET. Y el modelo C va a ser un autoecoder, vamos a reconstruir la imagen.

**Estado:** âœ… **IMPLEMENTADO**

**ComparaciÃ³n de arquitecturas:**

| Modelo | Arquitectura | Entrenamiento | Embeddings | PropÃ³sito |
|--------|--------------|---------------|------------|-----------|
| **A** | CNN (ResNet-18 primeras 3 conv) | Desde 0 | De capas convolucionales | ClasificaciÃ³n |
| **B** | CNN (ResNet-18 primeras 3 conv) | DestilaciÃ³n teacher-student | De capas convolucionales | ClasificaciÃ³n |
| **C** | U-Net Autoencoder | Desde 0 | Del espacio latente | ReconstrucciÃ³n |

**Notas:**
- âœ… Tres arquitecturas diferentes para construir embeddings
- âœ… Permite comparar diferentes enfoques para detecciÃ³n de anomalÃ­as
- âœ… Modelo C se enfoca en reconstrucciÃ³n (diferente a A y B que son clasificadores)

---

### âœ… III.B.6. 3 Configuraciones de HiperparÃ¡metros (Ya verificado en III.A.5)

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 4009-4088: 3 configuraciones para Modelo C (`model_c_configs`)
- Variaciones en hiperparÃ¡metros:
  - **Config 1**: `latent_dim=128`, `embedding_dim=128`, loss `L2`, `encoder_channels=[64, 128, 256, 512]`
  - **Config 2**: `latent_dim=256`, `embedding_dim=256`, loss `SSIM_L1`, `encoder_channels=[64, 128, 256, 512]`
  - **Config 3**: `latent_dim=64`, `embedding_dim=64`, loss `L1`, `encoder_channels=[32, 64, 128, 256]` (arquitectura mÃ¡s pequeÃ±a)

**Notas:**
- âœ… Ya verificado en secciÃ³n III.A.5
- âœ… Total de 9 entrenamientos (3 por cada modelo A, B, C)

---

## VerificaciÃ³n: SecciÃ³n IV. EVALUACIÃ“N DE ANOMALÃAS (LÃ­neas 103-136)

**Fecha de verificaciÃ³n:** 2025-01-27

### âœ… IV.1. CÃ¡lculo de Representaciones Latentes (LÃ­neas 105-106)

**Requisito del enunciado:**
> Una vez entrenados los modelos, se deben calcular las representaciones latentes (embeddings) de las imÃ¡genes del conjunto de validaciÃ³n para estimar una mÃ©trica que permita, posteriormente, identificar los datos anÃ³malos en el conjunto de prueba.  
> **Nota:** Tomar datos de validacion y apartir de ahi definir una metrica con lo cual vamos a ver que es una anomalia y que no es una anomalia. Realizar la deteccion de anomalias apartir de los embbedings.

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 4097-4117: SecciÃ³n "6. EvaluaciÃ³n de AnomalÃ­as"
- LÃ­nea 4206-4330: FunciÃ³n `extract_embeddings()` que extrae embeddings de un dataloader
- LÃ­nea 4367-4374: ExtracciÃ³n de embeddings del conjunto normal (validaciÃ³n/entrenamiento)
- LÃ­nea 4382-4385: ExtracciÃ³n de embeddings del conjunto de prueba

**Notas:**
- âœ… Embeddings extraÃ­dos del conjunto de validaciÃ³n/entrenamiento (solo datos normales)
- âœ… Embeddings extraÃ­dos del conjunto de prueba (normales y anÃ³malos)
- âœ… Funciona para todos los modelos (A, B, C)

---

### âœ… IV.2. EstimaciÃ³n de la DistribuciÃ³n Normal (LÃ­neas 114-125)

**Requisito del enunciado:**
> A partir del conjunto de validaciÃ³n o entrenamiento correspondiente a la clase sin defectos, se extraen los embeddings de cada imagen mediante el modelo previamente entrenado (para cada modelo A, B y C), ya sea del set de validacion o de entrenamiento para las clases buenas.  
> Cada embedding puede representarse como un vector $\mathbf{z}_{i} \in \mathbb{R}^{d}$. Con todos los embeddings del conjunto normal se calcula la media $\boldsymbol{\mu}$ y la matriz de covarianza $\boldsymbol{\Sigma}$:  
> $$
> \boldsymbol{\mu}=\frac{1}{N} \sum_{i=1}^{N} \mathbf{z}_{i}, \quad \boldsymbol{\Sigma}=\frac{1}{N-1} \sum_{i=1}^{N}\left(\mathbf{z}_{i}-\boldsymbol{\mu}\right)\left(\mathbf{z}_{i}-\boldsymbol{\mu}\right)^{T}
> $$  
> De esta forma se modela la distribuciÃ³n normal como una distribuciÃ³n gaussiana multivariada $\mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$, que representa los datos normales en el espacio de embeddings.

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 4295-4330: FunciÃ³n `estimate_normal_distribution()`:
  ```python
  # Media: Î¼ = (1/N) Î£ z_i
  mean = np.mean(normal_embeddings, axis=0)
  
  # Matriz de covarianza: Î£ = (1/(N-1)) Î£ (z_i - Î¼)(z_i - Î¼)^T
  cov = np.cov(normal_embeddings.T)  # np.cov usa (N-1) como denominador
  ```
- LÃ­nea 4376-4379: Uso en `evaluate_anomaly_detection()`:
  ```python
  mean, cov = estimate_normal_distribution(normal_embeddings)
  ```

**VerificaciÃ³n de fÃ³rmulas:**
- âœ… Media: `np.mean(normal_embeddings, axis=0)` = $\frac{1}{N} \sum_{i=1}^{N} \mathbf{z}_{i}$ âœ…
- âœ… Covarianza: `np.cov(normal_embeddings.T)` = $\frac{1}{N-1} \sum_{i=1}^{N}\left(\mathbf{z}_{i}-\boldsymbol{\mu}\right)\left(\mathbf{z}_{i}-\boldsymbol{\mu}\right)^{T}$ âœ…
- âœ… Modela distribuciÃ³n gaussiana multivariada $\mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ âœ…

**Notas:**
- âœ… FÃ³rmulas implementadas exactamente como en el enunciado
- âœ… ValidaciÃ³n de que embeddings tienen shape (N, d) donde d es la dimensiÃ³n
- âœ… ValidaciÃ³n de que hay al menos 2 muestras para calcular covarianza

---

### âœ… IV.3. CÃ¡lculo de la Distancia de Mahalanobis (LÃ­neas 127-131)

**Requisito del enunciado:**
> Para una nueva muestra con embedding $\mathbf{z}_{\text{test}}$, se calcula su distancia a la distribuciÃ³n normal.  
> Esta distancia mide quÃ© tan alejada se encuentra la muestra del centro de la distribuciÃ³n de los datos sin defectos, considerando la forma y correlaciones de dicha distribuciÃ³n.

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 4155-4203: FunciÃ³n `calculate_mahalanobis_distance()`:
  ```python
  def calculate_mahalanobis_distance(embeddings, mean, cov):
      """
      Distancia de Mahalanobis: d = sqrt((z - Î¼)^T Î£^(-1) (z - Î¼))
      """
      # RegularizaciÃ³n para evitar singularidad
      cov_reg = cov + np.eye(cov.shape[0]) * 1e-6
      cov_inv = inv(cov_reg)
      
      # Calcular distancias
      for emb in embeddings:
          diff = emb - mean
          dist = np.sqrt(diff @ cov_inv @ diff.T)
  ```
- LÃ­nea 4397-4406: Uso en evaluaciÃ³n:
  ```python
  if method == "mahalanobis":
      test_normal_distances = calculate_mahalanobis_distance(test_normal_embeddings, mean, cov)
      test_anomaly_distances = calculate_mahalanobis_distance(test_anomaly_embeddings, mean, cov)
  ```

**VerificaciÃ³n de fÃ³rmula:**
- âœ… FÃ³rmula implementada: $d = \sqrt{(\mathbf{z} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{z} - \boldsymbol{\mu})}$ âœ…
- âœ… RegularizaciÃ³n aÃ±adida para evitar singularidad de la matriz de covarianza
- âœ… Manejo de errores para distancias invÃ¡lidas (NaN, Inf)

**Notas:**
- âœ… ImplementaciÃ³n correcta de la distancia de Mahalanobis
- âœ… Considera correlaciones entre dimensiones (mediante matriz de covarianza)
- âœ… Calcula distancia para cada embedding del conjunto de prueba

---

### âœ… IV.4. ClasificaciÃ³n usando Percentiles (LÃ­nea 133)

**Requisito del enunciado:**
> A partir de acÃ¡ debe de averiguar como clasificar una anomalÃ­a o una clase sin defectos utilizando comparaciÃ³n de la distancia (e.g tomar el percentil).

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 4444-4464: CÃ¡lculo de umbral usando percentil:
  ```python
  # Determinar umbral usando percentil de las distancias normales del conjunto de validaciÃ³n
  validation_normal_distances = calculate_mahalanobis_distance(normal_embeddings, mean, cov)
  threshold = np.percentile(validation_normal_distances, percentile)
  print(f"ðŸ“ Umbral calculado (percentil {percentile}): {threshold:.4f}")
  ```
- LÃ­nea 4469-4474: ClasificaciÃ³n:
  ```python
  all_distances = np.concatenate([test_normal_distances, test_anomaly_distances])
  predictions = (all_distances > threshold).astype(int)  # 1 = anomalÃ­a, 0 = normal
  true_labels = np.concatenate([np.zeros_like(test_normal_distances), np.ones_like(test_anomaly_distances)])
  ```

**ConfiguraciÃ³n:**
- LÃ­nea 1064: `percentile_threshold: 95` en `conf/config.yaml`
- LÃ­nea 4349: ParÃ¡metro `percentile=95` por defecto en `evaluate_anomaly_detection()`

**Notas:**
- âœ… Umbral calculado usando percentil de distancias normales de validaciÃ³n
- âœ… ClasificaciÃ³n: distancias > umbral = anomalÃ­a, distancias â‰¤ umbral = normal
- âœ… Percentil configurable (default: 95)
- âœ… MÃ©tricas calculadas: AUC-ROC, AUC-PR

---

### âœ… IV.5. Otras Estrategias de DetecciÃ³n (LÃ­nea 135)

**Requisito del enunciado:**
> **Nota:** El estudiante puede implementar tambiÃ©n otras estrategias de detecciÃ³n, como la distancia euclidiana, reconstrucciÃ³n basada en error (reconstruction loss). Debe justificarse la implementada en el notebook

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 4361: MÃ©todos soportados: `["mahalanobis", "euclidean", "reconstruction_loss"]`

#### 1. Distancia Euclidiana

**UbicaciÃ³n:**
- LÃ­nea 4409-4419: ImplementaciÃ³n:
  ```python
  elif method == "euclidean":
      # Distancia euclidiana: d = ||z - Î¼||
      test_normal_distances = np.linalg.norm(test_normal_embeddings - mean, axis=1)
      test_anomaly_distances = np.linalg.norm(test_anomaly_embeddings - mean, axis=1)
  ```

**Notas:**
- âœ… Implementada: $d = ||\mathbf{z} - \boldsymbol{\mu}||$
- âœ… MÃ©trica mÃ¡s simple que Mahalanobis (no considera correlaciones)

#### 2. Reconstruction Loss

**UbicaciÃ³n:**
- LÃ­nea 4421-4440: ImplementaciÃ³n:
  ```python
  elif method == "reconstruction_loss":
      # Error de reconstrucciÃ³n: MSE entre reconstrucciÃ³n y original
      test_normal_distances = np.mean((test_reconstructions - test_originals) ** 2, axis=(1, 2, 3))
      test_anomaly_distances = np.mean((test_anomaly_recon - test_anomaly_orig) ** 2, axis=(1, 2, 3))
  ```

**Notas:**
- âœ… Implementada para autoencoders (Modelo C)
- âœ… Compara imÃ¡genes reconstruidas vs originales
- âœ… Responde a la pregunta: "Â¿Existen diferencias entre las imÃ¡genes que estamos haciendo con las originales?"

**JustificaciÃ³n en notebook:**
- LÃ­nea 4114-4117: DocumentaciÃ³n de mÃ©todos:
  ```markdown
  **MÃ©todos de evaluaciÃ³n**:
  - **Distancia de Mahalanobis**: d = sqrt((z - Î¼)^T Î£^(-1) (z - Î¼))
  - **Distancia Euclidiana**: d = ||z - Î¼||
  - **Reconstruction Loss**: Error de reconstrucciÃ³n para autoencoders
  ```

**Notas:**
- âœ… Tres mÃ©todos implementados: Mahalanobis, Euclidiana, Reconstruction Loss
- âœ… Cada mÃ©todo tiene su justificaciÃ³n y uso apropiado
- âœ… Mahalanobis: Considera correlaciones (mÃ¡s robusto)
- âœ… Euclidiana: MÃ©trica simple y rÃ¡pida
- âœ… Reconstruction Loss: EspecÃ­fico para autoencoders

---

## VerificaciÃ³n: SecciÃ³n V. MODELOS CUANTIZADOS (LÃ­neas 137-139)

**Fecha de verificaciÃ³n:** 2025-01-27

### âœ… V.1. SelecciÃ³n de los 3 Mejores Modelos (LÃ­nea 139)

**Requisito del enunciado:**
> Para esto, convierta los **tres modelos con mejores resultados** de acuerdo a su criterio a modelos cuantizados

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 4720-4728: SelecciÃ³n de los 3 mejores modelos:
  ```python
  # Ordenar por AUC-ROC (usar el mejor mÃ©todo para cada modelo)
  sorted_results = sorted(
      all_evaluation_results,
      key=lambda x: max(x.get("auc_roc", 0), x.get("auc_roc_mah", 0), x.get("auc_roc_recon", 0)),
      reverse=True
  )
  best_3_models = sorted_results[:3]
  ```
- LÃ­nea 4540: DocumentaciÃ³n: "Los mejores modelos se seleccionan segÃºn AUC-ROC para cuantizaciÃ³n y anÃ¡lisis DBSCAN"
- LÃ­nea 4730-4733: VisualizaciÃ³n de los top 3 modelos

**Criterio de selecciÃ³n:**
- âœ… SelecciÃ³n basada en AUC-ROC (mÃ©trica de rendimiento)
- âœ… Considera el mejor mÃ©todo de evaluaciÃ³n para cada modelo (Mahalanobis, Euclidiana, Reconstruction Loss)
- âœ… Ordena de mayor a menor AUC-ROC y toma los primeros 3

**Notas:**
- âœ… Criterio claro y justificado (AUC-ROC como mÃ©trica principal)
- âœ… Permite comparar modelos de diferentes tipos (A, B, C)

---

### âœ… V.2. ConversiÃ³n a Modelos Cuantizados (LÃ­nea 139)

**Requisito del enunciado:**
> convierta los **tres modelos con mejores resultados** de acuerdo a su criterio a modelos cuantizados

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 4800-4818: FunciÃ³n `quantize_model()`:
  ```python
  def quantize_model(model, method="dynamic"):
      """
      Cuantiza un modelo PyTorch
      """
      model.eval()
      if method == "dynamic":
          quantized_model = torch.quantization.quantize_dynamic(
              model, {torch.nn.Linear, torch.nn.Conv2d}, dtype=torch.qint8
          )
  ```
- LÃ­nea 4838-5040: Proceso de cuantizaciÃ³n de los 3 mejores modelos:
  ```python
  for i, best_model_info in enumerate(best_3_models, 1):
      # Extraer modelo base
      # Cuantizar modelo
      quantized_model = quantize_model(model_to_quantize, method="dynamic")
  ```

**MÃ©todo de cuantizaciÃ³n:**
- âœ… CuantizaciÃ³n dinÃ¡mica de PyTorch (`torch.quantization.quantize_dynamic`)
- âœ… Cuantiza capas `Linear` y `Conv2d` a `qint8` (int8)
- âœ… Reduce precisiÃ³n de float32 a int8

**Notas:**
- âœ… ConversiÃ³n implementada correctamente
- âœ… Modelos se ponen en modo evaluaciÃ³n antes de cuantizar
- âœ… Soporta cuantizaciÃ³n dinÃ¡mica (mÃ©todo mÃ¡s comÃºn)

---

### âœ… V.3. ComparaciÃ³n de TamaÃ±o (LÃ­nea 139)

**Requisito del enunciado:**
> y realice una comparaciÃ³n de latencias en respuesta, tamaÃ±o, y rendimiento

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 4821-4835: FunciÃ³n `compare_model_sizes()`:
  ```python
  def compare_model_sizes(original_model, quantized_model):
      """Compara el tamaÃ±o de modelos original y cuantizado"""
      def get_model_size(model):
          param_size = sum(p.numel() * p.element_size() for p in model.parameters())
          buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())
          return param_size + buffer_size
      
      original_size = get_model_size(original_model)
      quantized_size = get_model_size(quantized_model)
      
      return {
          'original_size_mb': original_size / (1024 * 1024),
          'quantized_size_mb': quantized_size / (1024 * 1024),
          'compression_ratio': original_size / quantized_size if quantized_size > 0 else 0
      }
  ```
- LÃ­nea 4878-4879: Uso en comparaciÃ³n:
  ```python
  size_comparison = compare_model_sizes(model_to_quantize, quantized_model)
  ```
- LÃ­nea 5003-5005: Almacenamiento en resultados:
  ```python
  "original_size_mb": size_comparison['original_size_mb'],
  "quantized_size_mb": size_comparison['quantized_size_mb'],
  "compression_ratio": size_comparison['compression_ratio']
  ```
- LÃ­nea 5020-5023: VisualizaciÃ³n:
  ```python
  print(f"  TamaÃ±o:")
  print(f"    Original: {size_comparison['original_size_mb']:.2f} MB")
  print(f"    Cuantizado: {size_comparison['quantized_size_mb']:.2f} MB")
  print(f"    CompresiÃ³n: {size_comparison['compression_ratio']:.2f}x")
  ```

**MÃ©tricas de tamaÃ±o:**
- âœ… TamaÃ±o original en MB
- âœ… TamaÃ±o cuantizado en MB
- âœ… Ratio de compresiÃ³n (cuÃ¡ntas veces mÃ¡s pequeÃ±o es el modelo cuantizado)

**Notas:**
- âœ… ComparaciÃ³n de tamaÃ±o implementada correctamente
- âœ… Calcula tamaÃ±o considerando parÃ¡metros y buffers
- âœ… Muestra ratio de compresiÃ³n para evaluar eficiencia

---

### âœ… V.4. ComparaciÃ³n de Latencia en Respuesta (LÃ­nea 139)

**Requisito del enunciado:**
> y realice una comparaciÃ³n de latencias en respuesta, tamaÃ±o, y rendimiento

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 4881-4917: MediciÃ³n de latencia:
  ```python
  # Latencia original - promedio sobre 100 iteraciones
  latencies_original = []
  with torch.no_grad():
      for _ in range(100):
          start_time = time.time()
          if hasattr(model_to_quantize, 'get_embedding'):
              _ = model_to_quantize.get_embedding(test_images)
          else:
              _ = model_to_quantize(test_images)
          latencies_original.append((time.time() - start_time) * 1000)  # ms
  original_latency = np.mean(latencies_original)
  
  # Latencia cuantizado - promedio sobre 100 iteraciones
  latencies_quantized = []
  with torch.no_grad():
      for _ in range(100):
          start_time = time.time()
          if hasattr(quantized_model, 'get_embedding'):
              _ = quantized_model.get_embedding(test_images)
          else:
              _ = quantized_model(test_images)
          latencies_quantized.append((time.time() - start_time) * 1000)  # ms
  quantized_latency = np.mean(latencies_quantized)
  ```
- LÃ­nea 5006-5007: Almacenamiento:
  ```python
  "original_latency_ms": original_latency,
  "quantized_latency_ms": quantized_latency,
  "speedup": original_latency / quantized_latency if quantized_latency > 0 else 0
  ```
- LÃ­nea 5025-5028: VisualizaciÃ³n:
  ```python
  print(f"  Latencia (promedio sobre 100 iteraciones):")
  print(f"    Original: {original_latency:.2f} ms")
  print(f"    Cuantizado: {quantized_latency:.2f} ms")
  print(f"    Speedup: {original_latency / quantized_latency if quantized_latency > 0 else 0:.2f}x")
  ```

**MÃ©tricas de latencia:**
- âœ… Latencia original (ms) - promedio sobre 100 iteraciones
- âœ… Latencia cuantizada (ms) - promedio sobre 100 iteraciones
- âœ… Speedup (cuÃ¡ntas veces mÃ¡s rÃ¡pido es el modelo cuantizado)

**Notas:**
- âœ… MediciÃ³n de latencia implementada correctamente
- âœ… Promedio sobre 100 iteraciones para mayor precisiÃ³n
- âœ… Calcula speedup para evaluar mejora en velocidad
- âœ… Mide tiempo de inferencia (extracciÃ³n de embeddings)

---

### âœ… V.5. ComparaciÃ³n de Rendimiento (LÃ­nea 139)

**Requisito del enunciado:**
> y realice una comparaciÃ³n de latencias en respuesta, tamaÃ±o, y rendimiento

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 4940-4992: EvaluaciÃ³n de rendimiento:
  ```python
  # Evaluar modelo original
  original_performance = {
      'auc_roc': result.get('auc_roc', 0),
      'auc_pr': result.get('auc_pr', 0)
  }
  
  # Evaluar modelo cuantizado
  eval_quantized = evaluate_anomaly_detection(
      model=quantized_lightning,
      normal_dataloader=data_module.val_dataloader(),
      test_dataloader=data_module.test_dataloader(),
      device=device,
      method="mahalanobis",
      percentile=95
  )
  quantized_performance = {
      'auc_roc': eval_quantized['auc_roc'],
      'auc_pr': eval_quantized['auc_pr']
  }
  
  # Calcular diferencia de rendimiento
  performance_diff_auc_roc = original_performance['auc_roc'] - quantized_performance['auc_roc']
  performance_retention_auc_roc = (quantized_performance['auc_roc'] / original_performance['auc_roc'] * 100) if original_performance['auc_roc'] > 0 else 0
  ```
- LÃ­nea 5008-5015: Almacenamiento:
  ```python
  "original_auc_roc": original_performance['auc_roc'],
  "quantized_auc_roc": quantized_performance['auc_roc'],
  "original_auc_pr": original_performance['auc_pr'],
  "quantized_auc_pr": quantized_performance['auc_pr'],
  "performance_diff_auc_roc": performance_diff_auc_roc,
  "performance_diff_auc_pr": performance_diff_auc_pr,
  "performance_retention_auc_roc": performance_retention_auc_roc,
  "performance_retention_auc_pr": performance_retention_auc_pr
  ```
- LÃ­nea 5030-5040: VisualizaciÃ³n:
  ```python
  print(f"  Rendimiento (AUC-ROC):")
  print(f"    Original: {original_performance['auc_roc']:.4f}")
  print(f"    Cuantizado: {quantized_performance['auc_roc']:.4f}")
  print(f"    Diferencia: {performance_diff_auc_roc:+.4f}")
  print(f"    RetenciÃ³n: {performance_retention_auc_roc:.2f}%")
  ```

**MÃ©tricas de rendimiento:**
- âœ… AUC-ROC original vs cuantizado
- âœ… AUC-PR original vs cuantizado
- âœ… Diferencia de rendimiento (cuÃ¡nto se pierde)
- âœ… Porcentaje de retenciÃ³n de rendimiento (cuÃ¡nto se mantiene)

**Notas:**
- âœ… ComparaciÃ³n de rendimiento implementada correctamente
- âœ… Usa las mismas mÃ©tricas que la evaluaciÃ³n principal (AUC-ROC, AUC-PR)
- âœ… Calcula diferencia y retenciÃ³n para evaluar impacto de cuantizaciÃ³n

---

### âœ… V.6. AnÃ¡lisis Incluido en el Informe (LÃ­nea 139)

**Requisito del enunciado:**
> incluya este anÃ¡lisis en su informe

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 5042-5075: Resumen comparativo completo:
  ```python
  print("="*80)
  print("RESUMEN COMPARATIVO DE CUANTIZACIÃ“N")
  print("="*80)
  print("\nComparaciÃ³n de los 3 mejores modelos: Original vs Cuantizado\n")
  
  for i, result in enumerate(quantization_results, 1):
      print(f"{i}. {result['model_type']} - {result['config']}")
      print(f"   TamaÃ±o: Original: {result['original_size_mb']:.2f} MB â†’ Cuantizado: {result['quantized_size_mb']:.2f} MB")
      print(f"   CompresiÃ³n: {result['compression_ratio']:.2f}x")
      print(f"   Latencia: Original: {result['original_latency_ms']:.2f} ms â†’ Cuantizado: {result['quantized_latency_ms']:.2f} ms")
      print(f"   Speedup: {result['speedup']:.2f}x")
      print(f"   Rendimiento (AUC-ROC): Original: {result['original_auc_roc']:.4f} â†’ Cuantizado: {result['quantized_auc_roc']:.4f}")
      print(f"   Diferencia: {result['performance_diff_auc_roc']:+.4f} ({result['performance_retention_auc_roc']:.2f}% retenciÃ³n)")
  ```
- LÃ­nea 5077-5088: Resumen estadÃ­stico:
  ```python
  print("="*80)
  print("RESUMEN ESTADÃSTICO")
  print("="*80)
  avg_compression = np.mean([r['compression_ratio'] for r in quantization_results])
  avg_speedup = np.mean([r['speedup'] for r in quantization_results])
  avg_retention_auc_roc = np.mean([r['performance_retention_auc_roc'] for r in quantization_results])
  avg_retention_auc_pr = np.mean([r['performance_retention_auc_pr'] for r in quantization_results])
  
  print(f"\nPromedio de compresiÃ³n: {avg_compression:.2f}x")
  print(f"Promedio de speedup: {avg_speedup:.2f}x")
  print(f"RetenciÃ³n promedio de rendimiento (AUC-ROC): {avg_retention_auc_roc:.2f}%")
  print(f"RetenciÃ³n promedio de rendimiento (AUC-PR): {avg_retention_auc_pr:.2f}%")
  ```

**AnÃ¡lisis incluido:**
- âœ… ComparaciÃ³n detallada por modelo (tamaÃ±o, latencia, rendimiento)
- âœ… Resumen estadÃ­stico con promedios
- âœ… VisualizaciÃ³n clara de resultados
- âœ… MÃ©tricas calculadas y documentadas

**Notas:**
- âœ… AnÃ¡lisis completo y estructurado
- âœ… FÃ¡cil de incluir en informe
- âœ… Incluye promedios para anÃ¡lisis general

---

## VerificaciÃ³n: SecciÃ³n VI. ANÃLISIS DE OUTLIERS MEDIANTE DBSCAN CLUSTERING (LÃ­neas 141-150)

**Fecha de verificaciÃ³n:** 2025-01-27

### âœ… VI.1. SelecciÃ³n del Mejor Modelo (LÃ­nea 143)

**Requisito del enunciado:**
> Una vez identificado el mejor modelo de detecciÃ³n de anomalÃ­as â€”ya sea el clasificador CNN entrenado desde cero, su versiÃ³n distilada mediante teacherâ€“student, o el modelo autoencoder basado en U-Netâ€” proceda a utilizar sus embeddings como insumo para realizar un anÃ¡lisis adicional mediante tÃ©cnicas de agrupamiento no supervisado.

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 5391-5393: SelecciÃ³n del mejor modelo:
  ```python
  if best_3_models:
      best_model_info = best_3_models[0]  # Toma el mejor (primer lugar)
      print(f"Analizando con el mejor modelo: {best_model_info['model_type']} - {best_model_info['config']}")
  ```
- LÃ­nea 5395-5415: BÃºsqueda del modelo en los resultados:
  ```python
  if best_model_info['model_type'] == "Modelo A":
      # Buscar en model_a_results
  elif best_model_info['model_type'] == "Modelo B":
      # Buscar en model_b_results
  elif best_model_info['model_type'] == "Modelo C":
      # Buscar en model_c_results
  ```
- LÃ­nea 4540: DocumentaciÃ³n: "Los mejores modelos se seleccionan segÃºn AUC-ROC para cuantizaciÃ³n y anÃ¡lisis DBSCAN"

**Notas:**
- âœ… Selecciona el mejor modelo segÃºn AUC-ROC (mismo criterio que para cuantizaciÃ³n)
- âœ… Soporta los tres tipos de modelos (A, B, C)
- âœ… El mejor modelo es el primero de `best_3_models` (mayor AUC-ROC)

---

### âœ… VI.2. ExtracciÃ³n de Embeddings del Conjunto de Prueba (LÃ­nea 145)

**Requisito del enunciado:**
> Extraiga los embeddings generados por el modelo seleccionado para cada imagen del conjunto de prueba.

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 5417-5446: ExtracciÃ³n de embeddings:
  ```python
  # Extraer embeddings del conjunto de prueba
  all_embeddings = []
  all_labels = []
  
  best_model.eval()
  with torch.no_grad():
      for batch in data_module.test_dataloader():
          images = images.to(device)
          
          # Extraer embeddings
          if hasattr(best_model, 'get_embedding'):
              embeddings = best_model.get_embedding(images)
          elif hasattr(best_model, 'model') and hasattr(best_model.model, 'get_embedding'):
              embeddings = best_model.model.get_embedding(images)
          else:
              logits, embeddings = best_model.model(images)
          
          all_embeddings.append(embeddings.cpu().numpy())
          if labels is not None:
              all_labels.append(labels.cpu().numpy())
  
  all_embeddings = np.concatenate(all_embeddings, axis=0)
  all_labels = np.concatenate(all_labels, axis=0) if all_labels else None
  ```

**Notas:**
- âœ… Extrae embeddings de todas las imÃ¡genes del conjunto de prueba
- âœ… Soporta diferentes formas de extraer embeddings segÃºn el tipo de modelo
- âœ… Guarda tambiÃ©n las etiquetas (ground truth) para comparaciÃ³n

---

### âœ… VI.3. ReducciÃ³n de Dimensionalidad con PCA y t-SNE (LÃ­nea 145)

**Requisito del enunciado:**
> Con el fin de facilitar tanto la visualizaciÃ³n como la separaciÃ³n estructural, aplique reducciÃ³n de dimensionalidad con **PCA** y **t-SNE**.

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 292-293: Importaciones:
  ```python
  from sklearn.decomposition import PCA
  from sklearn.manifold import TSNE
  ```
- LÃ­nea 5185-5196: AplicaciÃ³n de PCA:
  ```python
  # ReducciÃ³n de dimensionalidad con PCA
  if use_pca and embeddings.shape[1] > pca_components:
      print(f"  Aplicando PCA: {embeddings.shape[1]} â†’ {pca_components} dimensiones")
      pca = PCA(n_components=pca_components)
      embeddings_reduced = pca.fit_transform(embeddings)
      explained_variance = np.sum(pca.explained_variance_ratio_)
      print(f"  âœ“ Varianza explicada por PCA: {explained_variance:.4f} ({explained_variance*100:.2f}%)")
  ```
- LÃ­nea 5213-5223: AplicaciÃ³n de t-SNE:
  ```python
  # ReducciÃ³n para visualizaciÃ³n con t-SNE
  if use_tsne:
      print(f"  Aplicando t-SNE para visualizaciÃ³n 2D...")
      perplexity = min(tsne_perplexity, len(embeddings_reduced) - 1)
      if perplexity > 0:
          tsne = TSNE(n_components=tsne_components, random_state=42, perplexity=perplexity)
          embeddings_2d = tsne.fit_transform(embeddings_reduced)
          print(f"  âœ“ t-SNE completado: {embeddings_reduced.shape[1]} â†’ {tsne_components} dimensiones")
  ```

**ConfiguraciÃ³n:**
- LÃ­nea 1071-1075: En `conf/config.yaml`:
  ```yaml
  dbscan:
    use_pca: true
    pca_components: 50
    use_tsne: true
    tsne_components: 2
    tsne_perplexity: 30
  ```

**Proceso:**
- âœ… **PCA**: Reduce dimensionalidad manteniendo varianza (configurable, default: 50 componentes)
- âœ… **t-SNE**: Reduce a 2D para visualizaciÃ³n preservando estructura local
- âœ… Proceso: Embeddings originales â†’ PCA â†’ DBSCAN â†’ t-SNE (para visualizaciÃ³n)

**Notas:**
- âœ… PCA aplicado antes de DBSCAN (facilita procesamiento)
- âœ… t-SNE aplicado despuÃ©s de DBSCAN (para visualizaciÃ³n 2D)
- âœ… ConfiguraciÃ³n flexible mediante YAML

---

### âœ… VI.4. AplicaciÃ³n de DBSCAN (LÃ­neas 143, 147)

**Requisito del enunciado:**
> En particular **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise), un mÃ©todo basado en densidad que permite identificar regiones de alta concentraciÃ³n en el espacio latente y, simultÃ¡neamente, detectar puntos aislados que pueden interpretarse como outliers o anomalÃ­as.  
> Una vez obtenidas las representaciones latentes reducidas aplique **DBSCAN**.

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 294: ImportaciÃ³n: `from sklearn.cluster import DBSCAN`
- LÃ­nea 5159-5233: FunciÃ³n `dbscan_analysis()`:
  ```python
  def dbscan_analysis(embeddings, eps=0.5, min_samples=5, use_pca=True, pca_components=50,
                      use_tsne=True, tsne_components=2, tsne_perplexity=30):
      # Aplicar DBSCAN
      dbscan = DBSCAN(eps=eps, min_samples=min_samples)
      clusters = dbscan.fit_predict(embeddings_reduced)
      
      # Identificar outliers (ruido)
      n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)
      n_noise = list(clusters).count(-1)
  ```
- LÃ­nea 5461-5470: AplicaciÃ³n en el mejor modelo:
  ```python
  dbscan_results = dbscan_analysis(
      embeddings=all_embeddings,
      eps=dbscan_config.get("eps", 0.5),
      min_samples=dbscan_config.get("min_samples", 5),
      use_pca=dbscan_config.get("use_pca", True),
      pca_components=dbscan_config.get("pca_components", 50),
      use_tsne=dbscan_config.get("use_tsne", True),
      tsne_components=dbscan_config.get("tsne_components", 2),
      tsne_perplexity=dbscan_config.get("tsne_perplexity", 30)
  )
  ```

**ConfiguraciÃ³n:**
- LÃ­nea 1068-1075: En `conf/config.yaml`:
  ```yaml
  dbscan:
    eps: 0.5
    min_samples: 5
  ```

**Notas:**
- âœ… DBSCAN aplicado correctamente
- âœ… Identifica clusters (regiones de alta densidad)
- âœ… Identifica outliers/ruido (puntos etiquetados como -1)
- âœ… ParÃ¡metros configurables (eps, min_samples)

---

### âœ… VI.5. InterpretaciÃ³n de Ruido como AnomalÃ­as (LÃ­nea 147)

**Requisito del enunciado:**
> Desde la perspectiva de la detecciÃ³n de anomalÃ­as, los puntos etiquetados por DBSCAN como ruido constituyen una indicaciÃ³n natural de potencial anomalÃ­a, ya que representan vectores que se encuentran en zonas de baja densidad del espacio latente.

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 5203-5211: IdentificaciÃ³n de outliers:
  ```python
  # Identificar outliers (ruido)
  n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)
  n_noise = list(clusters).count(-1)  # Puntos etiquetados como -1 son ruido
  n_in_clusters = len(clusters) - n_noise
  
  print(f"  âœ“ DBSCAN completado:")
  print(f"    - Clusters encontrados: {n_clusters}")
  print(f"    - Puntos en clusters: {n_in_clusters}")
  print(f"    - Outliers (ruido): {n_noise}")
  ```
- LÃ­nea 5482-5483: Uso para detecciÃ³n de anomalÃ­as:
  ```python
  dbscan_outliers = (dbscan_results['clusters'] == -1).astype(int)  # 1 = outlier/anomalÃ­a
  true_anomalies = all_labels  # Ground truth
  ```

**Notas:**
- âœ… Puntos etiquetados como -1 se interpretan como outliers/anomalÃ­as
- âœ… Se comparan con ground truth para evaluaciÃ³n
- âœ… LÃ³gica correcta: ruido = baja densidad = potencial anomalÃ­a

---

### âœ… VI.6. AnÃ¡lisis Visual (LÃ­nea 149)

**Requisito del enunciado:**
> Analice los resultados desde el punto de vista visual

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 5240-5334: FunciÃ³n `visualize_dbscan_results()`:
  ```python
  def visualize_dbscan_results(dbscan_results, labels=None, save_path=None):
      """
      Visualiza los resultados de DBSCAN de forma completa.
      
      Muestra:
      1. Clustering DBSCAN (clusters y outliers)
      2. ComparaciÃ³n con ground truth labels
      3. AnÃ¡lisis de distribuciÃ³n de outliers vs normales
      """
  ```

**Visualizaciones implementadas:**

1. **Clustering DBSCAN** (LÃ­neas 5255-5268):
   - âœ… Muestra clusters con diferentes colores
   - âœ… Muestra outliers (ruido) en negro con marcador 'x'
   - âœ… Leyenda con nÃºmero de clusters y outliers

2. **Ground Truth Labels** (LÃ­neas 5270-5281):
   - âœ… Compara con etiquetas reales (normal vs anomalÃ­a)
   - âœ… Verde para normales, rojo para anomalÃ­as
   - âœ… Permite comparar visualmente con DBSCAN

3. **DBSCAN Outliers vs Ground Truth** (LÃ­neas 5283-5316):
   - âœ… VisualizaciÃ³n combinada:
     - Normal en cluster (lightgreen, pequeÃ±o)
     - Normal como outlier DBSCAN (green, grande, 'x')
     - AnomalÃ­a en cluster (lightcoral, pequeÃ±o)
     - AnomalÃ­a como outlier DBSCAN (red, grande, 'x')
   - âœ… Facilita identificar coincidencias y discrepancias

**UbicaciÃ³n de uso:**
- LÃ­nea 5479: `visualize_dbscan_results(dbscan_results, labels=all_labels, save_path=save_path)`

**Notas:**
- âœ… Tres visualizaciones diferentes para anÃ¡lisis completo
- âœ… ComparaciÃ³n visual con ground truth
- âœ… Guarda visualizaciÃ³n en archivo (opcional)
- âœ… Usa t-SNE 2D para visualizaciÃ³n

---

### âœ… VI.7. AnÃ¡lisis Cuantitativo (LÃ­nea 149)

**Requisito del enunciado:**
> Analice los resultados desde el punto de vista visual, y cuantitativa del resultado de la clasificaciÃ³n de anomalÃ­as.

**Estado:** âœ… **IMPLEMENTADO**

**UbicaciÃ³n en notebook:**
- LÃ­nea 5481-5550: AnÃ¡lisis cuantitativo completo:
  ```python
  # AnÃ¡lisis cuantitativo: Comparar outliers de DBSCAN con ground truth
  if all_labels is not None:
      dbscan_outliers = (dbscan_results['clusters'] == -1).astype(int)
      true_anomalies = all_labels
      
      # Calcular mÃ©tricas de clasificaciÃ³n
      dbscan_auc = roc_auc_score(true_anomalies, dbscan_outliers)
      dbscan_ap = average_precision_score(true_anomalies, dbscan_outliers)
      
      # Matriz de confusiÃ³n
      cm = confusion_matrix(true_anomalies, dbscan_outliers)
      tn, fp, fn, tp = cm.ravel()
      
      # Calcular precisiÃ³n, recall, F1, accuracy
      precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
      recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
      f1_score = 2 * (precision * recall) / (precision + recall)
      accuracy = (tp + tn) / (tp + tn + fp + fn)
  ```

**MÃ©tricas calculadas:**
- âœ… **AUC-ROC**: Ãrea bajo curva ROC
- âœ… **Average Precision (AUC-PR)**: Ãrea bajo curva Precision-Recall
- âœ… **Accuracy**: PrecisiÃ³n general
- âœ… **Precision**: PrecisiÃ³n de detecciÃ³n de anomalÃ­as
- âœ… **Recall**: Sensibilidad de detecciÃ³n
- âœ… **F1-Score**: Media armÃ³nica de precisiÃ³n y recall
- âœ… **Matriz de ConfusiÃ³n**: TN, FP, FN, TP

**EstadÃ­sticas adicionales:**
- âœ… Total de muestras
- âœ… Muestras normales vs anÃ³malas (ground truth)
- âœ… Clusters encontrados
- âœ… Outliers detectados por DBSCAN
- âœ… Porcentaje de outliers
- âœ… DistribuciÃ³n de outliers (normales vs anomalÃ­as)

**Notas:**
- âœ… AnÃ¡lisis cuantitativo completo y detallado
- âœ… Compara DBSCAN outliers con ground truth
- âœ… MÃ©tricas estÃ¡ndar de clasificaciÃ³n implementadas
- âœ… EstadÃ­sticas descriptivas para entender resultados

---

## Resumen de VerificaciÃ³n

| Componente | Estado | Observaciones |
|------------|--------|---------------|
| Objetivo del proyecto | âœ… | Correctamente implementado y documentado |
| Dataset MVTec AD | âœ… | Configurado y cargado correctamente |
| 10 clases seleccionadas | âœ… | Todas las clases estÃ¡n en la configuraciÃ³n |
| DataModule (Lightning) | âœ… | Implementado siguiendo mejores prÃ¡cticas |
| DetecciÃ³n por embeddings | âœ… | Implementado con mÃºltiples mÃ©todos |
| DetecciÃ³n por reconstrucciÃ³n | âœ… | Implementado, podrÃ­a mejorarse documentaciÃ³n |
| **GestiÃ³n con Hydra** | âœ… | **Inicializado y configurado correctamente** |
| **Estructura conf/** | âœ… | **Todos los archivos requeridos presentes** |
| **DimensiÃ³n espacio latente (z)** | âœ… | **Configurable en YAML** |
| **Ã‰pocas y batch size** | âœ… | **Configurables en YAML** |
| **Otros hiperparÃ¡metros** | âœ… | **Ampliamente configurables** |
| **PyTorch Lightning** | âœ… | **Correctamente implementado** |
| **LightningDataModule** | âœ… | **MVTecDataModule implementado** |
| **LightningModule mÃ©todos mÃ­nimos** | âœ… | **training_step, test_step, configure_optimizers** |
| **EarlyStopping callback** | âœ… | **Implementado y configurado** |
| **ReduceLROnPlateau callback** | âœ… | **Implementado (scheduler + monitor)** |
| **Entrenamiento solo sin defectos** | âœ… | **Solo datos 'good' en entrenamiento** |
| **Scripts externos** | âš ï¸ | **No implementado (opcional segÃºn nota)** |
| **Estructura ResNet-18 (conv1, conv2, conv3)** | âœ… | **Implementada correctamente** |
| **Modelo A (entrenado desde 0)** | âœ… | **Pesos aleatorios, sin pre-entrenamiento** |
| **Modelo B (destilaciÃ³n teacher-student)** | âœ… | **ResNet-18 como teacher, destilaciÃ³n implementada** |
| **ExtracciÃ³n de embeddings** | âœ… | **MÃ©todo get_embedding() implementado** |
| **3 configuraciones por modelo (9 totales)** | âœ… | **Cumple con requisito de 9 entrenamientos** |
| **Modelo C (U-Net Autoencoder)** | âœ… | **Basado en U-Net con skip connections** |
| **ReconstrucciÃ³n de imÃ¡genes** | âœ… | **Forward() reconstruye imÃ¡genes de entrada** |
| **Embeddings del autoencoder** | âœ… | **ExtraÃ­dos del espacio latente** |
| **Entrenamiento desde 0 (Modelo C)** | âœ… | **Sin pre-entrenamiento, pesos aleatorios** |
| **ExtracciÃ³n de embeddings (validaciÃ³n)** | âœ… | **Del conjunto de validaciÃ³n/entrenamiento (solo normales)** |
| **EstimaciÃ³n distribuciÃ³n normal (Î¼, Î£)** | âœ… | **FÃ³rmulas exactas segÃºn enunciado** |
| **Distancia de Mahalanobis** | âœ… | **d = sqrt((z - Î¼)^T Î£^(-1) (z - Î¼))** |
| **ClasificaciÃ³n por percentiles** | âœ… | **Umbral basado en percentil de distancias normales** |
| **Distancia Euclidiana** | âœ… | **d = ||z - Î¼|| implementada** |
| **Reconstruction Loss** | âœ… | **MSE entre reconstrucciÃ³n y original** |
| **SelecciÃ³n 3 mejores modelos** | âœ… | **SegÃºn AUC-ROC, criterio claro** |
| **ConversiÃ³n a cuantizados** | âœ… | **CuantizaciÃ³n dinÃ¡mica implementada** |
| **ComparaciÃ³n de tamaÃ±o** | âœ… | **Original vs cuantizado + ratio compresiÃ³n** |
| **ComparaciÃ³n de latencia** | âœ… | **Promedio 100 iteraciones + speedup** |
| **ComparaciÃ³n de rendimiento** | âœ… | **AUC-ROC y AUC-PR + retenciÃ³n** |
| **AnÃ¡lisis en informe** | âœ… | **Resumen comparativo y estadÃ­stico** |
| **SelecciÃ³n mejor modelo para DBSCAN** | âœ… | **Mejor segÃºn AUC-ROC** |
| **ExtracciÃ³n embeddings conjunto prueba** | âœ… | **Para todas las imÃ¡genes de prueba** |
| **ReducciÃ³n dimensionalidad PCA** | âœ… | **Configurable, default 50 componentes** |
| **ReducciÃ³n dimensionalidad t-SNE** | âœ… | **2D para visualizaciÃ³n** |
| **AplicaciÃ³n DBSCAN** | âœ… | **Clusters y outliers identificados** |
| **InterpretaciÃ³n ruido como anomalÃ­as** | âœ… | **Puntos -1 = outliers/anomalÃ­as** |
| **AnÃ¡lisis visual DBSCAN** | âœ… | **3 visualizaciones: clusters, ground truth, comparaciÃ³n** |
| **AnÃ¡lisis cuantitativo DBSCAN** | âœ… | **AUC-ROC, AUC-PR, matriz confusiÃ³n, precisiÃ³n, recall, F1** |

---

## Acciones Recomendadas

1. âœ… **Completado**: VerificaciÃ³n de implementaciÃ³n de secciÃ³n I y II
2. âœ… **Completado**: VerificaciÃ³n de implementaciÃ³n de secciÃ³n III (lÃ­neas 21-43)
3. âœ… **Completado**: VerificaciÃ³n de implementaciÃ³n de PyTorch Lightning (lÃ­neas 45-56)
4. âœ… **Completado**: VerificaciÃ³n de Modelo Clasificador CNN (lÃ­neas 58-76)
5. âœ… **Completado**: VerificaciÃ³n de Modelo C Autoencoder U-Net (lÃ­neas 78-84)
6. âœ… **Completado**: VerificaciÃ³n de EvaluaciÃ³n de AnomalÃ­as (lÃ­neas 103-136)
7. âœ… **Completado**: VerificaciÃ³n de Modelos Cuantizados (lÃ­neas 137-139)
8. âœ… **Completado**: VerificaciÃ³n de AnÃ¡lisis DBSCAN (lÃ­neas 141-150)
9. âš ï¸ **Opcional**: AÃ±adir nota explÃ­cita en el notebook sobre la pregunta filosÃ³fica de detecciÃ³n de anomalÃ­as (lÃ­nea 19 del enunciado)
10. âš ï¸ **Opcional**: Crear scripts externos en carpeta "scripts" y documentarlos en el notebook (segÃºn nota del profesor, lÃ­nea 56)

---

## Historial de Cambios

- **2025-01-27**: VerificaciÃ³n inicial de secciÃ³n I y II (lÃ­neas 9-19 del enunciado)
  - Confirmado: Objetivo implementado
  - Confirmado: Dataset MVTec AD configurado
  - Confirmado: 10 clases seleccionadas
  - Confirmado: DetecciÃ³n de anomalÃ­as implementada (embeddings y reconstrucciÃ³n)

- **2025-01-27**: VerificaciÃ³n de secciÃ³n III.1-III.3 (lÃ­neas 21-43 del enunciado)
  - Confirmado: Hydra inicializado y configurado correctamente
  - Confirmado: Estructura de directorios conf/ completa
  - Confirmado: Todos los archivos YAML requeridos presentes
  - Confirmado: DimensiÃ³n del espacio latente (z) configurable
  - Confirmado: Ã‰pocas, batch size y otros hiperparÃ¡metros configurables
  - Nota: Se usa `unet_autoencoder.yaml` en lugar de `vae.yaml` (equivalente funcional)

- **2025-01-27**: VerificaciÃ³n de secciÃ³n III.4 PyTorch Lightning (lÃ­neas 45-56 del enunciado)
  - Confirmado: PyTorch Lightning correctamente implementado
  - Confirmado: MVTecDataModule hereda de LightningDataModule con todos los mÃ©todos
  - Confirmado: CNNClassifierLightning y AutoencoderLightning implementan mÃ©todos mÃ­nimos requeridos
  - Confirmado: training_step, test_step, configure_optimizers implementados en ambos mÃ³dulos
  - Confirmado: EarlyStopping callback implementado y configurado
  - Confirmado: ReduceLROnPlateau scheduler implementado (patience=5)
  - Confirmado: LearningRateMonitor callback implementado
  - Confirmado: Entrenamiento solo con datos sin defectos (solo 'good')
  - Nota: Scripts externos no implementados (opcional segÃºn nota del profesor)

- **2025-01-27**: VerificaciÃ³n de secciÃ³n III.A Modelo Clasificador CNN (lÃ­neas 58-76 del enunciado)
  - Confirmado: Estructura ResNet-18 implementada para conv1, conv2_x, conv3_x
  - Confirmado: Clasificador FC implementado despuÃ©s de las 3 convoluciones
  - Confirmado: Modelo A entrenado desde 0 (pesos aleatorios, model_type="scratch")
  - Confirmado: Modelo B con destilaciÃ³n teacher-student (ResNet-18 como teacher)
  - Confirmado: DestilaciÃ³n implementada con temperatura, alpha y KL divergence
  - Confirmado: ExtracciÃ³n de embeddings implementada (mÃ©todo get_embedding())
  - Confirmado: 3 configuraciones para Modelo A (variando hiperparÃ¡metros)
  - Confirmado: 3 configuraciones para Modelo B (variando hiperparÃ¡metros y destilaciÃ³n)
  - Confirmado: 3 configuraciones para Modelo C (variando hiperparÃ¡metros)
  - Confirmado: Total de 9 entrenamientos (cumple con requisito)

- **2025-01-27**: VerificaciÃ³n de secciÃ³n III.B Modelo C Autoencoder U-Net (lÃ­neas 78-84 del enunciado)
  - Confirmado: Autoencoder basado en U-Net con skip connections implementado
  - Confirmado: Encoder, bottleneck y decoder con skip connections correctamente implementados
  - Confirmado: ReconstrucciÃ³n de imÃ¡genes implementada (forward() reconstruye entrada)
  - Confirmado: ExtracciÃ³n de embeddings del espacio latente implementada
  - Confirmado: MÃ©todo get_embedding() implementado para extraer embeddings
  - Confirmado: Entrenamiento completamente desde 0 (sin pre-entrenamiento)
  - Confirmado: Similar a implementaciÃ³n de Tarea 5 (como menciona la nota)
  - Confirmado: Permite comparaciÃ³n con Modelos A y B (diferentes arquitecturas para embeddings)

- **2025-01-27**: VerificaciÃ³n de secciÃ³n IV EvaluaciÃ³n de AnomalÃ­as (lÃ­neas 103-136 del enunciado)
  - Confirmado: ExtracciÃ³n de embeddings del conjunto de validaciÃ³n/entrenamiento (solo datos normales)
  - Confirmado: EstimaciÃ³n de distribuciÃ³n normal: Î¼ = (1/N) Î£ z_i y Î£ = (1/(N-1)) Î£ (z_i - Î¼)(z_i - Î¼)^T
  - Confirmado: FÃ³rmulas implementadas exactamente como en el enunciado
  - Confirmado: CÃ¡lculo de distancia de Mahalanobis: d = sqrt((z - Î¼)^T Î£^(-1) (z - Î¼))
  - Confirmado: ClasificaciÃ³n usando percentiles (umbral basado en percentil de distancias normales)
  - Confirmado: Distancia Euclidiana implementada: d = ||z - Î¼||
  - Confirmado: Reconstruction Loss implementado: MSE entre reconstrucciÃ³n y original
  - Confirmado: Tres mÃ©todos de detecciÃ³n implementados y justificados
  - Confirmado: MÃ©tricas calculadas: AUC-ROC, AUC-PR

- **2025-01-27**: VerificaciÃ³n de secciÃ³n V Modelos Cuantizados (lÃ­neas 137-139 del enunciado)
  - Confirmado: SelecciÃ³n de 3 mejores modelos segÃºn AUC-ROC (criterio claro y justificado)
  - Confirmado: ConversiÃ³n a modelos cuantizados usando cuantizaciÃ³n dinÃ¡mica de PyTorch
  - Confirmado: ComparaciÃ³n de tamaÃ±o: original vs cuantizado + ratio de compresiÃ³n
  - Confirmado: ComparaciÃ³n de latencia: promedio sobre 100 iteraciones + speedup
  - Confirmado: ComparaciÃ³n de rendimiento: AUC-ROC y AUC-PR + diferencia y retenciÃ³n
  - Confirmado: AnÃ¡lisis completo incluido: resumen comparativo y estadÃ­stico
  - Confirmado: MÃ©tricas calculadas para todos los aspectos requeridos

- **2025-01-27**: VerificaciÃ³n de secciÃ³n VI AnÃ¡lisis DBSCAN (lÃ­neas 141-150 del enunciado)
  - Confirmado: SelecciÃ³n del mejor modelo segÃºn AUC-ROC para anÃ¡lisis DBSCAN
  - Confirmado: ExtracciÃ³n de embeddings del conjunto de prueba (todas las imÃ¡genes)
  - Confirmado: ReducciÃ³n de dimensionalidad con PCA (configurable, default 50 componentes)
  - Confirmado: ReducciÃ³n de dimensionalidad con t-SNE (2D para visualizaciÃ³n)
  - Confirmado: AplicaciÃ³n de DBSCAN para identificar clusters y outliers
  - Confirmado: InterpretaciÃ³n de ruido (-1) como anomalÃ­as (puntos de baja densidad)
  - Confirmado: AnÃ¡lisis visual: 3 visualizaciones (clusters, ground truth, comparaciÃ³n)
  - Confirmado: AnÃ¡lisis cuantitativo: AUC-ROC, AUC-PR, matriz de confusiÃ³n, precisiÃ³n, recall, F1
  - Confirmado: ComparaciÃ³n de outliers DBSCAN con ground truth para evaluaciÃ³n

