{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto II - Detecci√≥n de Anomal√≠as con Destilaci√≥n de Modelos\n",
    "\n",
    "**Curso de Inteligencia Artificial**  \n",
    "**Escuela de Ingenier√≠a en Computaci√≥n**  \n",
    "**Instituto Tecnol√≥gico de Costa Rica**\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Validar la hip√≥tesis de que mediante destilaci√≥n de modelos se pueden resolver tareas complejas con modelos m√°s peque√±os y eficientes.\n",
    "\n",
    "Este notebook implementa tres modelos:\n",
    "- **Modelo A**: CNN clasificador desde cero\n",
    "- **Modelo B**: CNN clasificador con destilaci√≥n teacher-student\n",
    "- **Modelo C**: Autoencoder U-Net para reconstrucci√≥n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Configuraci√≥n Inicial\n",
    "\n",
    "### 0.1. Instalaci√≥n de Dependencias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 0. Configuraci√≥n Inicial\n",
    "\n",
    "### 0.1. Instalaci√≥n de Dependencias\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install matplotlib numpy scikit-learn opencv-python pillow tqdm wandb pytorch-lightning hydra-core omegaconf torchmetrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Montar Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 0.2. Montar Google Drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3. Imports y Configuraci√≥n Inicial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "import wandb\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure\n",
    "from torchmetrics.classification import Accuracy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Pytorch Lightning version: {pl.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Definici√≥n de Arquitecturas de Modelos\n",
    "\n",
    "En esta secci√≥n se definen las tres arquitecturas de modelos:\n",
    "\n",
    "- **CNNClassifier** (Modelo A y B): Basado en ResNet-18 para las primeras 3 convoluciones\n",
    "  - Modelo A: Entrenado desde cero (scratch)\n",
    "  - Modelo B: Entrenado con destilaci√≥n teacher-student (ResNet-18 como teacher)\n",
    "  \n",
    "- **UNetAutoencoder** (Modelo C): Autoencoder con skip connections para reconstrucci√≥n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los modelos desde el archivo models.py\n",
    "# En Colab, necesitamos copiar el contenido o importarlo desde Google Drive\n",
    "\n",
    "# Opci√≥n 1: Si tienes models.py en Google Drive, puedes importarlo as√≠:\n",
    "# import sys\n",
    "# sys.path.append('/content/drive/MyDrive/Colab Notebooks/Proyecto-II')\n",
    "# from models import CNNClassifier, UNetAutoencoder, BasicBlock\n",
    "\n",
    "# Opci√≥n 2: Definir las clases directamente en el notebook (recomendado para Colab)\n",
    "# Copiamos el contenido de models.py aqu√≠\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Bloque b√°sico de ResNet (2 convoluciones con skip connection)\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN Clasificador basado en ResNet-18 para las primeras 3 convoluciones\n",
    "    Modelo A: Desde cero (scratch)\n",
    "    Modelo B: Con destilaci√≥n (distilled)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10, conv1_channels=64, conv2_channels=[64, 64], \n",
    "                 conv3_channels=[128, 128], fc_hidden=512, dropout=0.5, \n",
    "                 embedding_dim=256, model_type=\"scratch\"):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.model_type = model_type\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # conv1: Primera convoluci√≥n (similar a ResNet-18)\n",
    "        self.conv1 = nn.Conv2d(3, conv1_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(conv1_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # conv2_x: Bloques residuales\n",
    "        self.conv2_x = self._make_layer(conv1_channels, conv2_channels[0], conv2_channels[1], num_blocks=2, stride=1)\n",
    "        \n",
    "        # conv3_x: Bloques residuales\n",
    "        self.conv3_x = self._make_layer(conv2_channels[-1], conv3_channels[0], conv3_channels[1], num_blocks=2, stride=2)\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Clasificador\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv3_channels[-1], fc_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(fc_hidden, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Capa para extraer embeddings (para detecci√≥n de anomal√≠as)\n",
    "        self.embedding_layer = nn.Linear(conv3_channels[-1], embedding_dim)\n",
    "    \n",
    "    def _make_layer(self, in_channels, base_channels, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(BasicBlock(in_channels, base_channels, stride))\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(BasicBlock(base_channels, out_channels, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # conv1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # conv2_x\n",
    "        x = self.conv2_x(x)\n",
    "        \n",
    "        # conv3_x\n",
    "        x = self.conv3_x(x)\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Embedding para detecci√≥n de anomal√≠as\n",
    "        embedding = self.embedding_layer(x)\n",
    "        \n",
    "        # Clasificaci√≥n\n",
    "        logits = self.fc(x)\n",
    "        \n",
    "        return logits, embedding\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        \"\"\"Extrae solo el embedding sin clasificaci√≥n\"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2_x(x)\n",
    "        x = self.conv3_x(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        embedding = self.embedding_layer(x)\n",
    "        return embedding\n",
    "\n",
    "\n",
    "class UNetAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoencoder U-Net con skip connections (Modelo C)\n",
    "    Reutilizado de Tarea05\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_channels=3, latent_dim=128, encoder_channels=None, \n",
    "                 decoder_channels=None, embedding_dim=128):\n",
    "        super(UNetAutoencoder, self).__init__()\n",
    "        self.architecture = \"unet_autoencoder\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        if encoder_channels is None:\n",
    "            encoder_channels = [64, 128, 256, 512]\n",
    "        if decoder_channels is None:\n",
    "            decoder_channels = [512, 256, 128, 64]\n",
    "        \n",
    "        # Encoder con skip connections\n",
    "        self.encoder_blocks = nn.ModuleList()\n",
    "        in_channels = input_channels\n",
    "        \n",
    "        for out_channels in encoder_channels:\n",
    "            self.encoder_blocks.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm2d(out_channels)\n",
    "                )\n",
    "            )\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        # Capa bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, latent_dim, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder con skip connections\n",
    "        self.decoder_blocks = nn.ModuleList()\n",
    "        in_channels = latent_dim\n",
    "        \n",
    "        # Primera capa del decoder (sin skip connection)\n",
    "        self.decoder_blocks.append(\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, decoder_channels[0], kernel_size=4, stride=2, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm2d(decoder_channels[0])\n",
    "            )\n",
    "        )\n",
    "        in_channels = decoder_channels[0]\n",
    "        \n",
    "        # Resto de capas del decoder con skip connections\n",
    "        for i, out_channels in enumerate(decoder_channels[1:], 1):\n",
    "            self.decoder_blocks.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(in_channels * 2, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm2d(out_channels)\n",
    "                )\n",
    "            )\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        # Capa final\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels * 2, input_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Capa para extraer embeddings\n",
    "        self.embedding_layer = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(latent_dim, embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Extrae el vector latente de la entrada\"\"\"\n",
    "        skip_connections = []\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x = encoder_block(x)\n",
    "            skip_connections.append(x)\n",
    "        \n",
    "        x = self.bottleneck(x)\n",
    "        return x, skip_connections\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder - guardar skip connections\n",
    "        skip_connections = []\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x = encoder_block(x)\n",
    "            skip_connections.append(x)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        # Decoder - usar skip connections\n",
    "        x = self.decoder_blocks[0](x)\n",
    "        \n",
    "        for i, decoder_block in enumerate(self.decoder_blocks[1:], start=1):\n",
    "            skip_idx = -i\n",
    "            skip = skip_connections[skip_idx]\n",
    "            \n",
    "            if x.shape[2:] != skip.shape[2:]:\n",
    "                x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            x = decoder_block(x)\n",
    "        \n",
    "        # Capa final\n",
    "        skip = skip_connections[0]\n",
    "        if x.shape[2:] != skip.shape[2:]:\n",
    "            x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.final_layer(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        \"\"\"Extrae el embedding del espacio latente\"\"\"\n",
    "        latent, _ = self.encode(x)\n",
    "        embedding = self.embedding_layer(latent)\n",
    "        return embedding\n",
    "\n",
    "print(\"‚úì Arquitecturas de modelos definidas correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. M√≥dulos de Pytorch Lightning\n",
    "\n",
    "En esta secci√≥n se definen los m√≥dulos Lightning para entrenar los modelos:\n",
    "- **CNNClassifierLightning**: Para Modelo A y B (con soporte para destilaci√≥n)\n",
    "- **AutoencoderLightning**: Para Modelo C\n",
    "- **LossFunctions**: Funciones de p√©rdida (L1, L2, SSIM, SSIM_L1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√≥dulos Lightning - Copiamos el contenido de lightning_modules.py\n",
    "\n",
    "class LossFunctions:\n",
    "    \"\"\"Funciones de p√©rdida para el entrenamiento\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def l1_loss(pred, target):\n",
    "        return F.l1_loss(pred, target)\n",
    "    \n",
    "    @staticmethod\n",
    "    def l2_loss(pred, target):\n",
    "        return F.mse_loss(pred, target)\n",
    "    \n",
    "    @staticmethod\n",
    "    def ssim_loss(pred, target):\n",
    "        ssim = StructuralSimilarityIndexMeasure(data_range=2.0)\n",
    "        ssim_val = ssim(pred, target)\n",
    "        return 1 - ssim_val\n",
    "    \n",
    "    @staticmethod\n",
    "    def ssim_l1_loss(pred, target, alpha=0.5):\n",
    "        ssim = LossFunctions.ssim_loss(pred, target)\n",
    "        l1 = LossFunctions.l1_loss(pred, target)\n",
    "        return alpha * ssim + (1 - alpha) * l1\n",
    "\n",
    "\n",
    "class CNNClassifierLightning(pl.LightningModule):\n",
    "    \"\"\"M√≥dulo Lightning para entrenar CNN clasificadores (Modelo A y B)\"\"\"\n",
    "    \n",
    "    def __init__(self, model, num_classes=10, learning_rate=0.001, weight_decay=1e-5,\n",
    "                 scheduler_config=None, model_type=\"scratch\", teacher_model=None,\n",
    "                 distillation_config=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.model_type = model_type\n",
    "        self.scheduler_config = scheduler_config or {\"name\": \"step\", \"step_size\": 15, \"gamma\": 0.5}\n",
    "        \n",
    "        # Configuraci√≥n de destilaci√≥n (solo para Modelo B)\n",
    "        self.distillation_config = distillation_config or {}\n",
    "        self.teacher_model = teacher_model\n",
    "        if model_type == \"distilled\" and teacher_model is None:\n",
    "            # Cargar ResNet-18 pre-entrenado como teacher\n",
    "            try:\n",
    "                # Versi√≥n nueva de torchvision (weights)\n",
    "                from torchvision.models import ResNet18_Weights\n",
    "                self.teacher_model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "            except:\n",
    "                # Versi√≥n antigua (pretrained)\n",
    "                self.teacher_model = resnet18(pretrained=True)\n",
    "            self.teacher_model.fc = nn.Linear(self.teacher_model.fc.in_features, num_classes)\n",
    "            self.teacher_model.eval()\n",
    "            for param in self.teacher_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # M√©tricas\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        \n",
    "        # Criterio de p√©rdida\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Guardar hiperpar√°metros\n",
    "        self.save_hyperparameters(ignore=['model', 'teacher_model'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        logits, embeddings = self(images)\n",
    "        \n",
    "        # P√©rdida de clasificaci√≥n\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        # P√©rdida de destilaci√≥n (solo para Modelo B)\n",
    "        if self.model_type == \"distilled\" and self.teacher_model is not None:\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = self.teacher_model(images)\n",
    "            \n",
    "            temperature = self.distillation_config.get(\"temperature\", 4.0)\n",
    "            alpha = self.distillation_config.get(\"alpha\", 0.7)\n",
    "            \n",
    "            # Softmax con temperatura\n",
    "            student_soft = F.log_softmax(logits / temperature, dim=1)\n",
    "            teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\n",
    "            \n",
    "            # P√©rdida de destilaci√≥n (KL divergence)\n",
    "            distillation_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\n",
    "            \n",
    "            # Combinar p√©rdidas\n",
    "            loss = alpha * distillation_loss + (1 - alpha) * loss\n",
    "        \n",
    "        # Logging\n",
    "        self.log('train/loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train/acc', self.train_acc(logits, labels), on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train/learning_rate', self.optimizers().param_groups[0]['lr'], on_step=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        logits, embeddings = self(images)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        # Logging\n",
    "        self.log('val/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val/acc', self.val_acc(logits, labels), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        logits, embeddings = self(images)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        # Logging\n",
    "        self.log('test/loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log('test/acc', self.test_acc(logits, labels), on_step=False, on_epoch=True)\n",
    "        \n",
    "        return {'logits': logits, 'labels': labels, 'embeddings': embeddings}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        \n",
    "        scheduler_name = self.scheduler_config.get(\"name\", \"step\")\n",
    "        if scheduler_name == \"step\":\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "                optimizer,\n",
    "                step_size=self.scheduler_config.get(\"step_size\", 15),\n",
    "                gamma=self.scheduler_config.get(\"gamma\", 0.5)\n",
    "            )\n",
    "        elif scheduler_name == \"cosine\":\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer,\n",
    "                T_max=self.scheduler_config.get(\"T_max\", 50)\n",
    "            )\n",
    "        elif scheduler_name == \"plateau\":\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                mode='min',\n",
    "                factor=self.scheduler_config.get(\"factor\", 0.5),\n",
    "                patience=self.scheduler_config.get(\"patience\", 5)\n",
    "            )\n",
    "        else:\n",
    "            scheduler = None\n",
    "        \n",
    "        if scheduler is None:\n",
    "            return optimizer\n",
    "        else:\n",
    "            # Para ReduceLROnPlateau, necesitamos incluir el monitor\n",
    "            if scheduler_name == \"plateau\":\n",
    "                return {\n",
    "                    \"optimizer\": optimizer,\n",
    "                    \"lr_scheduler\": {\n",
    "                        \"scheduler\": scheduler,\n",
    "                        \"interval\": \"epoch\",\n",
    "                        \"monitor\": \"val/loss\"  # M√©trica a monitorear para ReduceLROnPlateau\n",
    "                    }\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"optimizer\": optimizer,\n",
    "                    \"lr_scheduler\": {\n",
    "                        \"scheduler\": scheduler,\n",
    "                        \"interval\": \"epoch\"\n",
    "                    }\n",
    "                }\n",
    "\n",
    "\n",
    "class AutoencoderLightning(pl.LightningModule):\n",
    "    \"\"\"M√≥dulo Lightning para entrenar autoencoders (Modelo C)\"\"\"\n",
    "    \n",
    "    def __init__(self, model, learning_rate=0.001, loss_function=\"L2\", scheduler_config=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_function = loss_function\n",
    "        self.scheduler_config = scheduler_config or {\"name\": \"step\", \"step_size\": 15, \"gamma\": 0.5}\n",
    "        \n",
    "        # Inicializar funci√≥n de p√©rdida\n",
    "        if loss_function == \"L1\":\n",
    "            self.criterion = LossFunctions.l1_loss\n",
    "        elif loss_function == \"L2\":\n",
    "            self.criterion = LossFunctions.l2_loss\n",
    "        elif loss_function == \"SSIM\":\n",
    "            self.criterion = LossFunctions.ssim_loss\n",
    "        elif loss_function == \"SSIM_L1\":\n",
    "            self.criterion = LossFunctions.ssim_l1_loss\n",
    "        else:\n",
    "            raise ValueError(f\"Funci√≥n de p√©rdida no reconocida: {loss_function}\")\n",
    "        \n",
    "        # M√©tricas\n",
    "        self.ssim_metric = StructuralSimilarityIndexMeasure(data_range=2.0)\n",
    "        \n",
    "        # Guardar hiperpar√°metros\n",
    "        self.save_hyperparameters(ignore=['model'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[0] if isinstance(batch, tuple) else batch\n",
    "        x_recon = self(x)\n",
    "        loss = self.criterion(x_recon, x)\n",
    "        \n",
    "        # Logging\n",
    "        self.log('train/loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train/learning_rate', self.optimizers().param_groups[0]['lr'], on_step=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch[0] if isinstance(batch, tuple) else batch\n",
    "        x_recon = self(x)\n",
    "        loss = self.criterion(x_recon, x)\n",
    "        \n",
    "        # Calcular SSIM\n",
    "        ssim_val = self.ssim_metric(x_recon, x)\n",
    "        \n",
    "        # Logging\n",
    "        self.log('val/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val/ssim', ssim_val, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch[0] if isinstance(batch, tuple) else batch\n",
    "        x_recon = self(x)\n",
    "        loss = self.criterion(x_recon, x)\n",
    "        \n",
    "        # Extraer embeddings\n",
    "        embeddings = self.model.get_embedding(x)\n",
    "        \n",
    "        # Logging\n",
    "        self.log('test/loss', loss, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return {'reconstructions': x_recon, 'originals': x, 'embeddings': embeddings}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        scheduler_name = self.scheduler_config.get(\"name\", \"step\")\n",
    "        if scheduler_name == \"step\":\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "                optimizer,\n",
    "                step_size=self.scheduler_config.get(\"step_size\", 15),\n",
    "                gamma=self.scheduler_config.get(\"gamma\", 0.5)\n",
    "            )\n",
    "        else:\n",
    "            scheduler = None\n",
    "        \n",
    "        if scheduler is None:\n",
    "            return optimizer\n",
    "        else:\n",
    "            return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"interval\": \"epoch\"\n",
    "                }\n",
    "            }\n",
    "\n",
    "print(\"‚úì M√≥dulos Lightning definidos correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de rutas (similar a Tarea05)\n",
    "# Definir ruta base de Google Drive\n",
    "DRIVE_BASE_PATH = '/content/drive/MyDrive/Colab Notebooks'\n",
    "drive_conf_dir = os.path.join(DRIVE_BASE_PATH, 'conf')\n",
    "\n",
    "# Crear directorio base si no existe\n",
    "os.makedirs(DRIVE_BASE_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"üîç Configurando para usar Google Drive...\")\n",
    "print(f\"   Ruta base: {DRIVE_BASE_PATH}\")\n",
    "print(f\"   Configuraci√≥n: {drive_conf_dir}\")\n",
    "\n",
    "# Verificar si existe en Google Drive\n",
    "config_file_drive = os.path.join(drive_conf_dir, 'config.yaml')\n",
    "\n",
    "if not os.path.exists(config_file_drive):\n",
    "    # Si no existe, copiar desde el directorio actual (si existe)\n",
    "    current_conf_dir = './conf'\n",
    "    if os.path.exists(current_conf_dir):\n",
    "        print(f\"‚ö†Ô∏è No se encontr√≥ configuraci√≥n en Google Drive\")\n",
    "        print(f\"   Copiando desde directorio actual: {current_conf_dir}\")\n",
    "        import shutil\n",
    "        if os.path.exists(drive_conf_dir):\n",
    "            shutil.rmtree(drive_conf_dir)\n",
    "        shutil.copytree(current_conf_dir, drive_conf_dir)\n",
    "        print(f\"  ‚úì Configuraci√≥n copiada a Google Drive\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No se encontr√≥ configuraci√≥n. Creando estructura b√°sica...\")\n",
    "        # Crear estructura b√°sica (similar a Tarea05 pero adaptada)\n",
    "        os.makedirs(drive_conf_dir, exist_ok=True)\n",
    "        os.makedirs(os.path.join(drive_conf_dir, 'model'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(drive_conf_dir, 'trainer'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(drive_conf_dir, 'logger'), exist_ok=True)\n",
    "        \n",
    "        # Crear archivos de configuraci√≥n b√°sicos\n",
    "        # (El usuario deber√° ajustar las rutas seg√∫n su configuraci√≥n)\n",
    "        print(f\"  ‚úì Estructura creada. Por favor, ajusta los archivos de configuraci√≥n.\")\n",
    "\n",
    "# Copiar temporalmente al directorio actual para Hydra\n",
    "import shutil\n",
    "current_dir = os.getcwd()\n",
    "conf_dir_temp = os.path.join(current_dir, 'conf')\n",
    "\n",
    "if os.path.exists(conf_dir_temp):\n",
    "    shutil.rmtree(conf_dir_temp)\n",
    "\n",
    "if os.path.exists(drive_conf_dir):\n",
    "    shutil.copytree(drive_conf_dir, conf_dir_temp)\n",
    "    print(f\"  ‚úì Configuraci√≥n copiada temporalmente a: {conf_dir_temp}\")\n",
    "    config_path_for_hydra = 'conf'\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No se encontr√≥ configuraci√≥n. Usando configuraci√≥n por defecto.\")\n",
    "    config_path_for_hydra = None\n",
    "\n",
    "# Autenticaci√≥n de Weights & Biases\n",
    "wandb.login()\n",
    "\n",
    "# Limpiar Hydra si ya est√° inicializado\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "if GlobalHydra.instance().is_initialized():\n",
    "    GlobalHydra.instance().clear()\n",
    "    print(\"‚úì Limpiando instancia previa de Hydra\")\n",
    "\n",
    "# Inicializar Hydra\n",
    "if config_path_for_hydra and os.path.exists(os.path.join(conf_dir_temp, 'config.yaml')):\n",
    "    try:\n",
    "        print(f\"‚úì Inicializando Hydra con config_path='{config_path_for_hydra}'\")\n",
    "        hydra.initialize(config_path=config_path_for_hydra, version_base=None, job_name=\"notebook\")\n",
    "        print(\"‚úì Hydra inicializado correctamente\")\n",
    "        \n",
    "        # Registrar las clases del notebook en el resolver de Hydra\n",
    "        import types\n",
    "        notebook_models = types.ModuleType('notebook_models')\n",
    "        notebook_models.CNNClassifier = CNNClassifier\n",
    "        notebook_models.UNetAutoencoder = UNetAutoencoder\n",
    "        sys.modules['notebook_models'] = notebook_models\n",
    "        \n",
    "        print(\"‚úì Clases del notebook registradas para Hydra\")\n",
    "        \n",
    "        # Cargar configuraci√≥n\n",
    "        cfg = hydra.compose(config_name=\"config\")\n",
    "        print(\"‚úì Configuraci√≥n cargada:\")\n",
    "        print(OmegaConf.to_yaml(cfg))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al inicializar Hydra: {e}\")\n",
    "        print(\"   Continuando sin Hydra (configuraci√≥n manual)\")\n",
    "        cfg = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se encontr√≥ configuraci√≥n de Hydra. Usando valores por defecto.\")\n",
    "    cfg = None\n",
    "\n",
    "# Configurar dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'‚úì Usando dispositivo: {device}')\n",
    "\n",
    "# Si no hay configuraci√≥n de Hydra, usar valores por defecto\n",
    "if cfg is None:\n",
    "    print(\"‚ö†Ô∏è Usando configuraci√≥n por defecto (sin Hydra)\")\n",
    "    # Valores por defecto\n",
    "    DATASET_PATH = '/content/drive/MyDrive/Colab Notebooks/Proyecto2-IA/dataset'\n",
    "    CATEGORIES = [\"bottle\", \"cable\", \"capsule\", \"grid\", \"metal_nut\", \"pill\", \"screw\", \"tile\", \"transistor\", \"zipper\"]\n",
    "    IMAGE_SIZE = 128\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_WORKERS = 2\n",
    "else:\n",
    "    DATASET_PATH = cfg.dataset.path\n",
    "    CATEGORIES = cfg.dataset.categories\n",
    "    IMAGE_SIZE = cfg.dataset.image_size\n",
    "    BATCH_SIZE = cfg.dataset.batch_size\n",
    "    NUM_WORKERS = cfg.dataset.num_workers\n",
    "\n",
    "print(f'‚úì Ruta base de Google Drive configurada: {DRIVE_BASE_PATH}')\n",
    "\n",
    "# Validar que el dataset existe antes de continuar\n",
    "print(f\"\\nüîç Validando dataset...\")\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå ERROR: No se encontr√≥ el dataset en la ruta: {DATASET_PATH}\\n\"\n",
    "        f\"   Por favor, aseg√∫rate de que el dataset MVTec AD est√© en esa ubicaci√≥n.\"\n",
    "    )\n",
    "\n",
    "print(f\"  ‚úì Dataset encontrado en: {DATASET_PATH}\")\n",
    "\n",
    "# Validar que todas las categor√≠as existen\n",
    "missing_categories = []\n",
    "for category in CATEGORIES:\n",
    "    category_path = os.path.join(DATASET_PATH, category)\n",
    "    if not os.path.exists(category_path):\n",
    "        missing_categories.append(category)\n",
    "    else:\n",
    "        # Verificar que tiene las carpetas train y test\n",
    "        train_path = os.path.join(category_path, 'train')\n",
    "        test_path = os.path.join(category_path, 'test')\n",
    "        if not os.path.exists(train_path):\n",
    "            print(f\"  ‚ö†Ô∏è Advertencia: {category} no tiene carpeta 'train'\")\n",
    "        if not os.path.exists(test_path):\n",
    "            print(f\"  ‚ö†Ô∏è Advertencia: {category} no tiene carpeta 'test'\")\n",
    "\n",
    "if missing_categories:\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå ERROR: Las siguientes categor√≠as no se encontraron en el dataset:\\n\"\n",
    "        f\"   {missing_categories}\\n\"\n",
    "        f\"   Ruta del dataset: {DATASET_PATH}\\n\"\n",
    "        f\"   Categor√≠as esperadas: {CATEGORIES}\"\n",
    "    )\n",
    "\n",
    "print(f\"  ‚úì Todas las {len(CATEGORIES)} categor√≠as encontradas: {CATEGORIES}\")\n",
    "print(f\"‚úì Validaci√≥n del dataset completada\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Carga y Preprocesamiento de Datos\n",
    "\n",
    "En esta secci√≥n se carga el dataset MVTec AD con 10 clases. **Importante**: Solo se usan datos sin defectos ('good') para el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataModule para MVTec AD - Copiamos el contenido de data_module.py\n",
    "\n",
    "class AnomalyDataset(Dataset):\n",
    "    \"\"\"Dataset para cargar im√°genes de entrenamiento y prueba\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, labels=None, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            return image, self.labels[idx]\n",
    "        return image\n",
    "\n",
    "\n",
    "def load_dataset_paths(category_path, split='train', only_good=True):\n",
    "    \"\"\"Carga las rutas de las im√°genes del dataset\"\"\"\n",
    "    paths = []\n",
    "    labels = []\n",
    "    split_path = os.path.join(category_path, split)\n",
    "    \n",
    "    if split == 'train' and only_good:\n",
    "        # Solo im√°genes 'good' en entrenamiento\n",
    "        good_path = os.path.join(split_path, 'good')\n",
    "        if os.path.exists(good_path):\n",
    "            for img_file in os.listdir(good_path):\n",
    "                if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    paths.append(os.path.join(good_path, img_file))\n",
    "                    labels.append(0)  # Se actualizar√° con el √≠ndice de categor√≠a\n",
    "    else:\n",
    "        # En test, cargar todas las clases (good y anomal√≠as)\n",
    "        if os.path.exists(split_path):\n",
    "            for class_name in os.listdir(split_path):\n",
    "                class_path = os.path.join(split_path, class_name)\n",
    "                if os.path.isdir(class_path):\n",
    "                    for img_file in os.listdir(class_path):\n",
    "                        if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                            paths.append(os.path.join(class_path, img_file))\n",
    "                            # Label: 0 para 'good', 1 para anomal√≠as\n",
    "                            labels.append(0 if class_name == 'good' else 1)\n",
    "    \n",
    "    return paths, labels\n",
    "\n",
    "\n",
    "class MVTecDataModule(pl.LightningDataModule):\n",
    "    \"\"\"DataModule para MVTec AD con m√∫ltiples categor√≠as\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, categories, image_size=128, batch_size=32, \n",
    "                 num_workers=2, train_split=0.8):\n",
    "        super().__init__()\n",
    "        self.dataset_path = dataset_path\n",
    "        self.categories = categories\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.train_split = train_split\n",
    "        \n",
    "        # Transformaciones (normalizaci√≥n a [-1, 1] para compatibilidad con Tanh)\n",
    "        self.train_transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # [-1, 1]\n",
    "        ])\n",
    "        \n",
    "        self.val_transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        \n",
    "        self.test_transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        \n",
    "        self.train_paths = []\n",
    "        self.train_labels = []\n",
    "        self.val_paths = []\n",
    "        self.val_labels = []\n",
    "        self.test_paths = []\n",
    "        self.test_labels = []\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"Carga las rutas de las im√°genes para todas las categor√≠as\"\"\"\n",
    "        \"\"\"Carga las rutas de las im√°genes para todas las categor√≠as\"\"\"\n",
    "        # Validar que el dataset_path existe\n",
    "        if not os.path.exists(self.dataset_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"‚ùå ERROR: No se encontr√≥ el dataset en: {self.dataset_path}\\n\"\n",
    "                f\"   Por favor, verifica la ruta del dataset.\"\n",
    "            )\n",
    "        \n",
    "        all_train_paths = []\n",
    "        all_train_labels = []\n",
    "        all_test_paths = []\n",
    "        all_test_labels = []\n",
    "        \n",
    "        # Cargar datos de todas las categor√≠as\n",
    "        for cat_idx, category in enumerate(self.categories):\n",
    "            category_path = os.path.join(self.dataset_path, category)\n",
    "            \n",
    "            # Validar que la categor√≠a existe\n",
    "            if not os.path.exists(category_path):\n",
    "                raise FileNotFoundError(\n",
    "                    f\"‚ùå ERROR: Categor√≠a '{category}' no encontrada en: {category_path}\"\n",
    "                )\n",
    "            \n",
    "            # Entrenamiento (solo 'good')\n",
    "            train_paths, _ = load_dataset_paths(category_path, split='train', only_good=True)\n",
    "            if len(train_paths) == 0:\n",
    "                raise ValueError(\n",
    "                    f\"‚ùå ERROR: No se encontraron im√°genes de entrenamiento para la categor√≠a '{category}'\\n\"\n",
    "                    f\"   Ruta esperada: {os.path.join(category_path, 'train', 'good')}\"\n",
    "                )\n",
    "            # Asignar label de categor√≠a\n",
    "            train_labels = [cat_idx] * len(train_paths)\n",
    "            all_train_paths.extend(train_paths)\n",
    "            all_train_labels.extend(train_labels)\n",
    "            \n",
    "            # Prueba (todas las clases)\n",
    "            test_paths, test_labels = load_dataset_paths(category_path, split='test', only_good=False)\n",
    "            if len(test_paths) == 0:\n",
    "                raise ValueError(\n",
    "                    f\"‚ùå ERROR: No se encontraron im√°genes de prueba para la categor√≠a '{category}'\\n\"\n",
    "                    f\"   Ruta esperada: {os.path.join(category_path, 'test')}\"\n",
    "                )\n",
    "            all_test_paths.extend(test_paths)\n",
    "            all_test_labels.extend(test_labels)\n",
    "        \n",
    "        # Dividir entrenamiento en train y validation\n",
    "        total_train = len(all_train_paths)\n",
    "        train_size = int(self.train_split * total_train)\n",
    "        val_size = total_train - train_size\n",
    "        \n",
    "        indices = torch.randperm(total_train).tolist()\n",
    "        train_indices = indices[:train_size]\n",
    "        val_indices = indices[train_size:]\n",
    "        \n",
    "        self.train_paths = [all_train_paths[i] for i in train_indices]\n",
    "        self.train_labels = [all_train_labels[i] for i in train_indices]\n",
    "        self.val_paths = [all_train_paths[i] for i in val_indices]\n",
    "        self.val_labels = [all_train_labels[i] for i in val_indices]\n",
    "        \n",
    "        self.test_paths = all_test_paths\n",
    "        self.test_labels = all_test_labels\n",
    "        \n",
    "        \n",
    "        # Validaci√≥n final: verificar que hay datos\n",
    "        if len(self.train_paths) == 0:\n",
    "            raise ValueError(\"‚ùå ERROR: No se encontraron im√°genes de entrenamiento\")\n",
    "        if len(self.val_paths) == 0:\n",
    "            raise ValueError(\"‚ùå ERROR: No se encontraron im√°genes de validaci√≥n\")\n",
    "        if len(self.test_paths) == 0:\n",
    "            raise ValueError(\"‚ùå ERROR: No se encontraron im√°genes de prueba\")\n",
    "        \n",
    "        print(f\"Train: {len(self.train_paths)} im√°genes\")\n",
    "        print(f\"Validation: {len(self.val_paths)} im√°genes\")\n",
    "        print(f\"Test: {len(self.test_paths)} im√°genes\")\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataset = AnomalyDataset(self.train_paths, labels=self.train_labels, transform=self.train_transform)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        dataset = AnomalyDataset(self.val_paths, labels=self.val_labels, transform=self.val_transform)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        dataset = AnomalyDataset(self.test_paths, labels=self.test_labels, transform=self.test_transform)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "# Crear DataModule\n",
    "data_module = MVTecDataModule(\n",
    "    dataset_path=DATASET_PATH,\n",
    "    categories=CATEGORIES,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    train_split=0.8\n",
    ")\n",
    "\n",
    "# Setup (cargar datos)\n",
    "data_module.setup()\n",
    "\n",
    "try:\n",
    "    data_module.setup()\n",
    "    print(\"‚úì DataModule creado y configurado correctamente\")\n",
    "    \n",
    "    # Validaci√≥n adicional: verificar que hay datos\n",
    "    if len(data_module.train_paths) == 0:\n",
    "        raise ValueError(\"‚ùå ERROR: No se encontraron im√°genes de entrenamiento\")\n",
    "    if len(data_module.val_paths) == 0:\n",
    "        raise ValueError(\"‚ùå ERROR: No se encontraron im√°genes de validaci√≥n\")\n",
    "    if len(data_module.test_paths) == 0:\n",
    "        raise ValueError(\"‚ùå ERROR: No se encontraron im√°genes de prueba\")\n",
    "    \n",
    "    print(f\"  ‚úì Datos cargados correctamente:\")\n",
    "    print(f\"    - Entrenamiento: {len(data_module.train_paths)} im√°genes\")\n",
    "    print(f\"    - Validaci√≥n: {len(data_module.val_paths)} im√°genes\")\n",
    "    print(f\"    - Prueba: {len(data_module.test_paths)} im√°genes\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR al configurar DataModule: {e}\")\n",
    "    print(f\"   Por favor, verifica:\")\n",
    "    print(f\"   1. Que el dataset est√© en: {DATASET_PATH}\")\n",
    "    print(f\"   2. Que todas las categor√≠as existan: {CATEGORIES}\")\n",
    "    print(f\"   3. Que cada categor√≠a tenga carpetas 'train/good' y 'test'\")\n",
    "    raise\n",
    "\n",
    "print(\"‚úì DataModule creado y configurado correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento de Modelos\n",
    "\n",
    "En esta secci√≥n se entrenan los tres modelos:\n",
    "- **Modelo A**: CNN clasificador desde cero\n",
    "- **Modelo B**: CNN clasificador con destilaci√≥n\n",
    "- **Modelo C**: Autoencoder U-Net\n",
    "\n",
    "**Importante**: \n",
    "- Cada modelo debe entrenarse con al menos 3 configuraciones diferentes de hiperpar√°metros\n",
    "- Se usa EarlyStopping para evitar overfitting\n",
    "- Todos los modelos se entrenan solo con datos sin defectos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n auxiliar para entrenar un modelo\n",
    "def train_model(model_type, model_config, trainer_config, logger_config, experiment_name):\n",
    "    \"\"\"\n",
    "    Entrena un modelo con la configuraci√≥n especificada\n",
    "    \n",
    "    Args:\n",
    "        model_type: \"cnn_scratch\", \"cnn_distilled\", o \"unet\"\n",
    "        model_config: Configuraci√≥n del modelo\n",
    "        trainer_config: Configuraci√≥n del entrenamiento\n",
    "        logger_config: Configuraci√≥n del logger\n",
    "        experiment_name: Nombre del experimento para WandB\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ENTRENANDO: {experiment_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Separar par√°metros del Lightning module de los del Trainer\n",
    "    # Par√°metros que van al Lightning module\n",
    "    lightning_params = {\n",
    "        'learning_rate': trainer_config.get('learning_rate', 0.001),\n",
    "        'weight_decay': trainer_config.get('weight_decay', 1e-5),\n",
    "        'scheduler_config': trainer_config.get('scheduler_config', {\"name\": \"step\", \"step_size\": 15, \"gamma\": 0.5})\n",
    "    }\n",
    "    \n",
    "    # Para modelos con destilaci√≥n\n",
    "    if 'distillation_config' in trainer_config:\n",
    "        lightning_params['distillation_config'] = trainer_config['distillation_config']\n",
    "    \n",
    "    # Crear modelo base\n",
    "    if model_type == \"cnn_scratch\":\n",
    "        base_model = CNNClassifier(\n",
    "            num_classes=len(CATEGORIES),\n",
    "            model_type=\"scratch\",\n",
    "            **model_config\n",
    "        )\n",
    "        lightning_model = CNNClassifierLightning(\n",
    "            model=base_model,\n",
    "            num_classes=len(CATEGORIES),\n",
    "            model_type=\"scratch\",\n",
    "            **lightning_params\n",
    "        )\n",
    "    elif model_type == \"cnn_distilled\":\n",
    "        base_model = CNNClassifier(\n",
    "            num_classes=len(CATEGORIES),\n",
    "            model_type=\"distilled\",\n",
    "            **model_config\n",
    "        )\n",
    "        lightning_model = CNNClassifierLightning(\n",
    "            model=base_model,\n",
    "            num_classes=len(CATEGORIES),\n",
    "            model_type=\"distilled\",\n",
    "            **lightning_params\n",
    "        )\n",
    "    elif model_type == \"unet\":\n",
    "        base_model = UNetAutoencoder(**model_config)\n",
    "        # Para AutoencoderLightning, tambi√©n necesitamos loss_function\n",
    "        autoencoder_params = {\n",
    "            'learning_rate': trainer_config.get('learning_rate', 0.001),\n",
    "            'loss_function': trainer_config.get('loss_function', 'L2'),\n",
    "            'scheduler_config': trainer_config.get('scheduler_config', {\"name\": \"step\", \"step_size\": 15, \"gamma\": 0.5})\n",
    "        }\n",
    "        lightning_model = AutoencoderLightning(\n",
    "            model=base_model,\n",
    "            **autoencoder_params\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Tipo de modelo no reconocido: {model_type}\")\n",
    "    \n",
    "    # Configurar logger\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=logger_config.get(\"project\", \"proyecto-ii-anomaly-detection\"),\n",
    "        name=experiment_name,\n",
    "        config={\n",
    "            \"model_type\": model_type,\n",
    "            \"model_config\": model_config,\n",
    "            \"trainer_config\": trainer_config\n",
    "        },\n",
    "        reinit=True\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=trainer_config.get(\"early_stopping\", {}).get(\"monitor\", \"val/loss\"),\n",
    "        mode=trainer_config.get(\"early_stopping\", {}).get(\"mode\", \"min\"),\n",
    "        patience=trainer_config.get(\"early_stopping\", {}).get(\"patience\", 10),\n",
    "        min_delta=trainer_config.get(\"early_stopping\", {}).get(\"min_delta\", 0.001)\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=trainer_config.get(\"checkpoint\", {}).get(\"monitor\", \"val/loss\"),\n",
    "        mode=trainer_config.get(\"checkpoint\", {}).get(\"mode\", \"min\"),\n",
    "        save_top_k=trainer_config.get(\"checkpoint\", {}).get(\"save_top_k\", 3),\n",
    "        save_last=True,\n",
    "        dirpath=os.path.join(DRIVE_BASE_PATH, 'checkpoints', experiment_name),\n",
    "        filename=f'{experiment_name}-{{epoch:02d}}-{{val/loss:.4f}}'\n",
    "    )\n",
    "    \n",
    "    lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "    \n",
    "    # Crear Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=trainer_config.get(\"max_epochs\", 50),\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=[early_stopping, checkpoint_callback, lr_monitor],\n",
    "        log_every_n_steps=10,\n",
    "        enable_progress_bar=True,\n",
    "        gradient_clip_val=trainer_config.get(\"gradient_clip_val\", 1.0),\n",
    "        accumulate_grad_batches=trainer_config.get(\"accumulate_grad_batches\", 1)\n",
    "    )\n",
    "    \n",
    "    # Entrenar\n",
    "    \n",
    "    # Validar que el data_module est√° configurado\n",
    "    try:\n",
    "        if not hasattr(data_module, 'train_paths') or len(data_module.train_paths) == 0:\n",
    "            raise ValueError(\"‚ùå ERROR: El data_module no est√° configurado correctamente. Ejecuta data_module.setup() primero.\")\n",
    "    except AttributeError:\n",
    "        raise ValueError(\"‚ùå ERROR: El data_module no est√° configurado. Ejecuta data_module.setup() primero.\")\n",
    "    \n",
    "    trainer.fit(lightning_model, data_module)\n",
    "    \n",
    "    # Evaluar\n",
    "    trainer.test(lightning_model, data_module)\n",
    "    \n",
    "    # Evaluar\n",
    "    try:\n",
    "        trainer.test(lightning_model, data_module)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Advertencia: Error durante la evaluaci√≥n: {e}\")\n",
    "        # Continuar aunque falle la evaluaci√≥n\n",
    "    \n",
    "    return lightning_model, trainer, wandb_logger\n",
    "\n",
    "print(\"‚úì Funci√≥n de entrenamiento definida\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Entrenamiento del Modelo A (CNN desde cero)\n",
    "\n",
    "Entrenar el modelo A con m√∫ltiples configuraciones de hiperpar√°metros (al menos 3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraciones de hiperpar√°metros para Modelo A (al menos 3)\n",
    "model_a_configs = [\n",
    "    {\n",
    "        \"model_config\": {\n",
    "            \"conv1_channels\": 64,\n",
    "            \"conv2_channels\": [64, 64],\n",
    "            \"conv3_channels\": [128, 128],\n",
    "            \"fc_hidden\": 512,\n",
    "            \"dropout\": 0.5,\n",
    "            \"embedding_dim\": 256\n",
    "        },\n",
    "        \"trainer_config\": {\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"weight_decay\": 1e-5,\n",
    "            \"max_epochs\": 50,\n",
    "            \"scheduler_config\": {\"name\": \"step\", \"step_size\": 15, \"gamma\": 0.5},\n",
    "            \"early_stopping\": {\"monitor\": \"val/loss\", \"mode\": \"min\", \"patience\": 10, \"min_delta\": 0.001},\n",
    "            \"checkpoint\": {\"monitor\": \"val/loss\", \"mode\": \"min\", \"save_top_k\": 3}\n",
    "        },\n",
    "        \"experiment_name\": \"model_a_config1\"\n",
    "    },\n",
    "    {\n",
    "        \"model_config\": {\n",
    "            \"conv1_channels\": 64,\n",
    "            \"conv2_channels\": [64, 64],\n",
    "            \"conv3_channels\": [128, 128],\n",
    "            \"fc_hidden\": 256,\n",
    "            \"dropout\": 0.3,\n",
    "            \"embedding_dim\": 128\n",
    "        },\n",
    "        \"trainer_config\": {\n",
    "            \"learning_rate\": 0.0005,\n",
    "            \"weight_decay\": 1e-4,\n",
    "            \"max_epochs\": 50,\n",
    "            \"scheduler_config\": {\"name\": \"cosine\", \"T_max\": 50},\n",
    "            \"early_stopping\": {\"monitor\": \"val/loss\", \"mode\": \"min\", \"patience\": 10, \"min_delta\": 0.001},\n",
    "            \"checkpoint\": {\"monitor\": \"val/loss\", \"mode\": \"min\", \"save_top_k\": 3}\n",
    "        },\n",
    "        \"experiment_name\": \"model_a_config2\"\n",
    "    },\n",
    "    {\n",
    "        \"model_config\": {\n",
    "            \"conv1_channels\": 64,\n",
    "            \"conv2_channels\": [64, 64],\n",
    "            \"conv3_channels\": [128, 128],\n",
    "            \"fc_hidden\": 1024,\n",
    "            \"dropout\": 0.7,\n",
    "            \"embedding_dim\": 512\n",
    "        },\n",
    "        \"trainer_config\": {\n",
    "            \"learning_rate\": 0.002,\n",
    "            \"weight_decay\": 1e-6,\n",
    "            \"max_epochs\": 50,\n",
    "            \"scheduler_config\": {\"name\": \"plateau\", \"factor\": 0.5, \"patience\": 5},\n",
    "            \"early_stopping\": {\"monitor\": \"val/loss\", \"mode\": \"min\", \"patience\": 10, \"min_delta\": 0.001},\n",
    "            \"checkpoint\": {\"monitor\": \"val/loss\", \"mode\": \"min\", \"save_top_k\": 3}\n",
    "        },\n",
    "        \"experiment_name\": \"model_a_config3\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Entrenar Modelo A con todas las configuraciones\n",
    "model_a_results = []\n",
    "for config in model_a_configs:\n",
    "    try:\n",
    "        model, trainer, logger = train_model(\n",
    "            model_type=\"cnn_scratch\",\n",
    "            model_config=config[\"model_config\"],\n",
    "            trainer_config=config[\"trainer_config\"],\n",
    "            logger_config={\"project\": \"proyecto-ii-anomaly-detection\"},\n",
    "            experiment_name=config[\"experiment_name\"]\n",
    "        )\n",
    "        model_a_results.append({\n",
    "            \"config\": config[\"experiment_name\"],\n",
    "            \"model\": model,\n",
    "            \"trainer\": trainer,\n",
    "            \"logger\": logger\n",
    "        })\n",
    "        print(f\"‚úì {config['experiment_name']} completado\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en {config['experiment_name']}: {e}\\n\")\n",
    "\n",
    "print(f\"‚úì Modelo A: {len(model_a_results)} configuraciones entrenadas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Entrenamiento del Modelo B (CNN con destilaci√≥n)\n",
    "\n",
    "Entrenar el modelo B con m√∫ltiples configuraciones de hiperpar√°metros (al menos 3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraciones de hiperpar√°metros para Modelo B (al menos 3)\n",
    "model_b_configs = [\n",
    "    {\n",
    "        \"model_config\": {\n",
    "            \"conv1_channels\": 64,\n",
    "            \"conv2_channels\": [64, 64],\n",
    "            \"conv3_channels\": [128, 128],\n",
    "            \"fc_hidden\": 512,\n",
    "            \"dropout\": 0.5,\n",
    "            \"embedding_dim\": 256\n",
    "        },\n",
    "        \"trainer_config\": {\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"weight_decay\": 1e-5,\n",
    "            \"max_epochs\": 50,\n",
    "            \"scheduler_config\": {\"name\": \"step\", \"step_size\": 15, \"gamma\": 0.5},\n",
    "            \"distillation_config\": {\"temperature\": 4.0, \"alpha\": 0.7},\n",
    "            \"early_stopping\": {\"monitor\": \"val/loss\", \"mode\": \"min\", \"patience\": 10, \"min_delta\": 0.001},\n",
    "            \"checkpoint\": {\"monitor\": \"val/loss\", \"mode\": \"min\", \"save_top_k\": 3}\n",
    "        },\n",
    "        \"experiment_name\": \"model_b_config1\"\n",
    "    },\n",
    "    {\n",
    "        \"model_config\": {\n",
    "            \"conv1_channels\": 64,\n",
    "            \"conv2_channels\": [64, 64],\n",
    "            \"conv3_channels\": [128, 128],\n",
    "            \"fc_hidden\": 256,\n",
    "            \"dropout\": 0.3,\n",
    "            \"embedding_dim\": 128\n",
    "        },\n",
    "        \"trainer_config\": {\n",
    "            \"learning_rate\": 0.0005,\n",
    "            \"weight_decay\": 1e-4,\n",
    "            \"max_epochs\": 50,\n",
    "            \"scheduler_config\": {\"name\": \"cosine\", \"T_max\": 50},\n",
    "            \"distillation_config\": {\"temperature\": 5.0, \"alpha\": 0.8},\n",
    "            \"early_stopping\": {\"monitor\": \"val/loss\", \"mode\": \"min\", \"patience\": 10, \"min_delta\": 0.001},\n",
    "            \"checkpoint\": {\"monitor\": \"val/loss\", \"mode\": \"min\", \"save_top_k\": 3}\n",
    "        },\n",
    "        \"experiment_name\": \"model_b_config2\"\n",
    "    },\n",
    "    {\n",
    "        \"model_config\": {\n",
    "            \"conv1_channels\": 64,\n",
    "            \"conv2_channels\": [64, 64],\n",
    "            \"conv3_channels\": [128, 128],\n",
    "            \"fc_hidden\": 1024,\n",
    "            \"dropout\": 0.7,\n",
    "            \"embedding_dim\": 512\n",
    "        },\n",
    "        \"trainer_config\": {\n",
    "            \"learning_rate\": 0.002,\n",
    "            \"weight_decay\": 1e-6,\n",
    "            \"max_epochs\": 50,\n",
    "            \"scheduler_config\": {\"name\": \"plateau\", \"factor\": 0.5, \"patience\": 5},\n",
    "            \"distillation_config\": {\"temperature\": 3.0, \"alpha\": 0.6},\n",
    "            \"early_stopping\": {\"monitor\": \"val/loss\", \"mode\": \"min\", \"patience\": 10, \"min_delta\": 0.001},\n",
    "            \"checkpoint\": {\"monitor\": \"val/loss\", \"mode\": \"min\", \"save_top_k\": 3}\n",
    "        },\n",
    "        \"experiment_name\": \"model_b_config3\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Entrenar Modelo B con todas las configuraciones\n",
    "model_b_results = []\n",
    "for config in model_b_configs:\n",
    "    try:\n",
    "        # Crear modelo con destilaci√≥n\n",
    "        base_model = CNNClassifier(\n",
    "            num_classes=len(CATEGORIES),\n",
    "            model_type=\"distilled\",\n",
    "            **config[\"model_config\"]\n",
    "        )\n",
    "        lightning_model = CNNClassifierLightning(\n",
    "            model=base_model,\n",
    "            num_classes=len(CATEGORIES),\n",
    "            model_type=\"distilled\",\n",
    "            distillation_config=config[\"trainer_config\"].get(\"distillation_config\", {}),\n",
    "            **{k: v for k, v in config[\"trainer_config\"].items() if k != \"distillation_config\"}\n",
    "        )\n",
    "        \n",
    "        # Configurar logger\n",
    "        wandb_logger = WandbLogger(\n",
    "            project=\"proyecto-ii-anomaly-detection\",\n",
    "            name=config[\"experiment_name\"],\n",
    "            config={\n",
    "                \"model_type\": \"cnn_distilled\",\n",
    "                \"model_config\": config[\"model_config\"],\n",
    "                \"trainer_config\": config[\"trainer_config\"]\n",
    "            },\n",
    "            reinit=True\n",
    "        )\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=config[\"trainer_config\"].get(\"early_stopping\", {}).get(\"monitor\", \"val/loss\"),\n",
    "            mode=config[\"trainer_config\"].get(\"early_stopping\", {}).get(\"mode\", \"min\"),\n",
    "            patience=config[\"trainer_config\"].get(\"early_stopping\", {}).get(\"patience\", 10),\n",
    "            min_delta=config[\"trainer_config\"].get(\"early_stopping\", {}).get(\"min_delta\", 0.001)\n",
    "        )\n",
    "        \n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            monitor=config[\"trainer_config\"].get(\"checkpoint\", {}).get(\"monitor\", \"val/loss\"),\n",
    "            mode=config[\"trainer_config\"].get(\"checkpoint\", {}).get(\"mode\", \"min\"),\n",
    "            save_top_k=config[\"trainer_config\"].get(\"checkpoint\", {}).get(\"save_top_k\", 3),\n",
    "            save_last=True,\n",
    "            dirpath=os.path.join(DRIVE_BASE_PATH, 'checkpoints', config[\"experiment_name\"]),\n",
    "            filename=f'{config[\"experiment_name\"]}-{{epoch:02d}}-{{val/loss:.4f}}'\n",
    "        )\n",
    "        \n",
    "        lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "        \n",
    "        # Crear Trainer\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=config[\"trainer_config\"].get(\"max_epochs\", 50),\n",
    "            accelerator='auto',\n",
    "            devices=1,\n",
    "            logger=wandb_logger,\n",
    "            callbacks=[early_stopping, checkpoint_callback, lr_monitor],\n",
    "            log_every_n_steps=10,\n",
    "            enable_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        # Entrenar\n",
    "        trainer.fit(lightning_model, data_module)\n",
    "        trainer.test(lightning_model, data_module)\n",
    "        \n",
    "        model_b_results.append({\n",
    "            \"config\": config[\"experiment_name\"],\n",
    "            \"model\": lightning_model,\n",
    "            \"trainer\": trainer,\n",
    "            \"logger\": wandb_logger\n",
    "        })\n",
    "        print(f\"‚úì {config['experiment_name']} completado\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en {config['experiment_name']}: {e}\\n\")\n",
    "\n",
    "print(f\"‚úì Modelo B: {len(model_b_results)} configuraciones entrenadas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraciones de hiperpar√°metros para Modelo C (al menos 3)\n",
    "model_c_configs = [\n",
    "    {\n",
    "        \"model_config\": {\n",
    "            \"input_channels\": 3,\n",
    "            \"latent_dim\": 128,\n",
    "            \"encoder_channels\": [64, 128, 256, 512],\n",
    "            \"decoder_channels\": [512, 256, 128, 64],\n",
    "            \"embedding_dim\": 128\n",
    "        },\n",
    "        \"trainer_config\": {\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"loss_function\": \"L2\",\n",
    "            \"max_epochs\": 50,\n",
    "            \"scheduler_config\": {\"name\": \"step\", \"step_size\": 15, \"gamma\": 0.5},\n",
    "            \"early_stopping\": {\"monitor\": \"val/loss\", \"mode\": \"min\", \"patience\": 10, \"min_delta\": 0.001},\n",
    "            \"checkpoint\": {\"monitor\": \"val/loss\", \"mode\": \"min\", \"save_top_k\": 3}\n",
    "        },\n",
    "        \"experiment_name\": \"model_c_config1_l2\"\n",
    "    },\n",
    "    {\n",
    "        \"model_config\": {\n",
    "            \"input_channels\": 3,\n",
    "            \"latent_dim\": 256,\n",
    "            \"encoder_channels\": [64, 128, 256, 512],\n",
    "            \"decoder_channels\": [512, 256, 128, 64],\n",
    "            \"embedding_dim\": 256\n",
    "        },\n",
    "        \"trainer_config\": {\n",
    "            \"learning_rate\": 0.0005,\n",
    "            \"loss_function\": \"SSIM_L1\",\n",
    "            \"max_epochs\": 50,\n",
    "            \"scheduler_config\": {\"name\": \"cosine\", \"T_max\": 50},\n",
    "            \"early_stopping\": {\"monitor\": \"val/loss\", \"mode\": \"min\", \"patience\": 10, \"min_delta\": 0.001},\n",
    "            \"checkpoint\": {\"monitor\": \"val/loss\", \"mode\": \"min\", \"save_top_k\": 3}\n",
    "        },\n",
    "        \"experiment_name\": \"model_c_config2_ssim_l1\"\n",
    "    },\n",
    "    {\n",
    "        \"model_config\": {\n",
    "            \"input_channels\": 3,\n",
    "            \"latent_dim\": 64,\n",
    "            \"encoder_channels\": [32, 64, 128, 256],\n",
    "            \"decoder_channels\": [256, 128, 64, 32],\n",
    "            \"embedding_dim\": 64\n",
    "        },\n",
    "        \"trainer_config\": {\n",
    "            \"learning_rate\": 0.002,\n",
    "            \"loss_function\": \"L1\",\n",
    "            \"max_epochs\": 50,\n",
    "            \"scheduler_config\": {\"name\": \"plateau\", \"factor\": 0.5, \"patience\": 5},\n",
    "            \"early_stopping\": {\"monitor\": \"val/loss\", \"mode\": \"min\", \"patience\": 10, \"min_delta\": 0.001},\n",
    "            \"checkpoint\": {\"monitor\": \"val/loss\", \"mode\": \"min\", \"save_top_k\": 3}\n",
    "        },\n",
    "        \"experiment_name\": \"model_c_config3_l1\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Entrenar Modelo C con todas las configuraciones\n",
    "model_c_results = []\n",
    "for config in model_c_configs:\n",
    "    try:\n",
    "        model, trainer, logger = train_model(\n",
    "            model_type=\"unet\",\n",
    "            model_config=config[\"model_config\"],\n",
    "            trainer_config=config[\"trainer_config\"],\n",
    "            logger_config={\"project\": \"proyecto-ii-anomaly-detection\"},\n",
    "            experiment_name=config[\"experiment_name\"]\n",
    "        )\n",
    "        model_c_results.append({\n",
    "            \"config\": config[\"experiment_name\"],\n",
    "            \"model\": model,\n",
    "            \"trainer\": trainer,\n",
    "            \"logger\": logger\n",
    "        })\n",
    "        print(f\"‚úì {config['experiment_name']} completado\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en {config['experiment_name']}: {e}\\n\")\n",
    "\n",
    "print(f\"‚úì Modelo C: {len(model_c_results)} configuraciones entrenadas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluaci√≥n de Anomal√≠as\n",
    "\n",
    "Una vez entrenados los modelos, se calculan las representaciones latentes (embeddings) de las im√°genes del conjunto de **validaci√≥n o entrenamiento** (solo datos normales) para estimar la distribuci√≥n normal. Luego se aplica esa distribuci√≥n al conjunto de prueba para identificar datos an√≥malos.\n",
    "\n",
    "### Proceso de Evaluaci√≥n:\n",
    "\n",
    "1. **Estimaci√≥n de la distribuci√≥n normal**: \n",
    "   - Extraer embeddings del conjunto de validaci√≥n/entrenamiento (solo datos normales)\n",
    "   - Calcular media Œº = (1/N) Œ£ z_i\n",
    "   - Calcular matriz de covarianza Œ£ = (1/(N-1)) Œ£ (z_i - Œº)(z_i - Œº)^T\n",
    "   - Modelar como distribuci√≥n gaussiana multivariada N(Œº, Œ£)\n",
    "\n",
    "2. **C√°lculo de distancias en conjunto de prueba**:\n",
    "   - Extraer embeddings del conjunto de prueba\n",
    "   - Calcular distancias usando la distribuci√≥n normal estimada\n",
    "   - Determinar umbral usando percentil de distancias normales de validaci√≥n\n",
    "\n",
    "**M√©todos de evaluaci√≥n**:\n",
    "- **Distancia de Mahalanobis**: d = sqrt((z - Œº)^T Œ£^(-1) (z - Œº))\n",
    "- **Distancia Euclidiana**: d = ||z - Œº||\n",
    "- **Reconstruction Loss**: Error de reconstrucci√≥n para autoencoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de evaluaci√≥n - Copiamos el contenido de evaluation.py\n",
    "\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.linalg import inv\n",
    "\n",
    "def calculate_mahalanobis_distance(embeddings, mean, cov):\n",
    "    \"\"\"\n",
    "    Calcula la distancia de Mahalanobis para cada embedding.\n",
    "    \n",
    "    Distancia de Mahalanobis: d = sqrt((z - Œº)^T Œ£^(-1) (z - Œº))\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Array numpy de shape (N, d) con embeddings\n",
    "        mean: Vector media de shape (d,)\n",
    "        cov: Matriz de covarianza de shape (d, d)\n",
    "    \n",
    "    Returns:\n",
    "        distances: Array numpy de shape (N,) con distancias de Mahalanobis\n",
    "    \"\"\"\n",
    "    if embeddings is None or len(embeddings) == 0:\n",
    "        raise ValueError(\"‚ùå ERROR: embeddings no puede estar vac√≠o\")\n",
    "    if mean is None:\n",
    "        raise ValueError(\"‚ùå ERROR: mean no puede ser None\")\n",
    "    if cov is None:\n",
    "        raise ValueError(\"‚ùå ERROR: cov no puede ser None\")\n",
    "    \n",
    "    try:\n",
    "        # Regularizaci√≥n para evitar singularidad\n",
    "        cov_reg = cov + np.eye(cov.shape[0]) * 1e-6\n",
    "        \n",
    "        # Calcular inversa de la matriz de covarianza\n",
    "        try:\n",
    "            cov_inv = inv(cov_reg)\n",
    "        except np.linalg.LinAlgError as e:\n",
    "            raise ValueError(f\"‚ùå ERROR: No se pudo invertir la matriz de covarianza: {e}\") from e\n",
    "        \n",
    "        # Calcular distancias\n",
    "        distances = []\n",
    "        for emb in embeddings:\n",
    "            diff = emb - mean\n",
    "            try:\n",
    "                dist = np.sqrt(diff @ cov_inv @ diff.T)\n",
    "                if np.isnan(dist) or np.isinf(dist):\n",
    "                    print(f\"  ‚ö†Ô∏è Advertencia: Distancia inv√°lida detectada, usando 0\")\n",
    "                    dist = 0.0\n",
    "                distances.append(dist)\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è Advertencia: Error calculando distancia: {e}, usando 0\")\n",
    "                distances.append(0.0)\n",
    "        \n",
    "        return np.array(distances)\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"‚ùå ERROR al calcular distancias de Mahalanobis: {e}\") from e\n",
    "\n",
    "\n",
    "def extract_embeddings(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Extrae embeddings de un dataloader usando el modelo entrenado\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        dataloader: DataLoader con las im√°genes\n",
    "        device: Dispositivo (cuda/cpu)\n",
    "    \n",
    "    Returns:\n",
    "        all_embeddings: Array numpy con todos los embeddings\n",
    "        all_labels: Array numpy con todas las etiquetas (o None)\n",
    "        all_reconstructions: Lista de reconstrucciones (para autoencoders)\n",
    "        all_originals: Lista de im√°genes originales (para autoencoders)\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        raise ValueError(\"‚ùå ERROR: El modelo no puede ser None\")\n",
    "    \n",
    "    if dataloader is None:\n",
    "        raise ValueError(\"‚ùå ERROR: El dataloader no puede ser None\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    all_reconstructions = []\n",
    "    all_originals = []\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(dataloader):\n",
    "                try:\n",
    "                    if isinstance(batch, tuple):\n",
    "                        images, labels = batch\n",
    "                    else:\n",
    "                        images = batch\n",
    "                        labels = None\n",
    "                    \n",
    "                    if images is None or images.numel() == 0:\n",
    "                        print(f\"  ‚ö†Ô∏è Advertencia: Batch {batch_idx} est√° vac√≠o, saltando...\")\n",
    "                        continue\n",
    "                    \n",
    "                    images = images.to(device)\n",
    "                    \n",
    "                    # Extraer embeddings\n",
    "                    try:\n",
    "                        if hasattr(model, 'get_embedding'):\n",
    "                            embeddings = model.get_embedding(images)\n",
    "                        elif hasattr(model, 'model') and hasattr(model.model, 'get_embedding'):\n",
    "                            embeddings = model.model.get_embedding(images)\n",
    "                        else:\n",
    "                            if hasattr(model, 'model'):\n",
    "                                logits, embeddings = model.model(images)\n",
    "                            else:\n",
    "                                logits, embeddings = model(images)\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚ùå Error extrayendo embeddings del batch {batch_idx}: {e}\")\n",
    "                        raise\n",
    "                    \n",
    "                    all_embeddings.append(embeddings.cpu().numpy())\n",
    "                    \n",
    "                    if labels is not None:\n",
    "                        all_labels.append(labels.cpu().numpy())\n",
    "                    \n",
    "                    # Para autoencoders, guardar reconstrucciones\n",
    "                    if hasattr(model, 'model') and hasattr(model.model, 'forward'):\n",
    "                        try:\n",
    "                            reconstructions = model.model(images)\n",
    "                            all_reconstructions.append(reconstructions.cpu().numpy())\n",
    "                            all_originals.append(images.cpu().numpy())\n",
    "                        except Exception as e:\n",
    "                            # Si falla la reconstrucci√≥n, continuar sin ella\n",
    "                            pass\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ö†Ô∏è Error procesando batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        if len(all_embeddings) == 0:\n",
    "            raise ValueError(\"‚ùå ERROR: No se pudieron extraer embeddings. El dataloader podr√≠a estar vac√≠o.\")\n",
    "        \n",
    "        all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "        all_labels = np.concatenate(all_labels, axis=0) if all_labels else None\n",
    "        \n",
    "        return all_embeddings, all_labels, all_reconstructions, all_originals\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"‚ùå ERROR al extraer embeddings: {e}\") from e\n",
    "\n",
    "\n",
    "def estimate_normal_distribution(normal_embeddings):\n",
    "    \"\"\"\n",
    "    Estima la distribuci√≥n normal (gaussiana multivariada) a partir de embeddings normales.\n",
    "    \n",
    "    Calcula la media Œº y la matriz de covarianza Œ£:\n",
    "    Œº = (1/N) Œ£ z_i\n",
    "    Œ£ = (1/(N-1)) Œ£ (z_i - Œº)(z_i - Œº)^T\n",
    "    \n",
    "    Args:\n",
    "        normal_embeddings: Array numpy de shape (N, d) con embeddings normales\n",
    "    \n",
    "    Returns:\n",
    "        mean: Vector media de shape (d,)\n",
    "        cov: Matriz de covarianza de shape (d, d)\n",
    "    \"\"\"\n",
    "    if normal_embeddings is None or len(normal_embeddings) == 0:\n",
    "        raise ValueError(\"‚ùå ERROR: normal_embeddings no puede estar vac√≠o\")\n",
    "    \n",
    "    if len(normal_embeddings.shape) != 2:\n",
    "        raise ValueError(f\"‚ùå ERROR: normal_embeddings debe ser 2D, pero tiene shape {normal_embeddings.shape}\")\n",
    "    \n",
    "    if len(normal_embeddings) < 2:\n",
    "        raise ValueError(f\"‚ùå ERROR: Se necesitan al menos 2 muestras para calcular covarianza, pero hay {len(normal_embeddings)}\")\n",
    "    \n",
    "    try:\n",
    "        # Media: Œº = (1/N) Œ£ z_i\n",
    "        mean = np.mean(normal_embeddings, axis=0)\n",
    "        \n",
    "        # Matriz de covarianza: Œ£ = (1/(N-1)) Œ£ (z_i - Œº)(z_i - Œº)^T\n",
    "        # np.cov usa (N-1) como denominador por defecto\n",
    "        cov = np.cov(normal_embeddings.T)\n",
    "        \n",
    "        # Validar que la matriz de covarianza es v√°lida\n",
    "        if np.any(np.isnan(cov)) or np.any(np.isinf(cov)):\n",
    "            raise ValueError(\"‚ùå ERROR: La matriz de covarianza contiene NaN o Inf\")\n",
    "        \n",
    "        return mean, cov\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"‚ùå ERROR al estimar distribuci√≥n normal: {e}\") from e\n",
    "\n",
    "\n",
    "def evaluate_anomaly_detection(model, normal_dataloader, test_dataloader, device, method=\"mahalanobis\", percentile=95):\n",
    "    \"\"\"\n",
    "    Eval√∫a la detecci√≥n de anomal√≠as siguiendo el proceso correcto:\n",
    "    1. Estima la distribuci√≥n normal usando el conjunto de validaci√≥n/entrenamiento (solo datos normales)\n",
    "    2. Aplica esa distribuci√≥n al conjunto de prueba para detectar anomal√≠as\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        normal_dataloader: Dataloader con datos normales (validaci√≥n o entrenamiento)\n",
    "        test_dataloader: Dataloader con datos de prueba (normales y an√≥malos)\n",
    "        device: Dispositivo (cuda/cpu)\n",
    "        method: M√©todo de evaluaci√≥n (\"mahalanobis\", \"euclidean\", \"reconstruction_loss\")\n",
    "        percentile: Percentil para determinar el umbral (default: 95)\n",
    "    \n",
    "    Returns:\n",
    "        results: Diccionario con resultados de la evaluaci√≥n\n",
    "    \"\"\"\n",
    "    # Validaci√≥n de par√°metros\n",
    "    if model is None:\n",
    "        raise ValueError(\"‚ùå ERROR: El modelo no puede ser None\")\n",
    "    if normal_dataloader is None:\n",
    "        raise ValueError(\"‚ùå ERROR: normal_dataloader no puede ser None\")\n",
    "    if test_dataloader is None:\n",
    "        raise ValueError(\"‚ùå ERROR: test_dataloader no puede ser None\")\n",
    "    if method not in [\"mahalanobis\", \"euclidean\", \"reconstruction_loss\"]:\n",
    "        raise ValueError(f\"‚ùå ERROR: M√©todo '{method}' no reconocido. Use: 'mahalanobis', 'euclidean', o 'reconstruction_loss'\")\n",
    "    if not (0 < percentile <= 100):\n",
    "        raise ValueError(f\"‚ùå ERROR: Percentil debe estar entre 0 y 100, pero es {percentile}\")\n",
    "    \n",
    "    try:\n",
    "        # Paso 1: Extraer embeddings del conjunto normal (validaci√≥n/entrenamiento)\n",
    "        print(\"üìä Estimando distribuci√≥n normal a partir del conjunto de validaci√≥n/entrenamiento...\")\n",
    "        normal_embeddings, _, normal_reconstructions, normal_originals = extract_embeddings(\n",
    "            model, normal_dataloader, device\n",
    "        )\n",
    "        \n",
    "        if len(normal_embeddings) == 0:\n",
    "            raise ValueError(\"‚ùå ERROR: No se pudieron extraer embeddings del conjunto normal\")\n",
    "        \n",
    "        # Estimar distribuci√≥n normal: Œº y Œ£\n",
    "        mean, cov = estimate_normal_distribution(normal_embeddings)\n",
    "        print(f\"  ‚úì Media (Œº) calculada: shape {mean.shape}\")\n",
    "        print(f\"  ‚úì Matriz de covarianza (Œ£) calculada: shape {cov.shape}\")\n",
    "    \n",
    "        # Paso 2: Extraer embeddings del conjunto de prueba\n",
    "        print(\"\\nüìä Extrayendo embeddings del conjunto de prueba...\")\n",
    "        test_embeddings, test_labels, test_reconstructions, test_originals = extract_embeddings(\n",
    "            model, test_dataloader, device\n",
    "        )\n",
    "        \n",
    "        if len(test_embeddings) == 0:\n",
    "            raise ValueError(\"‚ùå ERROR: No se pudieron extraer embeddings del conjunto de prueba\")\n",
    "        \n",
    "        # Separar embeddings normales y an√≥malos del conjunto de prueba\n",
    "        if test_labels is not None:\n",
    "            test_normal_embeddings = test_embeddings[test_labels == 0]\n",
    "            test_anomaly_embeddings = test_embeddings[test_labels == 1]\n",
    "            \n",
    "            # Calcular distancias usando la distribuci√≥n normal estimada\n",
    "            try:\n",
    "                if method == \"mahalanobis\":\n",
    "                    # Distancia de Mahalanobis: d = sqrt((z - Œº)^T Œ£^(-1) (z - Œº))\n",
    "                    if len(test_normal_embeddings) > 0:\n",
    "                        test_normal_distances = calculate_mahalanobis_distance(test_normal_embeddings, mean, cov)\n",
    "                    else:\n",
    "                        test_normal_distances = np.array([])\n",
    "                    \n",
    "                    if len(test_anomaly_embeddings) > 0:\n",
    "                        test_anomaly_distances = calculate_mahalanobis_distance(test_anomaly_embeddings, mean, cov)\n",
    "                    else:\n",
    "                        test_anomaly_distances = np.array([])\n",
    "                        \n",
    "                elif method == \"euclidean\":\n",
    "                    # Distancia euclidiana: d = ||z - Œº||\n",
    "                    if len(test_normal_embeddings) > 0:\n",
    "                        test_normal_distances = np.linalg.norm(test_normal_embeddings - mean, axis=1)\n",
    "                    else:\n",
    "                        test_normal_distances = np.array([])\n",
    "                    \n",
    "                    if len(test_anomaly_embeddings) > 0:\n",
    "                        test_anomaly_distances = np.linalg.norm(test_anomaly_embeddings - mean, axis=1)\n",
    "                    else:\n",
    "                        test_anomaly_distances = np.array([])\n",
    "                        \n",
    "                elif method == \"reconstruction_loss\":\n",
    "                    if len(test_reconstructions) > 0 and len(test_originals) > 0:\n",
    "                        test_reconstructions = np.concatenate(test_reconstructions, axis=0)\n",
    "                        test_originals = np.concatenate(test_originals, axis=0)\n",
    "                        test_normal_recon = test_reconstructions[test_labels == 0]\n",
    "                        test_normal_orig = test_originals[test_labels == 0]\n",
    "                        test_anomaly_recon = test_reconstructions[test_labels == 1]\n",
    "                        test_anomaly_orig = test_originals[test_labels == 1]\n",
    "                        \n",
    "                        if len(test_normal_recon) > 0:\n",
    "                            test_normal_distances = np.mean((test_normal_recon - test_normal_orig) ** 2, axis=(1, 2, 3))\n",
    "                        else:\n",
    "                            test_normal_distances = np.array([])\n",
    "                        \n",
    "                        if len(test_anomaly_recon) > 0:\n",
    "                            test_anomaly_distances = np.mean((test_anomaly_recon - test_anomaly_orig) ** 2, axis=(1, 2, 3))\n",
    "                        else:\n",
    "                            test_anomaly_distances = np.array([])\n",
    "                    else:\n",
    "                        raise ValueError(\"‚ùå ERROR: Reconstruction loss requiere reconstrucciones. Aseg√∫rate de usar un modelo autoencoder.\")\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"‚ùå ERROR al calcular distancias con m√©todo '{method}': {e}\") from e\n",
    "        \n",
    "            # Determinar umbral usando percentil de las distancias normales del conjunto de validaci√≥n\n",
    "            # Primero calculamos distancias de los datos normales de validaci√≥n\n",
    "            try:\n",
    "                if method == \"mahalanobis\":\n",
    "                    validation_normal_distances = calculate_mahalanobis_distance(normal_embeddings, mean, cov)\n",
    "                elif method == \"euclidean\":\n",
    "                    validation_normal_distances = np.linalg.norm(normal_embeddings - mean, axis=1)\n",
    "                elif method == \"reconstruction_loss\":\n",
    "                    if len(normal_reconstructions) > 0 and len(normal_originals) > 0:\n",
    "                        normal_reconstructions = np.concatenate(normal_reconstructions, axis=0)\n",
    "                        normal_originals = np.concatenate(normal_originals, axis=0)\n",
    "                        validation_normal_distances = np.mean((normal_reconstructions - normal_originals) ** 2, axis=(1, 2, 3))\n",
    "                    else:\n",
    "                        raise ValueError(\"‚ùå ERROR: Reconstruction loss requiere reconstrucciones del conjunto normal\")\n",
    "                \n",
    "                if len(validation_normal_distances) == 0:\n",
    "                    raise ValueError(\"‚ùå ERROR: No se pudieron calcular distancias de validaci√≥n\")\n",
    "                \n",
    "                # Umbral basado en percentil de distancias normales de validaci√≥n\n",
    "                threshold = np.percentile(validation_normal_distances, percentile)\n",
    "                print(f\"\\nüìè Umbral calculado (percentil {percentile}): {threshold:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"‚ùå ERROR al calcular umbral: {e}\") from e\n",
    "            \n",
    "            # Clasificar datos de prueba\n",
    "            if len(test_normal_distances) == 0 and len(test_anomaly_distances) == 0:\n",
    "                raise ValueError(\"‚ùå ERROR: No hay distancias para clasificar\")\n",
    "            \n",
    "            all_distances = np.concatenate([test_normal_distances, test_anomaly_distances])\n",
    "            predictions = (all_distances > threshold).astype(int)\n",
    "            true_labels = np.concatenate([np.zeros_like(test_normal_distances), np.ones_like(test_anomaly_distances)])\n",
    "            \n",
    "            # Calcular m√©tricas\n",
    "            try:\n",
    "                auc_roc = roc_auc_score(true_labels, all_distances)\n",
    "                auc_pr = average_precision_score(true_labels, all_distances)\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è Advertencia: Error al calcular m√©tricas: {e}\")\n",
    "                auc_roc = 0.0\n",
    "                auc_pr = 0.0\n",
    "        \n",
    "            results = {\n",
    "                'method': method,\n",
    "                'threshold': threshold,\n",
    "                'auc_roc': auc_roc,\n",
    "                'auc_pr': auc_pr,\n",
    "                'normal_distances': test_normal_distances,\n",
    "                'anomaly_distances': test_anomaly_distances,\n",
    "                'all_distances': all_distances,\n",
    "                'predictions': predictions,\n",
    "                'true_labels': true_labels,\n",
    "                'mean': mean,\n",
    "                'cov': cov,\n",
    "                'validation_normal_distances': validation_normal_distances\n",
    "            }\n",
    "        else:\n",
    "            # Si no hay labels, solo retornar estad√≠sticas\n",
    "            mean = np.mean(normal_embeddings, axis=0)\n",
    "            cov = np.cov(normal_embeddings.T)\n",
    "            results = {\n",
    "                'method': method,\n",
    "                'mean': mean,\n",
    "                'cov': cov,\n",
    "                'normal_embeddings': normal_embeddings,\n",
    "                'test_embeddings': test_embeddings\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå ERROR en evaluate_anomaly_detection: {e}\"\n",
    "        print(error_msg)\n",
    "        raise RuntimeError(error_msg) from e\n",
    "\n",
    "print(\"‚úì Funciones de evaluaci√≥n definidas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Evaluaci√≥n de todos los modelos entrenados\n",
    "\n",
    "Evaluar cada modelo entrenado usando distancia de Mahalanobis y otras m√©tricas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar todos los modelos entrenados\n",
    "all_evaluation_results = []\n",
    "\n",
    "# Evaluar Modelo A\n",
    "print(\"Evaluando Modelo A...\")\n",
    "for result in model_a_results:\n",
    "    try:\n",
    "        eval_result = evaluate_anomaly_detection(\n",
    "            model=result[\"model\"],\n",
    "            normal_dataloader=data_module.val_dataloader(),  # Usar validaci√≥n para estimar distribuci√≥n normal\n",
    "            test_dataloader=data_module.test_dataloader(),    # Usar test para evaluar\n",
    "            device=device,\n",
    "            method=\"mahalanobis\",\n",
    "            percentile=95\n",
    "        )\n",
    "        all_evaluation_results.append({\n",
    "            \"model_type\": \"Modelo A\",\n",
    "            \"config\": result[\"config\"],\n",
    "            \"auc_roc\": eval_result[\"auc_roc\"],\n",
    "            \"auc_pr\": eval_result[\"auc_pr\"],\n",
    "            \"threshold\": eval_result[\"threshold\"]\n",
    "        })\n",
    "        print(f\"  {result['config']}: AUC-ROC={eval_result['auc_roc']:.4f}, AUC-PR={eval_result['auc_pr']:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error evaluando {result['config']}: {e}\")\n",
    "\n",
    "# Evaluar Modelo B\n",
    "print(\"\\nEvaluando Modelo B...\")\n",
    "for result in model_b_results:\n",
    "    try:\n",
    "        eval_result = evaluate_anomaly_detection(\n",
    "            model=result[\"model\"],\n",
    "            normal_dataloader=data_module.val_dataloader(),  # Usar validaci√≥n para estimar distribuci√≥n normal\n",
    "            test_dataloader=data_module.test_dataloader(),    # Usar test para evaluar\n",
    "            device=device,\n",
    "            method=\"mahalanobis\",\n",
    "            percentile=95\n",
    "        )\n",
    "        all_evaluation_results.append({\n",
    "            \"model_type\": \"Modelo B\",\n",
    "            \"config\": result[\"config\"],\n",
    "            \"auc_roc\": eval_result[\"auc_roc\"],\n",
    "            \"auc_pr\": eval_result[\"auc_pr\"],\n",
    "            \"threshold\": eval_result[\"threshold\"]\n",
    "        })\n",
    "        print(f\"  {result['config']}: AUC-ROC={eval_result['auc_roc']:.4f}, AUC-PR={eval_result['auc_pr']:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error evaluando {result['config']}: {e}\")\n",
    "\n",
    "# Evaluar Modelo C\n",
    "print(\"\\nEvaluando Modelo C...\")\n",
    "for result in model_c_results:\n",
    "    try:\n",
    "        # Para autoencoders, tambi√©n evaluar con reconstruction_loss\n",
    "        eval_result_mah = evaluate_anomaly_detection(\n",
    "            model=result[\"model\"],\n",
    "            normal_dataloader=data_module.val_dataloader(),  # Usar validaci√≥n para estimar distribuci√≥n normal\n",
    "            test_dataloader=data_module.test_dataloader(),    # Usar test para evaluar\n",
    "            device=device,\n",
    "            method=\"mahalanobis\",\n",
    "            percentile=95\n",
    "        )\n",
    "        eval_result_recon = evaluate_anomaly_detection(\n",
    "            model=result[\"model\"],\n",
    "            normal_dataloader=data_module.val_dataloader(),  # Usar validaci√≥n para estimar distribuci√≥n normal\n",
    "            test_dataloader=data_module.test_dataloader(),    # Usar test para evaluar\n",
    "            device=device,\n",
    "            method=\"reconstruction_loss\",\n",
    "            percentile=95\n",
    "        )\n",
    "        all_evaluation_results.append({\n",
    "            \"model_type\": \"Modelo C\",\n",
    "            \"config\": result[\"config\"],\n",
    "            \"auc_roc_mah\": eval_result_mah[\"auc_roc\"],\n",
    "            \"auc_pr_mah\": eval_result_mah[\"auc_pr\"],\n",
    "            \"auc_roc_recon\": eval_result_recon[\"auc_roc\"],\n",
    "            \"auc_pr_recon\": eval_result_recon[\"auc_pr\"]\n",
    "        })\n",
    "        print(f\"  {result['config']}: Mahalanobis AUC-ROC={eval_result_mah['auc_roc']:.4f}, Recon AUC-ROC={eval_result_recon['auc_roc']:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error evaluando {result['config']}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úì Evaluaci√≥n completada: {len(all_evaluation_results)} modelos evaluados\")\n",
    "\n",
    "# Identificar los 3 mejores modelos\n",
    "if all_evaluation_results:\n",
    "    # Ordenar por AUC-ROC (usar el mejor m√©todo para cada modelo)\n",
    "    sorted_results = sorted(\n",
    "        all_evaluation_results,\n",
    "        key=lambda x: max(x.get(\"auc_roc\", 0), x.get(\"auc_roc_mah\", 0), x.get(\"auc_roc_recon\", 0)),\n",
    "        reverse=True\n",
    "    )\n",
    "    best_3_models = sorted_results[:3]\n",
    "    print(f\"\\nüèÜ Top 3 modelos:\")\n",
    "    for i, result in enumerate(best_3_models, 1):\n",
    "        best_auc = max(result.get(\"auc_roc\", 0), result.get(\"auc_roc_mah\", 0), result.get(\"auc_roc_recon\", 0))\n",
    "        print(f\"  {i}. {result['model_type']} - {result['config']}: AUC-ROC={best_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cuantizaci√≥n de Modelos\n",
    "\n",
    "Convertir los **3 mejores modelos** (seg√∫n AUC-ROC) a modelos cuantizados y realizar una comparaci√≥n completa:\n",
    "\n",
    "### Comparaciones realizadas:\n",
    "1. **Tama√±o del modelo**: Comparaci√≥n de tama√±o en MB y ratio de compresi√≥n\n",
    "2. **Latencia en respuesta**: Tiempo de inferencia promedio (100 iteraciones)\n",
    "3. **Rendimiento**: Comparaci√≥n de m√©tricas de detecci√≥n de anomal√≠as:\n",
    "   - AUC-ROC (Area Under ROC Curve)\n",
    "   - AUC-PR (Area Under Precision-Recall Curve)\n",
    "   - Diferencia y porcentaje de retenci√≥n de rendimiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de cuantizaci√≥n - Copiamos el contenido de evaluation.py\n",
    "\n",
    "def quantize_model(model, method=\"dynamic\"):\n",
    "    \"\"\"\n",
    "    Cuantiza un modelo PyTorch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    if method == \"dynamic\":\n",
    "        quantized_model = torch.quantization.quantize_dynamic(\n",
    "            model, {torch.nn.Linear, torch.nn.Conv2d}, dtype=torch.qint8\n",
    "        )\n",
    "    elif method == \"static\":\n",
    "        # Para cuantizaci√≥n est√°tica, necesitar√≠amos un dataset de calibraci√≥n\n",
    "        quantized_model = torch.quantization.quantize_dynamic(\n",
    "            model, {torch.nn.Linear, torch.nn.Conv2d}, dtype=torch.qint8\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"M√©todo de cuantizaci√≥n no reconocido: {method}\")\n",
    "    \n",
    "    return quantized_model\n",
    "\n",
    "\n",
    "def compare_model_sizes(original_model, quantized_model):\n",
    "    \"\"\"Compara el tama√±o de modelos original y cuantizado\"\"\"\n",
    "    def get_model_size(model):\n",
    "        param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "        buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "        return param_size + buffer_size\n",
    "    \n",
    "    original_size = get_model_size(original_model)\n",
    "    quantized_size = get_model_size(quantized_model)\n",
    "    \n",
    "    return {\n",
    "        'original_size_mb': original_size / (1024 * 1024),\n",
    "        'quantized_size_mb': quantized_size / (1024 * 1024),\n",
    "        'compression_ratio': original_size / quantized_size if quantized_size > 0 else 0\n",
    "    }\n",
    "\n",
    "\n",
    "# Cuantizar los 3 mejores modelos\n",
    "quantization_results = []\n",
    "\n",
    "# Verificar que best_3_models existe y tiene contenido\n",
    "if 'best_3_models' not in globals() or not best_3_models:\n",
    "    print(\"‚ö†Ô∏è ERROR: No se encontraron los mejores modelos.\")\n",
    "    print(\"   Por favor, ejecuta primero la celda de evaluaci√≥n (Secci√≥n 6) para identificar los mejores modelos.\")\n",
    "    print(\"   La variable 'best_3_models' debe estar definida antes de ejecutar la cuantizaci√≥n.\")\n",
    "else:\n",
    "    print(\"Cuantizando los 3 mejores modelos...\\n\")\n",
    "    \n",
    "    for i, best_model_info in enumerate(best_3_models, 1):\n",
    "        print(f\"Modelo {i}: {best_model_info['model_type']} - {best_model_info['config']}\")\n",
    "        \n",
    "        # Encontrar el modelo correspondiente\n",
    "        model_to_quantize = None\n",
    "        if best_model_info['model_type'] == \"Modelo A\":\n",
    "            for result in model_a_results:\n",
    "                if result['config'] == best_model_info['config']:\n",
    "                    model_to_quantize = result['model'].model\n",
    "                    break\n",
    "        elif best_model_info['model_type'] == \"Modelo B\":\n",
    "            for result in model_b_results:\n",
    "                if result['config'] == best_model_info['config']:\n",
    "                    model_to_quantize = result['model'].model\n",
    "                    break\n",
    "        elif best_model_info['model_type'] == \"Modelo C\":\n",
    "            for result in model_c_results:\n",
    "                if result['config'] == best_model_info['config']:\n",
    "                    model_to_quantize = result['model'].model\n",
    "                    break\n",
    "        \n",
    "        if model_to_quantize is None:\n",
    "            print(f\"  ‚ö†Ô∏è No se encontr√≥ el modelo\\n\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Cuantizar\n",
    "            quantized_model = quantize_model(model_to_quantize, method=\"dynamic\")\n",
    "            \n",
    "            # Comparar tama√±os\n",
    "            size_comparison = compare_model_sizes(model_to_quantize, quantized_model)\n",
    "            \n",
    "            # Medir latencia (inferencia) - promedio sobre m√∫ltiples iteraciones\n",
    "            model_to_quantize.eval()\n",
    "            quantized_model.eval()\n",
    "            \n",
    "            # Crear un batch de prueba\n",
    "            test_batch = next(iter(data_module.test_dataloader()))\n",
    "            if isinstance(test_batch, tuple):\n",
    "                test_images = test_batch[0][:1].to(device)\n",
    "            else:\n",
    "                test_images = test_batch[:1].to(device)\n",
    "            \n",
    "            # Latencia original - promedio sobre 100 iteraciones\n",
    "            import time\n",
    "            model_to_quantize = model_to_quantize.to(device)\n",
    "            latencies_original = []\n",
    "            with torch.no_grad():\n",
    "                for _ in range(100):\n",
    "                    start_time = time.time()\n",
    "                    if hasattr(model_to_quantize, 'get_embedding'):\n",
    "                        _ = model_to_quantize.get_embedding(test_images)\n",
    "                    else:\n",
    "                        _ = model_to_quantize(test_images)\n",
    "                    latencies_original.append((time.time() - start_time) * 1000)  # ms\n",
    "            original_latency = np.mean(latencies_original)\n",
    "            \n",
    "            # Latencia cuantizado - promedio sobre 100 iteraciones\n",
    "            quantized_model = quantized_model.to(device)\n",
    "            latencies_quantized = []\n",
    "            with torch.no_grad():\n",
    "                for _ in range(100):\n",
    "                    start_time = time.time()\n",
    "                    if hasattr(quantized_model, 'get_embedding'):\n",
    "                        _ = quantized_model.get_embedding(test_images)\n",
    "                    else:\n",
    "                        _ = quantized_model(test_images)\n",
    "                    latencies_quantized.append((time.time() - start_time) * 1000)  # ms\n",
    "            quantized_latency = np.mean(latencies_quantized)\n",
    "            \n",
    "            # Evaluar rendimiento (precisi√≥n/accuracy) del modelo original\n",
    "            print(\"  üìä Evaluando rendimiento del modelo original...\")\n",
    "            original_performance = None\n",
    "            try:\n",
    "                # Usar el modelo Lightning completo para evaluaci√≥n\n",
    "                if best_model_info['model_type'] == \"Modelo A\":\n",
    "                    for result in model_a_results:\n",
    "                        if result['config'] == best_model_info['config']:\n",
    "                            lightning_model_original = result['model']\n",
    "                            break\n",
    "                elif best_model_info['model_type'] == \"Modelo B\":\n",
    "                    for result in model_b_results:\n",
    "                        if result['config'] == best_model_info['config']:\n",
    "                            lightning_model_original = result['model']\n",
    "                            break\n",
    "                elif best_model_info['model_type'] == \"Modelo C\":\n",
    "                    for result in model_c_results:\n",
    "                        if result['config'] == best_model_info['config']:\n",
    "                            lightning_model_original = result['model']\n",
    "                            break\n",
    "                \n",
    "                # Evaluar con distancia de Mahalanobis\n",
    "                eval_original = evaluate_anomaly_detection(\n",
    "                    model=lightning_model_original,\n",
    "                    normal_dataloader=data_module.val_dataloader(),\n",
    "                    test_dataloader=data_module.test_dataloader(),\n",
    "                    device=device,\n",
    "                    method=\"mahalanobis\",\n",
    "                    percentile=95\n",
    "                )\n",
    "                original_performance = {\n",
    "                    'auc_roc': eval_original['auc_roc'],\n",
    "                    'auc_pr': eval_original['auc_pr']\n",
    "                }\n",
    "                print(f\"    ‚úì AUC-ROC: {original_performance['auc_roc']:.4f}, AUC-PR: {original_performance['auc_pr']:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ö†Ô∏è Error evaluando modelo original: {e}\")\n",
    "                original_performance = {'auc_roc': 0.0, 'auc_pr': 0.0}\n",
    "            \n",
    "            # Evaluar rendimiento del modelo cuantizado\n",
    "            print(\"  üìä Evaluando rendimiento del modelo cuantizado...\")\n",
    "            quantized_performance = None\n",
    "            try:\n",
    "                # Crear un wrapper Lightning para el modelo cuantizado\n",
    "                if best_model_info['model_type'] in [\"Modelo A\", \"Modelo B\"]:\n",
    "                    # Para modelos de clasificaci√≥n, necesitamos el wrapper Lightning\n",
    "                    quantized_lightning = CNNClassifierLightning(\n",
    "                        model=quantized_model,\n",
    "                        num_classes=len(CATEGORIES),\n",
    "                        model_type=\"scratch\" if best_model_info['model_type'] == \"Modelo A\" else \"distilled\"\n",
    "                    )\n",
    "                else:  # Modelo C\n",
    "                    quantized_lightning = AutoencoderLightning(\n",
    "                        model=quantized_model,\n",
    "                        loss_function=\"L2\"\n",
    "                    )\n",
    "                \n",
    "                # Evaluar con distancia de Mahalanobis\n",
    "                eval_quantized = evaluate_anomaly_detection(\n",
    "                    model=quantized_lightning,\n",
    "                    normal_dataloader=data_module.val_dataloader(),\n",
    "                    test_dataloader=data_module.test_dataloader(),\n",
    "                    device=device,\n",
    "                    method=\"mahalanobis\",\n",
    "                    percentile=95\n",
    "                )\n",
    "                quantized_performance = {\n",
    "                    'auc_roc': eval_quantized['auc_roc'],\n",
    "                    'auc_pr': eval_quantized['auc_pr']\n",
    "                }\n",
    "                print(f\"    ‚úì AUC-ROC: {quantized_performance['auc_roc']:.4f}, AUC-PR: {quantized_performance['auc_pr']:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ö†Ô∏è Error evaluando modelo cuantizado: {e}\")\n",
    "                quantized_performance = {'auc_roc': 0.0, 'auc_pr': 0.0}\n",
    "            \n",
    "            # Calcular diferencia de rendimiento\n",
    "            performance_diff_auc_roc = original_performance['auc_roc'] - quantized_performance['auc_roc']\n",
    "            performance_diff_auc_pr = original_performance['auc_pr'] - quantized_performance['auc_pr']\n",
    "            performance_retention_auc_roc = (quantized_performance['auc_roc'] / original_performance['auc_roc'] * 100) if original_performance['auc_roc'] > 0 else 0\n",
    "            performance_retention_auc_pr = (quantized_performance['auc_pr'] / original_performance['auc_pr'] * 100) if original_performance['auc_pr'] > 0 else 0\n",
    "            \n",
    "            quantization_results.append({\n",
    "                \"model_type\": best_model_info['model_type'],\n",
    "                \"config\": best_model_info['config'],\n",
    "                \"original_size_mb\": size_comparison['original_size_mb'],\n",
    "                \"quantized_size_mb\": size_comparison['quantized_size_mb'],\n",
    "                \"compression_ratio\": size_comparison['compression_ratio'],\n",
    "                \"original_latency_ms\": original_latency,\n",
    "                \"quantized_latency_ms\": quantized_latency,\n",
    "                \"speedup\": original_latency / quantized_latency if quantized_latency > 0 else 0,\n",
    "                \"original_auc_roc\": original_performance['auc_roc'],\n",
    "                \"quantized_auc_roc\": quantized_performance['auc_roc'],\n",
    "                \"original_auc_pr\": original_performance['auc_pr'],\n",
    "                \"quantized_auc_pr\": quantized_performance['auc_pr'],\n",
    "                \"performance_diff_auc_roc\": performance_diff_auc_roc,\n",
    "                \"performance_diff_auc_pr\": performance_diff_auc_pr,\n",
    "                \"performance_retention_auc_roc\": performance_retention_auc_roc,\n",
    "                \"performance_retention_auc_pr\": performance_retention_auc_pr\n",
    "            })\n",
    "            \n",
    "            print(f\"\\n  üìä COMPARACI√ìN DE RESULTADOS:\")\n",
    "            print(f\"  {'='*60}\")\n",
    "            print(f\"  Tama√±o:\")\n",
    "            print(f\"    Original: {size_comparison['original_size_mb']:.2f} MB\")\n",
    "            print(f\"    Cuantizado: {size_comparison['quantized_size_mb']:.2f} MB\")\n",
    "            print(f\"    Compresi√≥n: {size_comparison['compression_ratio']:.2f}x\")\n",
    "            print(f\"\\n  Latencia (promedio sobre 100 iteraciones):\")\n",
    "            print(f\"    Original: {original_latency:.2f} ms\")\n",
    "            print(f\"    Cuantizado: {quantized_latency:.2f} ms\")\n",
    "            print(f\"    Speedup: {original_latency / quantized_latency if quantized_latency > 0 else 0:.2f}x\")\n",
    "            print(f\"\\n  Rendimiento (AUC-ROC):\")\n",
    "            print(f\"    Original: {original_performance['auc_roc']:.4f}\")\n",
    "            print(f\"    Cuantizado: {quantized_performance['auc_roc']:.4f}\")\n",
    "            print(f\"    Diferencia: {performance_diff_auc_roc:+.4f}\")\n",
    "            print(f\"    Retenci√≥n: {performance_retention_auc_roc:.2f}%\")\n",
    "            print(f\"\\n  Rendimiento (AUC-PR):\")\n",
    "            print(f\"    Original: {original_performance['auc_pr']:.4f}\")\n",
    "            print(f\"    Cuantizado: {quantized_performance['auc_pr']:.4f}\")\n",
    "            print(f\"    Diferencia: {performance_diff_auc_pr:+.4f}\")\n",
    "            print(f\"    Retenci√≥n: {performance_retention_auc_pr:.2f}%\")\n",
    "            print(f\"  {'='*60}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error al cuantizar: {e}\\n\")\n",
    "    \n",
    "    print(f\"‚úì Cuantizaci√≥n completada: {len(quantization_results)} modelos cuantizados\")\n",
    "    \n",
    "    # Mostrar resumen comparativo completo\n",
    "    if quantization_results:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"RESUMEN COMPARATIVO DE CUANTIZACI√ìN\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"\\nComparaci√≥n de los 3 mejores modelos: Original vs Cuantizado\\n\")\n",
    "        \n",
    "        for i, result in enumerate(quantization_results, 1):\n",
    "            print(f\"{i}. {result['model_type']} - {result['config']}\")\n",
    "            print(f\"   {'-'*70}\")\n",
    "            print(f\"   Tama√±o:\")\n",
    "            print(f\"     Original: {result['original_size_mb']:.2f} MB ‚Üí Cuantizado: {result['quantized_size_mb']:.2f} MB\")\n",
    "            print(f\"     Compresi√≥n: {result['compression_ratio']:.2f}x\")\n",
    "            print(f\"   Latencia:\")\n",
    "            print(f\"     Original: {result['original_latency_ms']:.2f} ms ‚Üí Cuantizado: {result['quantized_latency_ms']:.2f} ms\")\n",
    "            print(f\"     Speedup: {result['speedup']:.2f}x\")\n",
    "            print(f\"   Rendimiento (AUC-ROC):\")\n",
    "            print(f\"     Original: {result['original_auc_roc']:.4f} ‚Üí Cuantizado: {result['quantized_auc_roc']:.4f}\")\n",
    "            print(f\"     Diferencia: {result['performance_diff_auc_roc']:+.4f} ({result['performance_retention_auc_roc']:.2f}% retenci√≥n)\")\n",
    "            print(f\"   Rendimiento (AUC-PR):\")\n",
    "            print(f\"     Original: {result['original_auc_pr']:.4f} ‚Üí Cuantizado: {result['quantized_auc_pr']:.4f}\")\n",
    "            print(f\"     Diferencia: {result['performance_diff_auc_pr']:+.4f} ({result['performance_retention_auc_pr']:.2f}% retenci√≥n)\")\n",
    "            print()\n",
    "        \n",
    "        # Resumen estad√≠stico\n",
    "        print(\"=\"*80)\n",
    "        print(\"RESUMEN ESTAD√çSTICO\")\n",
    "        print(\"=\"*80)\n",
    "        avg_compression = np.mean([r['compression_ratio'] for r in quantization_results])\n",
    "        avg_speedup = np.mean([r['speedup'] for r in quantization_results])\n",
    "        avg_retention_auc_roc = np.mean([r['performance_retention_auc_roc'] for r in quantization_results])\n",
    "        avg_retention_auc_pr = np.mean([r['performance_retention_auc_pr'] for r in quantization_results])\n",
    "        \n",
    "        print(f\"\\nPromedio de compresi√≥n: {avg_compression:.2f}x\")\n",
    "        print(f\"Promedio de speedup: {avg_speedup:.2f}x\")\n",
    "        print(f\"Retenci√≥n promedio de rendimiento (AUC-ROC): {avg_retention_auc_roc:.2f}%\")\n",
    "        print(f\"Retenci√≥n promedio de rendimiento (AUC-PR): {avg_retention_auc_pr:.2f}%\")\n",
    "        print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. An√°lisis de Outliers mediante DBSCAN Clustering\n",
    "\n",
    "Una vez identificado el mejor modelo de detecci√≥n de anomal√≠as, se utilizan sus embeddings para realizar un an√°lisis adicional mediante t√©cnicas de agrupamiento no supervisado.\n",
    "\n",
    "**DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) es un m√©todo basado en densidad que permite:\n",
    "- Identificar regiones de alta concentraci√≥n en el espacio latente\n",
    "- Detectar puntos aislados (outliers/anomal√≠as) que se encuentran en zonas de baja densidad\n",
    "\n",
    "### Proceso de An√°lisis:\n",
    "\n",
    "1. **Extracci√≥n de embeddings**: Se extraen los embeddings del mejor modelo para cada imagen del conjunto de prueba\n",
    "2. **Reducci√≥n de dimensionalidad**: \n",
    "   - **PCA**: Para facilitar el procesamiento y reducir dimensionalidad\n",
    "   - **t-SNE**: Para visualizaci√≥n 2D y separaci√≥n estructural\n",
    "3. **Aplicaci√≥n de DBSCAN**: Los puntos etiquetados como ruido (-1) representan potenciales anomal√≠as\n",
    "4. **An√°lisis visual y cuantitativo**: Comparaci√≥n de resultados DBSCAN con ground truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones DBSCAN - Copiamos el contenido de evaluation.py\n",
    "\n",
    "def dbscan_analysis(embeddings, eps=0.5, min_samples=5, use_pca=True, pca_components=50,\n",
    "                    use_tsne=True, tsne_components=2, tsne_perplexity=30):\n",
    "    \"\"\"\n",
    "    An√°lisis DBSCAN para detecci√≥n de outliers mediante clustering basado en densidad.\n",
    "    \n",
    "    Proceso:\n",
    "    1. Reducci√≥n de dimensionalidad con PCA (opcional)\n",
    "    2. Aplicaci√≥n de DBSCAN para identificar clusters y outliers\n",
    "    3. Reducci√≥n adicional con t-SNE para visualizaci√≥n 2D\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Array numpy de shape (N, d) con embeddings\n",
    "        eps: Distancia m√°xima entre muestras para formar un cluster\n",
    "        min_samples: N√∫mero m√≠nimo de muestras para formar un cluster\n",
    "        use_pca: Si usar PCA para reducci√≥n de dimensionalidad\n",
    "        pca_components: N√∫mero de componentes PCA\n",
    "        use_tsne: Si usar t-SNE para visualizaci√≥n 2D\n",
    "        tsne_components: Dimensiones de salida de t-SNE (t√≠picamente 2)\n",
    "        tsne_perplexity: Perplejidad para t-SNE\n",
    "    \n",
    "    Returns:\n",
    "        results: Diccionario con resultados del an√°lisis\n",
    "    \"\"\"\n",
    "    print(f\"üìä Iniciando an√°lisis DBSCAN...\")\n",
    "    print(f\"  Embeddings originales: shape {embeddings.shape}\")\n",
    "    \n",
    "    # Reducci√≥n de dimensionalidad con PCA\n",
    "    if use_pca and embeddings.shape[1] > pca_components:\n",
    "        print(f\"  Aplicando PCA: {embeddings.shape[1]} ‚Üí {pca_components} dimensiones\")\n",
    "        pca = PCA(n_components=pca_components)\n",
    "        embeddings_reduced = pca.fit_transform(embeddings)\n",
    "        explained_variance = np.sum(pca.explained_variance_ratio_)\n",
    "        print(f\"  ‚úì Varianza explicada por PCA: {explained_variance:.4f} ({explained_variance*100:.2f}%)\")\n",
    "    else:\n",
    "        embeddings_reduced = embeddings\n",
    "        pca = None\n",
    "        explained_variance = 1.0\n",
    "        print(f\"  ‚ö†Ô∏è PCA no aplicado (use_pca=False o dimensi√≥n ya es {embeddings.shape[1]})\")\n",
    "    \n",
    "    # Aplicar DBSCAN\n",
    "    print(f\"\\n  Aplicando DBSCAN (eps={eps}, min_samples={min_samples})...\")\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    clusters = dbscan.fit_predict(embeddings_reduced)\n",
    "    \n",
    "    # Identificar outliers (ruido)\n",
    "    n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "    n_noise = list(clusters).count(-1)\n",
    "    n_in_clusters = len(clusters) - n_noise\n",
    "    \n",
    "    print(f\"  ‚úì DBSCAN completado:\")\n",
    "    print(f\"    - Clusters encontrados: {n_clusters}\")\n",
    "    print(f\"    - Puntos en clusters: {n_in_clusters} ({n_in_clusters/len(clusters)*100:.2f}%)\")\n",
    "    print(f\"    - Outliers (ruido): {n_noise} ({n_noise/len(clusters)*100:.2f}%)\")\n",
    "    \n",
    "    # Reducci√≥n para visualizaci√≥n con t-SNE\n",
    "    embeddings_2d = None\n",
    "    if use_tsne:\n",
    "        print(f\"\\n  Aplicando t-SNE para visualizaci√≥n 2D...\")\n",
    "        perplexity = min(tsne_perplexity, len(embeddings_reduced) - 1)\n",
    "        if perplexity > 0:\n",
    "            tsne = TSNE(n_components=tsne_components, random_state=42, perplexity=perplexity)\n",
    "            embeddings_2d = tsne.fit_transform(embeddings_reduced)\n",
    "            print(f\"  ‚úì t-SNE completado: {embeddings_reduced.shape[1]} ‚Üí {tsne_components} dimensiones\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Perplexity inv√°lida ({perplexity}), saltando t-SNE\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è t-SNE deshabilitado (use_tsne=False)\")\n",
    "    \n",
    "    results = {\n",
    "        'clusters': clusters,\n",
    "        'n_clusters': n_clusters,\n",
    "        'n_noise': n_noise,\n",
    "        'embeddings_reduced': embeddings_reduced,\n",
    "        'embeddings_2d': embeddings_2d,\n",
    "        'pca': pca,\n",
    "        'explained_variance': explained_variance\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def visualize_dbscan_results(dbscan_results, labels=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualiza los resultados de DBSCAN de forma completa.\n",
    "    \n",
    "    Muestra:\n",
    "    1. Clustering DBSCAN (clusters y outliers)\n",
    "    2. Comparaci√≥n con ground truth labels\n",
    "    3. An√°lisis de distribuci√≥n de outliers vs normales\n",
    "    \"\"\"\n",
    "    clusters = dbscan_results['clusters']\n",
    "    embeddings_2d = dbscan_results['embeddings_2d']\n",
    "    \n",
    "    if embeddings_2d is None:\n",
    "        print(\"‚ö†Ô∏è No hay visualizaci√≥n 2D disponible (t-SNE no se aplic√≥)\")\n",
    "        return\n",
    "    \n",
    "    # Crear figura con m√∫ltiples subplots para an√°lisis completo\n",
    "    fig = plt.figure(figsize=(18, 6))\n",
    "    axes = fig.subplots(1, 3)\n",
    "    \n",
    "    # Visualizaci√≥n por clusters\n",
    "    unique_clusters = set(clusters)\n",
    "    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_clusters)))\n",
    "    \n",
    "    for cluster, color in zip(unique_clusters, colors):\n",
    "        if cluster == -1:\n",
    "            # Ruido (outliers)\n",
    "            mask = clusters == cluster\n",
    "            axes[0].scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], \n",
    "                          c='black', marker='x', s=50, label='Outliers', alpha=0.6)\n",
    "        else:\n",
    "            mask = clusters == cluster\n",
    "            axes[0].scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], \n",
    "                          c=[color], s=50, label=f'Cluster {cluster}', alpha=0.6)\n",
    "    \n",
    "    axes[0].set_title(f'DBSCAN Clustering (Clusters: {dbscan_results[\"n_clusters\"]}, Outliers: {dbscan_results[\"n_noise\"]})')\n",
    "    axes[0].set_xlabel('t-SNE Component 1')\n",
    "    axes[0].set_ylabel('t-SNE Component 2')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Visualizaci√≥n por labels (si est√°n disponibles)\n",
    "    if labels is not None:\n",
    "        normal_mask = labels == 0\n",
    "        anomaly_mask = labels == 1\n",
    "        \n",
    "        axes[1].scatter(embeddings_2d[normal_mask, 0], embeddings_2d[normal_mask, 1], \n",
    "                      c='green', s=50, label='Normal', alpha=0.6)\n",
    "        axes[1].scatter(embeddings_2d[anomaly_mask, 0], embeddings_2d[anomaly_mask, 1], \n",
    "                      c='red', s=50, label='Anomaly', alpha=0.6)\n",
    "        \n",
    "        axes[1].set_title('Ground Truth Labels')\n",
    "        axes[1].set_xlabel('t-SNE Component 1')\n",
    "        axes[1].set_ylabel('t-SNE Component 2')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Visualizaci√≥n adicional: Outliers DBSCAN vs Ground Truth\n",
    "        outlier_mask = clusters == -1\n",
    "        in_cluster_mask = clusters != -1\n",
    "        \n",
    "        # Colores: verde=normal, rojo=anomal√≠a, tama√±o grande=outlier DBSCAN\n",
    "        normal_outliers = (outlier_mask) & (normal_mask)\n",
    "        anomaly_outliers = (outlier_mask) & (anomaly_mask)\n",
    "        normal_in_cluster = (in_cluster_mask) & (normal_mask)\n",
    "        anomaly_in_cluster = (in_cluster_mask) & (anomaly_mask)\n",
    "        \n",
    "        axes[2].scatter(embeddings_2d[normal_in_cluster, 0], embeddings_2d[normal_in_cluster, 1], \n",
    "                       c='lightgreen', s=30, label='Normal (en cluster)', alpha=0.5, marker='o')\n",
    "        axes[2].scatter(embeddings_2d[normal_outliers, 0], embeddings_2d[normal_outliers, 1], \n",
    "                       c='green', s=150, label='Normal (outlier DBSCAN)', alpha=0.8, marker='x', linewidths=2)\n",
    "        axes[2].scatter(embeddings_2d[anomaly_in_cluster, 0], embeddings_2d[anomaly_in_cluster, 1], \n",
    "                       c='lightcoral', s=30, label='Anomal√≠a (en cluster)', alpha=0.5, marker='o')\n",
    "        axes[2].scatter(embeddings_2d[anomaly_outliers, 0], embeddings_2d[anomaly_outliers, 1], \n",
    "                       c='red', s=150, label='Anomal√≠a (outlier DBSCAN)', alpha=0.8, marker='x', linewidths=2)\n",
    "        \n",
    "        axes[2].set_title('DBSCAN Outliers vs Ground Truth')\n",
    "        axes[2].set_xlabel('t-SNE Component 1')\n",
    "        axes[2].set_ylabel('t-SNE Component 2')\n",
    "        axes[2].legend(loc='best', fontsize=8)\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        # Si no hay labels, solo mostrar clustering\n",
    "        axes[1].axis('off')\n",
    "        axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"  ‚úì Visualizaci√≥n guardada en: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úì Funciones DBSCAN definidas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. An√°lisis DBSCAN del mejor modelo\n",
    "\n",
    "Aplicar DBSCAN al mejor modelo identificado para an√°lisis de outliers:\n",
    "\n",
    "1. **Extracci√≥n de embeddings**: Se extraen embeddings del conjunto de prueba usando el mejor modelo\n",
    "2. **Reducci√≥n de dimensionalidad**: PCA y t-SNE para facilitar visualizaci√≥n y procesamiento\n",
    "3. **Clustering DBSCAN**: Identificaci√≥n de clusters y outliers (puntos de baja densidad)\n",
    "4. **An√°lisis visual**: Visualizaci√≥n de clusters, outliers y comparaci√≥n con ground truth\n",
    "5. **An√°lisis cuantitativo**: M√©tricas de clasificaci√≥n comparando outliers DBSCAN con ground truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar el mejor modelo para an√°lisis DBSCAN\n",
    "if best_3_models:\n",
    "    best_model_info = best_3_models[0]\n",
    "    print(f\"Analizando con el mejor modelo: {best_model_info['model_type']} - {best_model_info['config']}\\n\")\n",
    "    \n",
    "    # Encontrar el modelo\n",
    "    best_model = None\n",
    "    if best_model_info['model_type'] == \"Modelo A\":\n",
    "        for result in model_a_results:\n",
    "            if result['config'] == best_model_info['config']:\n",
    "                best_model = result['model']\n",
    "                break\n",
    "    elif best_model_info['model_type'] == \"Modelo B\":\n",
    "        for result in model_b_results:\n",
    "            if result['config'] == best_model_info['config']:\n",
    "                best_model = result['model']\n",
    "                break\n",
    "    elif best_model_info['model_type'] == \"Modelo C\":\n",
    "        for result in model_c_results:\n",
    "            if result['config'] == best_model_info['config']:\n",
    "                best_model = result['model']\n",
    "                break\n",
    "    \n",
    "    if best_model is not None:\n",
    "        # Extraer embeddings del conjunto de prueba\n",
    "        all_embeddings = []\n",
    "        all_labels = []\n",
    "        \n",
    "        best_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in data_module.test_dataloader():\n",
    "                if isinstance(batch, tuple):\n",
    "                    images, labels = batch\n",
    "                else:\n",
    "                    images = batch\n",
    "                    labels = None\n",
    "                \n",
    "                images = images.to(device)\n",
    "                \n",
    "                # Extraer embeddings\n",
    "                if hasattr(best_model, 'get_embedding'):\n",
    "                    embeddings = best_model.get_embedding(images)\n",
    "                elif hasattr(best_model, 'model') and hasattr(best_model.model, 'get_embedding'):\n",
    "                    embeddings = best_model.model.get_embedding(images)\n",
    "                else:\n",
    "                    if hasattr(best_model, 'model'):\n",
    "                        logits, embeddings = best_model.model(images)\n",
    "                    else:\n",
    "                        logits, embeddings = best_model(images)\n",
    "                \n",
    "                all_embeddings.append(embeddings.cpu().numpy())\n",
    "                if labels is not None:\n",
    "                    all_labels.append(labels.cpu().numpy())\n",
    "        \n",
    "        all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "        all_labels = np.concatenate(all_labels, axis=0) if all_labels else None\n",
    "        \n",
    "        print(f\"Embeddings extra√≠dos: {all_embeddings.shape}\")\n",
    "        \n",
    "        # Aplicar DBSCAN\n",
    "        dbscan_config = cfg.dbscan if cfg else {\n",
    "            \"eps\": 0.5,\n",
    "            \"min_samples\": 5,\n",
    "            \"use_pca\": True,\n",
    "            \"pca_components\": 50,\n",
    "            \"use_tsne\": True,\n",
    "            \"tsne_components\": 2,\n",
    "            \"tsne_perplexity\": 30\n",
    "        }\n",
    "        \n",
    "        dbscan_results = dbscan_analysis(\n",
    "            embeddings=all_embeddings,\n",
    "            eps=dbscan_config.get(\"eps\", 0.5),\n",
    "            min_samples=dbscan_config.get(\"min_samples\", 5),\n",
    "            use_pca=dbscan_config.get(\"use_pca\", True),\n",
    "            pca_components=dbscan_config.get(\"pca_components\", 50),\n",
    "            use_tsne=dbscan_config.get(\"use_tsne\", True),\n",
    "            tsne_components=dbscan_config.get(\"tsne_components\", 2),\n",
    "            tsne_perplexity=dbscan_config.get(\"tsne_perplexity\", 30)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úì DBSCAN completado:\")\n",
    "        print(f\"  Clusters encontrados: {dbscan_results['n_clusters']}\")\n",
    "        print(f\"  Outliers detectados: {dbscan_results['n_noise']}\")\n",
    "        print(f\"  Varianza explicada (PCA): {dbscan_results['explained_variance']:.4f}\")\n",
    "        \n",
    "        # Visualizar\n",
    "        save_path = os.path.join(DRIVE_BASE_PATH, 'dbscan_analysis.png')\n",
    "        visualize_dbscan_results(dbscan_results, labels=all_labels, save_path=save_path)\n",
    "        \n",
    "        # An√°lisis cuantitativo: Comparar outliers de DBSCAN con ground truth\n",
    "        if all_labels is not None:\n",
    "            dbscan_outliers = (dbscan_results['clusters'] == -1).astype(int)\n",
    "            true_anomalies = all_labels\n",
    "            \n",
    "            # Calcular m√©tricas de clasificaci√≥n\n",
    "            dbscan_auc = roc_auc_score(true_anomalies, dbscan_outliers)\n",
    "            dbscan_ap = average_precision_score(true_anomalies, dbscan_outliers)\n",
    "            \n",
    "            # Calcular m√©tricas adicionales\n",
    "            from sklearn.metrics import confusion_matrix\n",
    "            \n",
    "            # Matriz de confusi√≥n\n",
    "            cm = confusion_matrix(true_anomalies, dbscan_outliers)\n",
    "            if cm.size == 4:\n",
    "                tn, fp, fn, tp = cm.ravel()\n",
    "            else:\n",
    "                tn, fp, fn, tp = 0, 0, 0, 0\n",
    "            \n",
    "            # Calcular precisi√≥n, recall, F1\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "            accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(\"AN√ÅLISIS CUANTITATIVO: DBSCAN vs Ground Truth\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"\\nüìä M√©tricas de Clasificaci√≥n:\")\n",
    "            print(f\"  AUC-ROC: {dbscan_auc:.4f}\")\n",
    "            print(f\"  Average Precision (AUC-PR): {dbscan_ap:.4f}\")\n",
    "            print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"  Precision: {precision:.4f}\")\n",
    "            print(f\"  Recall: {recall:.4f}\")\n",
    "            print(f\"  F1-Score: {f1_score:.4f}\")\n",
    "            \n",
    "            print(f\"\\nüìã Matriz de Confusi√≥n:\")\n",
    "            print(f\"                Predicci√≥n\")\n",
    "            print(f\"              Normal  Anomal√≠a\")\n",
    "            print(f\"  Normal      {tn:6d}  {fp:6d}\")\n",
    "            print(f\"  Anomal√≠a    {fn:6d}  {tp:6d}\")\n",
    "            \n",
    "            print(f\"\\nüìà Estad√≠sticas de Clusters:\")\n",
    "            print(f\"  Total de muestras: {len(all_labels)}\")\n",
    "            print(f\"  Muestras normales (ground truth): {np.sum(true_anomalies == 0)}\")\n",
    "            print(f\"  Muestras an√≥malas (ground truth): {np.sum(true_anomalies == 1)}\")\n",
    "            print(f\"  Clusters encontrados: {dbscan_results['n_clusters']}\")\n",
    "            print(f\"  Outliers detectados por DBSCAN: {dbscan_results['n_noise']}\")\n",
    "            print(f\"  Porcentaje de outliers: {dbscan_results['n_noise'] / len(all_labels) * 100:.2f}%\")\n",
    "            \n",
    "            # An√°lisis de distribuci√≥n de outliers\n",
    "            normal_outliers = np.sum((dbscan_outliers == 1) & (true_anomalies == 0))\n",
    "            anomaly_outliers = np.sum((dbscan_outliers == 1) & (true_anomalies == 1))\n",
    "            normal_in_cluster = np.sum((dbscan_outliers == 0) & (true_anomalies == 0))\n",
    "            anomaly_in_cluster = np.sum((dbscan_outliers == 0) & (true_anomalies == 1))\n",
    "            \n",
    "            n_normal = np.sum(true_anomalies == 0)\n",
    "            n_anomaly = np.sum(true_anomalies == 1)\n",
    "            \n",
    "            print(f\"\\nüîç An√°lisis de Distribuci√≥n:\")\n",
    "            if n_normal > 0:\n",
    "                print(f\"  Normales detectadas como outliers: {normal_outliers} ({normal_outliers/n_normal*100:.2f}% de normales)\")\n",
    "                print(f\"  Normales en clusters: {normal_in_cluster} ({normal_in_cluster/n_normal*100:.2f}% de normales)\")\n",
    "            if n_anomaly > 0:\n",
    "                print(f\"  Anomal√≠as detectadas como outliers: {anomaly_outliers} ({anomaly_outliers/n_anomaly*100:.2f}% de anomal√≠as)\")\n",
    "                print(f\"  Anomal√≠as en clusters: {anomaly_in_cluster} ({anomaly_in_cluster/n_anomaly*100:.2f}% de anomal√≠as)\")\n",
    "            \n",
    "            print(f\"\\nüí° Interpretaci√≥n:\")\n",
    "            if dbscan_auc > 0.7:\n",
    "                print(f\"  ‚úì DBSCAN muestra buena capacidad para detectar anomal√≠as (AUC-ROC > 0.7)\")\n",
    "            elif dbscan_auc > 0.5:\n",
    "                print(f\"  ‚ö†Ô∏è DBSCAN tiene capacidad moderada para detectar anomal√≠as (0.5 < AUC-ROC < 0.7)\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå DBSCAN tiene capacidad limitada para detectar anomal√≠as (AUC-ROC < 0.5)\")\n",
    "            \n",
    "            if n_anomaly > 0 and anomaly_outliers / n_anomaly > 0.5:\n",
    "                print(f\"  ‚úì M√°s del 50% de las anomal√≠as fueron detectadas como outliers\")\n",
    "            elif n_anomaly > 0:\n",
    "                print(f\"  ‚ö†Ô∏è Menos del 50% de las anomal√≠as fueron detectadas como outliers\")\n",
    "            \n",
    "            print(f\"{'='*80}\\n\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No se encontr√≥ el mejor modelo\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No hay modelos entrenados para analizar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Resumen y Conclusiones\n",
    "\n",
    "Resumen de resultados y comparaci√≥n final de todos los modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear resumen final\n",
    "print(\"=\"*80)\n",
    "print(\"RESUMEN FINAL DEL PROYECTO II\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Modelos entrenados:\")\n",
    "if 'model_a_results' in globals():\n",
    "    print(f\"  - Modelo A (CNN desde cero): {len(model_a_results)} configuraciones\")\n",
    "if 'model_b_results' in globals():\n",
    "    print(f\"  - Modelo B (CNN con destilaci√≥n): {len(model_b_results)} configuraciones\")\n",
    "if 'model_c_results' in globals():\n",
    "    print(f\"  - Modelo C (Autoencoder U-Net): {len(model_c_results)} configuraciones\")\n",
    "\n",
    "if 'best_3_models' in globals() and best_3_models:\n",
    "    print(f\"\\nüèÜ Top 3 modelos (por AUC-ROC):\")\n",
    "    for i, result in enumerate(best_3_models, 1):\n",
    "        best_auc = max(result.get(\"auc_roc\", 0), result.get(\"auc_roc_mah\", 0), result.get(\"auc_roc_recon\", 0))\n",
    "        print(f\"  {i}. {result['model_type']} - {result['config']}: AUC-ROC={best_auc:.4f}\")\n",
    "\n",
    "if 'quantization_results' in globals() and quantization_results:\n",
    "    print(f\"\\n‚ö° Cuantizaci√≥n (3 mejores modelos):\")\n",
    "    for result in quantization_results:\n",
    "        print(f\"  - {result['model_type']} - {result['config']}:\")\n",
    "        print(f\"    Compresi√≥n: {result['compression_ratio']:.2f}x\")\n",
    "        print(f\"    Speedup: {result['speedup']:.2f}x\")\n",
    "        print(f\"    Retenci√≥n AUC-ROC: {result.get('performance_retention_auc_roc', 0):.2f}%\")\n",
    "        print(f\"    Retenci√≥n AUC-PR: {result.get('performance_retention_auc_pr', 0):.2f}%\")\n",
    "\n",
    "print(f\"\\n‚úì Proyecto completado exitosamente\")\n",
    "print(f\"  Revisa los resultados en WandB: https://wandb.ai\")\n",
    "print(f\"  Proyecto: proyecto-ii-anomaly-detection\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Notas Finales\n",
    "\n",
    "### Instrucciones de Uso\n",
    "\n",
    "1. **Ejecutar en orden**: Ejecuta las celdas en orden secuencial desde el inicio\n",
    "2. **Google Drive**: Aseg√∫rate de tener montado Google Drive y que contenga:\n",
    "   - El dataset MVTec AD en la ruta especificada\n",
    "   - Los archivos de configuraci√≥n en `conf/` (opcional, se crean autom√°ticamente si no existen)\n",
    "3. **WandB**: Necesitas autenticarte con WandB cuando se ejecute `wandb.login()`\n",
    "4. **Tiempo de ejecuci√≥n**: El entrenamiento completo puede tardar varias horas dependiendo del hardware\n",
    "\n",
    "### Estructura del C√≥digo\n",
    "\n",
    "Todo el c√≥digo est√° incluido en este notebook:\n",
    "- ‚úÖ **Secci√≥n 1**: Definici√≥n de modelos (BasicBlock, CNNClassifier, UNetAutoencoder)\n",
    "- ‚úÖ **Secci√≥n 2**: M√≥dulos Lightning (CNNClassifierLightning, AutoencoderLightning, LossFunctions)\n",
    "- ‚úÖ **Secci√≥n 3**: Configuraci√≥n Hydra\n",
    "- ‚úÖ **Secci√≥n 4**: DataModule (MVTecDataModule, AnomalyDataset)\n",
    "- ‚úÖ **Secci√≥n 5**: Entrenamiento de modelos\n",
    "- ‚úÖ **Secci√≥n 6**: Evaluaci√≥n de anomal√≠as\n",
    "- ‚úÖ **Secci√≥n 7**: Cuantizaci√≥n\n",
    "- ‚úÖ **Secci√≥n 8**: An√°lisis DBSCAN\n",
    "- ‚úÖ **Secci√≥n 9**: Resumen final\n",
    "\n",
    "### Resultados\n",
    "\n",
    "Todos los resultados se guardan en:\n",
    "- **WandB**: M√©tricas, gr√°ficas y visualizaciones\n",
    "- **Google Drive**: Checkpoints de modelos, im√°genes y an√°lisis\n",
    "- **Consola**: Res√∫menes y m√©tricas principales\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
