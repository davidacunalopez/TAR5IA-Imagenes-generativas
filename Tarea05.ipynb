{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tarea 05 - Imágenes Generativas\n",
        "## Detección de Anomalías en Imágenes Industriales\n",
        "\n",
        "Este notebook implementa un sistema de detección de anomalías usando el dataset MVTec AD.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalación de dependencias necesarias\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "%pip install matplotlib numpy scikit-learn opencv-python pillow tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuración de rutas\n",
        "# Ajusta esta ruta según donde tengas el dataset en tu Google Drive\n",
        "DATASET_PATH = '/content/drive/MyDrive/Colab Notebooks/Tarea5-IA/dataset'  # Ajusta según tu estructura\n",
        "\n",
        "# Categorías disponibles\n",
        "CATEGORIES = ['cable', 'capsule', 'screw', 'transistor']\n",
        "\n",
        "# Selecciona la categoría a trabajar (puedes cambiar esto)\n",
        "SELECTED_CATEGORY = 'cable'  # Cambia a 'capsule', 'screw', o 'transistor' según necesites\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import cv2\n",
        "\n",
        "# Configurar dispositivo\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Usando dispositivo: {device}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Carga y Preprocesamiento de Datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AnomalyDataset(Dataset):\n",
        "    \"\"\"Dataset para cargar imágenes de entrenamiento y prueba\"\"\"\n",
        "    \n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image\n",
        "\n",
        "def load_dataset_paths(category_path, split='train'):\n",
        "    \"\"\"Carga las rutas de las imágenes del dataset\"\"\"\n",
        "    paths = []\n",
        "    split_path = os.path.join(category_path, split)\n",
        "    \n",
        "    if split == 'train':\n",
        "        # Solo imágenes 'good' en entrenamiento\n",
        "        good_path = os.path.join(split_path, 'good')\n",
        "        if os.path.exists(good_path):\n",
        "            for img_file in os.listdir(good_path):\n",
        "                if img_file.endswith('.png'):\n",
        "                    paths.append(os.path.join(good_path, img_file))\n",
        "    else:\n",
        "        # En test, cargar todas las clases (good y defectos)\n",
        "        if os.path.exists(split_path):\n",
        "            for class_name in os.listdir(split_path):\n",
        "                class_path = os.path.join(split_path, class_name)\n",
        "                if os.path.isdir(class_path):\n",
        "                    for img_file in os.listdir(class_path):\n",
        "                        if img_file.endswith('.png'):\n",
        "                            paths.append(os.path.join(class_path, img_file))\n",
        "    \n",
        "    return paths\n",
        "\n",
        "# Cargar rutas del dataset\n",
        "category_path = os.path.join(DATASET_PATH, SELECTED_CATEGORY)\n",
        "train_paths = load_dataset_paths(category_path, split='train')\n",
        "test_paths = load_dataset_paths(category_path, split='test')\n",
        "\n",
        "print(f'Imágenes de entrenamiento: {len(train_paths)}')\n",
        "print(f'Imágenes de prueba: {len(test_paths)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformaciones de datos\n",
        "image_size = 256\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Crear datasets\n",
        "train_dataset = AnomalyDataset(train_paths, transform=train_transform)\n",
        "test_dataset = AnomalyDataset(test_paths, transform=test_transform)\n",
        "\n",
        "# Crear dataloaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f'Batches de entrenamiento: {len(train_loader)}')\n",
        "print(f'Batches de prueba: {len(test_loader)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Definición del Modelo - Autoencoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    \"\"\"Autoencoder para detección de anomalías\"\"\"\n",
        "    \n",
        "    def __init__(self, input_channels=3, latent_dim=128):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            # 256x256 -> 128x128\n",
        "            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            \n",
        "            # 128x128 -> 64x64\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            \n",
        "            # 64x64 -> 32x32\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            \n",
        "            # 32x32 -> 16x16\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            \n",
        "            # 16x16 -> 8x8\n",
        "            nn.Conv2d(512, latent_dim, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            # 8x8 -> 16x16\n",
        "            nn.ConvTranspose2d(latent_dim, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            \n",
        "            # 16x16 -> 32x32\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            \n",
        "            # 32x32 -> 64x64\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            \n",
        "            # 64x64 -> 128x128\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            \n",
        "            # 128x128 -> 256x256\n",
        "            nn.ConvTranspose2d(64, input_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "# Crear modelo\n",
        "model = Autoencoder(input_channels=3, latent_dim=128).to(device)\n",
        "print(f'Modelo creado con {sum(p.numel() for p in model.parameters())} parámetros')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Entrenamiento del Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función de pérdida y optimizador\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "# Función de entrenamiento\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for batch_idx, images in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        reconstructed = model(images)\n",
        "        loss = criterion(reconstructed, images)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
        "    \n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "# Entrenar modelo\n",
        "num_epochs = 20\n",
        "train_losses = []\n",
        "\n",
        "print('Iniciando entrenamiento...')\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'\\nÉpoca {epoch+1}/{num_epochs}')\n",
        "    avg_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    train_losses.append(avg_loss)\n",
        "    scheduler.step()\n",
        "    print(f'Pérdida promedio: {avg_loss:.4f}')\n",
        "\n",
        "print('\\nEntrenamiento completado!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar curva de pérdida\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses)\n",
        "plt.title('Curva de Pérdida de Entrenamiento')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Pérdida (MSE)')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluación y Detección de Anomalías\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_anomaly_labels(test_paths, category_path):\n",
        "    \"\"\"Obtiene las etiquetas de anomalía (0=normal, 1=anomalía)\"\"\"\n",
        "    labels = []\n",
        "    good_path = os.path.join(category_path, 'test', 'good')\n",
        "    \n",
        "    for path in test_paths:\n",
        "        if good_path in path:\n",
        "            labels.append(0)  # Normal\n",
        "        else:\n",
        "            labels.append(1)  # Anomalía\n",
        "    \n",
        "    return np.array(labels)\n",
        "\n",
        "# Obtener etiquetas de prueba\n",
        "test_labels = get_anomaly_labels(test_paths, category_path)\n",
        "print(f'Imágenes normales: {np.sum(test_labels == 0)}')\n",
        "print(f'Imágenes con anomalías: {np.sum(test_labels == 1)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluar modelo en datos de prueba\n",
        "model.eval()\n",
        "anomaly_scores = []\n",
        "sample_images = []\n",
        "sample_reconstructions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, images in enumerate(test_loader):\n",
        "        images = images.to(device)\n",
        "        reconstructed = model(images)\n",
        "        \n",
        "        # Calcular error de reconstrucción (MSE) como score de anomalía\n",
        "        mse = torch.mean((images - reconstructed) ** 2, dim=(1, 2, 3))\n",
        "        anomaly_scores.extend(mse.cpu().numpy())\n",
        "        \n",
        "        # Guardar algunas muestras para visualización\n",
        "        if batch_idx == 0:\n",
        "            sample_images = images[:10].cpu()\n",
        "            sample_reconstructions = reconstructed[:10].cpu()\n",
        "\n",
        "anomaly_scores = np.array(anomaly_scores)\n",
        "print(f'Rango de scores: [{anomaly_scores.min():.4f}, {anomaly_scores.max():.4f}]')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular métricas\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n",
        "\n",
        "auc_score = roc_auc_score(test_labels, anomaly_scores)\n",
        "ap_score = average_precision_score(test_labels, anomaly_scores)\n",
        "\n",
        "print(f'AUC-ROC: {auc_score:.4f}')\n",
        "print(f'Average Precision: {ap_score:.4f}')\n",
        "\n",
        "# Curva ROC\n",
        "fpr, tpr, thresholds = roc_curve(test_labels, anomaly_scores)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.4f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualización de Resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "    \"\"\"Desnormaliza un tensor de imagen\"\"\"\n",
        "    for t, m, s in zip(tensor, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "    return tensor.clamp_(0, 1)\n",
        "\n",
        "# Visualizar reconstrucciones\n",
        "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(min(10, len(sample_images))):\n",
        "    # Imagen original\n",
        "    img = denormalize(sample_images[i].clone())\n",
        "    axes[i*2].imshow(img.permute(1, 2, 0))\n",
        "    axes[i*2].set_title('Original')\n",
        "    axes[i*2].axis('off')\n",
        "    \n",
        "    # Reconstrucción\n",
        "    recon = denormalize(sample_reconstructions[i].clone())\n",
        "    axes[i*2+1].imshow(recon.permute(1, 2, 0))\n",
        "    axes[i*2+1].set_title('Reconstrucción')\n",
        "    axes[i*2+1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar distribución de scores\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "normal_scores = anomaly_scores[test_labels == 0]\n",
        "anomaly_scores_plot = anomaly_scores[test_labels == 1]\n",
        "plt.hist(normal_scores, bins=50, alpha=0.7, label='Normal', color='green')\n",
        "plt.hist(anomaly_scores_plot, bins=50, alpha=0.7, label='Anomalía', color='red')\n",
        "plt.xlabel('Anomaly Score (MSE)')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.title('Distribución de Scores de Anomalía')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.boxplot([normal_scores, anomaly_scores_plot], labels=['Normal', 'Anomalía'])\n",
        "plt.ylabel('Anomaly Score (MSE)')\n",
        "plt.title('Boxplot de Scores')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encontrar mejores y peores reconstrucciones\n",
        "sorted_indices = np.argsort(anomaly_scores)\n",
        "\n",
        "print(\"Top 5 imágenes con menor error (más normales):\")\n",
        "for idx in sorted_indices[:5]:\n",
        "    print(f\"  {test_paths[idx]} - Score: {anomaly_scores[idx]:.4f}\")\n",
        "\n",
        "print(\"\\nTop 5 imágenes con mayor error (más anómalas):\")\n",
        "for idx in sorted_indices[-5:]:\n",
        "    print(f\"  {test_paths[idx]} - Score: {anomaly_scores[idx]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Guardar Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar modelo entrenado\n",
        "model_save_path = '/content/drive/MyDrive/Tarea5-IA/modelo_autoencoder.pth'\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'train_losses': train_losses,\n",
        "    'auc_score': auc_score,\n",
        "    'ap_score': ap_score\n",
        "}, model_save_path)\n",
        "\n",
        "print(f'Modelo guardado en: {model_save_path}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notas Finales\n",
        "\n",
        "- El modelo ha sido entrenado solo con imágenes normales (good)\n",
        "- Las anomalías se detectan mediante el error de reconstrucción\n",
        "- Puedes ajustar el umbral de detección según tus necesidades\n",
        "- Para cargar el modelo guardado, usa: `torch.load(model_save_path)`\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
