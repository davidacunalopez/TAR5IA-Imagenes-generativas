{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA-LqDSHRfDW"
      },
      "source": [
        "# Tarea 05 - Imágenes Generativas\n",
        "## Detección de Anomalías en Imágenes Industriales\n",
        "\n",
        "Este notebook implementa un sistema de detección de anomalías usando el dataset MVTec AD.\n",
        "\n",
        "**Objetivo**: Implementar modelos de autoencoder (clásico y U-net) para la reconstrucción de imágenes y detección de anomalías, utilizando:\n",
        "- **Pytorch Lightning** para la estructuración del entrenamiento\n",
        "- **Hydra** para la gestión modular de configuraciones\n",
        "- **WandB** para el monitoreo y comparación de experimentos\n",
        "\n",
        "**Estructura del Notebook**:\n",
        "1. **Configuración inicial**: Instalación de dependencias y montaje de Google Drive\n",
        "2. **Definición de modelos**: Arquitecturas de autoencoders (clásico y U-net)\n",
        "3. **Módulo Lightning**: Funciones de pérdida y callbacks de visualización\n",
        "4. **Configuración Hydra**: Gestión modular de hiperparámetros\n",
        "5. **Carga de datos**: Dataset MVTec AD con transformaciones\n",
        "6. **Entrenamiento**: Ejecución de experimentos (automatizada o individual)\n",
        "7. **Evaluación**: Métricas y visualizaciones de resultados\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd0XQyOXghnm"
      },
      "source": [
        "En la siguiente celda se instalan todas las librerías necesarias para el proyecto:\n",
        "- PyTorch y TorchVision para el manejo de redes neuronales\n",
        "- Pytorch Lightning para la estructuración del entrenamiento\n",
        "- Hydra para la gestión de configuraciones\n",
        "- WandB para el monitoreo de experimentos\n",
        "- Otras librerías auxiliares (matplotlib, scikit-learn, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XILPaemiRfDc",
        "outputId": "7966c55f-833d-4ff5-a3d9-04cb5d53c4f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.3)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.5.6-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting hydra-core\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (2.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.44.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (2.8.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.0)\n",
            "Collecting torchmetrics>0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core) (4.9.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.13.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.10.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.3)\n",
            "Downloading pytorch_lightning-2.5.6-py3-none-any.whl (831 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.6/831.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, hydra-core, torchmetrics, pytorch-lightning\n",
            "Successfully installed hydra-core-1.3.2 lightning-utilities-0.15.2 pytorch-lightning-2.5.6 torchmetrics-1.8.2\n"
          ]
        }
      ],
      "source": [
        "## 0. Configuración Inicial\n",
        "\n",
        "### 0.1. Instalación de Dependencias\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "%pip install matplotlib numpy scikit-learn opencv-python pillow tqdm wandb pytorch-lightning hydra-core omegaconf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY7lM8uTgwzb"
      },
      "source": [
        "Esta celda monta Google Drive para acceder a los archivos del proyecto (dataset, configuraciones, checkpoints, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "uSmxHlRFRfDe",
        "outputId": "6f6fb798-0b2a-402f-e584-b238e11f7ae0"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2540786305.py, line 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2540786305.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    Esta celda monta Google Drive para acceder a los archivos del proyecto (dataset, configuraciones, checkpoints, etc.).\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "### 0.2. Montar Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZf5X-m2cq1j"
      },
      "source": [
        "## 0.4. Definición de Arquitecturas de Modelos\n",
        "\n",
        "En esta sección se definen las dos arquitecturas de autoencoders que se utilizarán en el proyecto:\n",
        "\n",
        "- **Autoencoder Clásico**: Arquitectura tradicional sin skip connections\n",
        "  - Encoder: Convoluciones con BatchNorm y ReLU\n",
        "  - Decoder: Transconvoluciones con BatchNorm, ReLU y Tanh en la salida\n",
        "  - Función `encode()`: Extrae el vector latente\n",
        "  \n",
        "- **U-Net Autoencoder**: Arquitectura con skip connections para mejorar la reconstrucción\n",
        "  - Encoder: Convoluciones con skip connections guardadas\n",
        "  - Decoder: Transconvoluciones que concatenan con skip connections del encoder\n",
        "  - Función `encode()`: Extrae el vector latente y las skip connections\n",
        "\n",
        "**Características comunes**:\n",
        "- Imágenes de entrada: 128x128x3\n",
        "- Espacio latente configurable (por defecto: 128 canales)\n",
        "- Salida: 128x128x3 con valores en [-1, 1] (gracias a Tanh)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7AVTmBlcq1j"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Módulo con las arquitecturas de autoencoders para detección de anomalías\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class AutoencoderClassic(nn.Module):\n",
        "    \"\"\"Autoencoder clásico sin skip connections\"\"\"\n",
        "\n",
        "    def __init__(self, input_channels=3, latent_dim=128, encoder_channels=None, decoder_channels=None, architecture=None):\n",
        "        super(AutoencoderClassic, self).__init__()\n",
        "        self.architecture = architecture  # Store for logging purposes\n",
        "\n",
        "        if encoder_channels is None:\n",
        "            encoder_channels = [64, 128, 256, 512]\n",
        "        if decoder_channels is None:\n",
        "            decoder_channels = [512, 256, 128, 64]\n",
        "\n",
        "        # Encoder\n",
        "        encoder_layers = []\n",
        "        in_channels = input_channels\n",
        "\n",
        "        for out_channels in encoder_channels:\n",
        "            encoder_layers.extend([\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            ])\n",
        "            in_channels = out_channels\n",
        "\n",
        "        # Capa final del encoder\n",
        "        encoder_layers.extend([\n",
        "            nn.Conv2d(in_channels, latent_dim, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU()\n",
        "        ])\n",
        "\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_layers = []\n",
        "        in_channels = latent_dim\n",
        "\n",
        "        for out_channels in decoder_channels:\n",
        "            decoder_layers.extend([\n",
        "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            ])\n",
        "            in_channels = out_channels\n",
        "\n",
        "        # Capa final del decoder\n",
        "        decoder_layers.extend([\n",
        "            nn.ConvTranspose2d(in_channels, input_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        ])\n",
        "\n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"Extrae el vector latente de la entrada\"\"\"\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "\n",
        "class UNetAutoencoder(nn.Module):\n",
        "    \"\"\"Autoencoder U-net con skip connections\"\"\"\n",
        "\n",
        "    def __init__(self, input_channels=3, latent_dim=128, encoder_channels=None, decoder_channels=None, architecture=None):\n",
        "        super(UNetAutoencoder, self).__init__()\n",
        "        self.architecture = architecture  # Store for logging purposes\n",
        "\n",
        "        if encoder_channels is None:\n",
        "            encoder_channels = [64, 128, 256, 512]\n",
        "        if decoder_channels is None:\n",
        "            decoder_channels = [512, 256, 128, 64]\n",
        "\n",
        "        # Encoder con skip connections\n",
        "        self.encoder_blocks = nn.ModuleList()\n",
        "        in_channels = input_channels\n",
        "\n",
        "        for out_channels in encoder_channels:\n",
        "            self.encoder_blocks.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm2d(out_channels)\n",
        "                )\n",
        "            )\n",
        "            in_channels = out_channels\n",
        "\n",
        "        # Capa bottleneck\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, latent_dim, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder con skip connections\n",
        "        self.decoder_blocks = nn.ModuleList()\n",
        "        in_channels = latent_dim\n",
        "\n",
        "        # Primera capa del decoder (sin skip connection)\n",
        "        self.decoder_blocks.append(\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose2d(in_channels, decoder_channels[0], kernel_size=4, stride=2, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm2d(decoder_channels[0])\n",
        "            )\n",
        "        )\n",
        "        in_channels = decoder_channels[0]\n",
        "\n",
        "        # Resto de capas del decoder con skip connections\n",
        "        for i, out_channels in enumerate(decoder_channels[1:], 1):\n",
        "            # Duplicar canales de entrada para concatenar con skip connection\n",
        "            self.decoder_blocks.append(\n",
        "                nn.Sequential(\n",
        "                    nn.ConvTranspose2d(in_channels * 2, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm2d(out_channels)\n",
        "                )\n",
        "            )\n",
        "            in_channels = out_channels\n",
        "\n",
        "        # Capa final\n",
        "        self.final_layer = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels * 2, input_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"Extrae el vector latente de la entrada\"\"\"\n",
        "        # Encoder - guardar skip connections\n",
        "        skip_connections = []\n",
        "        for encoder_block in self.encoder_blocks:\n",
        "            x = encoder_block(x)\n",
        "            skip_connections.append(x)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "        return x, skip_connections\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder - guardar skip connections\n",
        "        skip_connections = []\n",
        "        for encoder_block in self.encoder_blocks:\n",
        "            x = encoder_block(x)\n",
        "            skip_connections.append(x)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        # Decoder - usar skip connections\n",
        "        # Primera capa del decoder (sin skip connection)\n",
        "        x = self.decoder_blocks[0](x)\n",
        "\n",
        "        # Resto de capas del decoder con skip connections\n",
        "        # Las skip connections están en orden: [64, 128, 256, 512] (índices 0, 1, 2, 3)\n",
        "        # Necesitamos usarlas en orden inverso: [512, 256, 128, 64]\n",
        "        # decoder_blocks[1:] son los bloques que necesitan skip connections\n",
        "        # Para decoder_blocks[1] (segundo bloque): necesita skip_connections[-1] = 512\n",
        "        # Para decoder_blocks[2] (tercer bloque): necesita skip_connections[-2] = 256\n",
        "        # Para decoder_blocks[3] (cuarto bloque): necesita skip_connections[-3] = 128\n",
        "        for i, decoder_block in enumerate(self.decoder_blocks[1:], start=1):\n",
        "            # Obtener skip connection correspondiente (en orden inverso)\n",
        "            # i va de 1 a 3, necesitamos índices -1, -2, -3\n",
        "            skip_idx = -i  # Para i=1: -1 (512), para i=2: -2 (256), para i=3: -3 (128)\n",
        "            skip = skip_connections[skip_idx]\n",
        "\n",
        "            # Asegurar que las dimensiones espaciales coincidan\n",
        "            if x.shape[2:] != skip.shape[2:]:\n",
        "                x = nn.functional.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "            # Concatenar con skip connection\n",
        "            x = torch.cat([x, skip], dim=1)\n",
        "            x = decoder_block(x)\n",
        "\n",
        "        # Capa final con último skip connection (skip_connections[0] = 64)\n",
        "        skip = skip_connections[0]\n",
        "        if x.shape[2:] != skip.shape[2:]:\n",
        "            x = nn.functional.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        x = self.final_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "print(\"✓ Arquitecturas de modelos definidas correctamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEZhsYpGcq1k"
      },
      "source": [
        "### Módulo de Pytorch Lightning\n",
        "\n",
        "En esta sección se define el módulo de Lightning que encapsula:\n",
        "- **Funciones de pérdida**: L1, L2, SSIM, SSIM+L1\n",
        "- **AutoencoderLightning**: Clase que gestiona el entrenamiento, validación y logging automático\n",
        "- **Callback de Visualización**: Genera visualizaciones periódicas durante el entrenamiento (reconstrucciones y t-SNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuyIe4Mxcq1l"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Módulo de Pytorch Lightning para el entrenamiento de autoencoders\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "from torchmetrics import StructuralSimilarityIndexMeasure\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class LossFunctions:\n",
        "    \"\"\"Funciones de pérdida para el entrenamiento\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def l1_loss(pred, target):\n",
        "        return F.l1_loss(pred, target)\n",
        "\n",
        "    @staticmethod\n",
        "    def l2_loss(pred, target):\n",
        "        return F.mse_loss(pred, target)\n",
        "\n",
        "    @staticmethod\n",
        "    def ssim_loss(pred, target):\n",
        "        # data_range=2.0 porque las imágenes están normalizadas a [-1, 1]\n",
        "        ssim = StructuralSimilarityIndexMeasure(data_range=2.0)\n",
        "        ssim_val = ssim(pred, target)\n",
        "        return 1 - ssim_val  # SSIM es una métrica de similitud, convertimos a pérdida\n",
        "\n",
        "    @staticmethod\n",
        "    def ssim_l1_loss(pred, target, alpha=0.5):\n",
        "        ssim = LossFunctions.ssim_loss(pred, target)\n",
        "        l1 = LossFunctions.l1_loss(pred, target)\n",
        "        return alpha * ssim + (1 - alpha) * l1\n",
        "\n",
        "\n",
        "class AutoencoderLightning(pl.LightningModule):\n",
        "    \"\"\"Módulo de Lightning para entrenar autoencoders\"\"\"\n",
        "\n",
        "    def __init__(self, model, learning_rate=0.001, loss_function=\"L2\", scheduler_config=None):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss_function = loss_function\n",
        "        self.scheduler_config = scheduler_config or {\"step_size\": 10, \"gamma\": 0.5}\n",
        "\n",
        "        # Inicializar función de pérdida\n",
        "        if loss_function == \"L1\":\n",
        "            self.criterion = LossFunctions.l1_loss\n",
        "        elif loss_function == \"L2\":\n",
        "            self.criterion = LossFunctions.l2_loss\n",
        "        elif loss_function == \"SSIM\":\n",
        "            self.criterion = LossFunctions.ssim_loss\n",
        "        elif loss_function == \"SSIM_L1\":\n",
        "            self.criterion = LossFunctions.ssim_l1_loss\n",
        "        else:\n",
        "            raise ValueError(f\"Función de pérdida no reconocida: {loss_function}\")\n",
        "\n",
        "        # Métricas\n",
        "        # data_range=2.0 porque las imágenes están normalizadas a [-1, 1]\n",
        "        self.ssim_metric = StructuralSimilarityIndexMeasure(data_range=2.0)\n",
        "\n",
        "        # Guardar pérdidas de entrenamiento\n",
        "        self.train_losses = []\n",
        "\n",
        "        # Guardar hiperparámetros\n",
        "        self.save_hyperparameters(ignore=['model'])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x = batch\n",
        "        x_recon = self(x)\n",
        "        loss = self.criterion(x_recon, x)\n",
        "\n",
        "        # Logging\n",
        "        self.log('train/loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('train/learning_rate', self.optimizers().param_groups[0]['lr'], on_step=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        # Guardar pérdida promedio de la época\n",
        "        epoch_loss = self.trainer.callback_metrics.get('train/loss_epoch', None)\n",
        "        if epoch_loss is not None:\n",
        "            self.train_losses.append(epoch_loss.item())\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x = batch\n",
        "        x_recon = self(x)\n",
        "        loss = self.criterion(x_recon, x)\n",
        "\n",
        "        # Calcular SSIM\n",
        "        ssim_val = self.ssim_metric(x_recon, x)\n",
        "\n",
        "        # Logging\n",
        "        self.log('val/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
        "        self.log('val/ssim', ssim_val, on_step=False, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "            optimizer,\n",
        "            step_size=self.scheduler_config.get(\"step_size\", 10),\n",
        "            gamma=self.scheduler_config.get(\"gamma\", 0.5)\n",
        "        )\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"interval\": \"epoch\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "print(\"✓ Módulo de Lightning definido correctamente\")\n",
        "\n",
        "# Callback para visualizaciones periódicas durante el entrenamiento\n",
        "from pytorch_lightning.callbacks import Callback\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "class VisualizationCallback(Callback):\n",
        "    \"\"\"Callback para generar visualizaciones periódicas durante el entrenamiento\"\"\"\n",
        "\n",
        "    def __init__(self, val_loader, device, log_every_n_epochs=5, num_samples=16):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            val_loader: DataLoader del set de validación\n",
        "            device: Dispositivo (cuda/cpu)\n",
        "            log_every_n_epochs: Cada cuántas épocas generar visualizaciones\n",
        "            num_samples: Número de imágenes a visualizar\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "        self.log_every_n_epochs = log_every_n_epochs\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        \"\"\"Genera visualizaciones al final de cada N épocas\"\"\"\n",
        "        current_epoch = trainer.current_epoch\n",
        "\n",
        "        # Solo generar visualizaciones cada N épocas\n",
        "        if current_epoch % self.log_every_n_epochs == 0 or current_epoch == trainer.max_epochs - 1:\n",
        "            pl_module.eval()\n",
        "\n",
        "            # Función auxiliar para denormalizar\n",
        "            def denormalize(tensor, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]):\n",
        "                for t, m, s in zip(tensor, mean, std):\n",
        "                    t.mul_(s).add_(m)\n",
        "                return tensor.clamp_(0, 1)\n",
        "\n",
        "            val_images = []\n",
        "            val_reconstructions = []\n",
        "            val_latent_vectors = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_idx, images in enumerate(self.val_loader):\n",
        "                    images = images.to(self.device)\n",
        "\n",
        "                    # Obtener reconstrucciones\n",
        "                    reconstructed = pl_module.model(images)\n",
        "\n",
        "                    # Extraer vectores latentes\n",
        "                    if hasattr(pl_module.model, 'encode'):\n",
        "                        latent = pl_module.model.encode(images)\n",
        "                        # Para UNet, encode devuelve (latent, skip_connections), solo necesitamos latent\n",
        "                        if isinstance(latent, tuple):\n",
        "                            latent = latent[0]\n",
        "                        # Aplanar el vector latente para t-SNE\n",
        "                        latent_flat = latent.view(latent.size(0), -1)\n",
        "                        val_latent_vectors.append(latent_flat.cpu().numpy())\n",
        "\n",
        "                    # Guardar imágenes para visualización (hasta num_samples)\n",
        "                    if len(val_images) < self.num_samples:\n",
        "                        remaining = self.num_samples - len(val_images)\n",
        "                        val_images.append(images[:remaining].cpu())\n",
        "                        val_reconstructions.append(reconstructed[:remaining].cpu())\n",
        "\n",
        "                    if len(val_images) >= self.num_samples:\n",
        "                        break\n",
        "\n",
        "            # Concatenar todas las imágenes\n",
        "            if val_images:\n",
        "                val_images = torch.cat(val_images[:self.num_samples])[:self.num_samples]\n",
        "                val_reconstructions = torch.cat(val_reconstructions[:self.num_samples])[:self.num_samples]\n",
        "\n",
        "                # Visualizar reconstrucciones del set de validación\n",
        "                import matplotlib.pyplot as plt\n",
        "                import numpy as np\n",
        "                import wandb\n",
        "\n",
        "                fig, axes = plt.subplots(4, 8, figsize=(20, 10))\n",
        "                axes = axes.flatten()\n",
        "                val_reconstruction_images = []\n",
        "\n",
        "                for i in range(min(self.num_samples, len(val_images))):\n",
        "                    # Imagen original\n",
        "                    img = denormalize(val_images[i].clone())\n",
        "                    axes[i*2].imshow(img.permute(1, 2, 0))\n",
        "                    axes[i*2].set_title(f'Original {i+1}')\n",
        "                    axes[i*2].axis('off')\n",
        "\n",
        "                    # Reconstrucción\n",
        "                    recon = denormalize(val_reconstructions[i].clone())\n",
        "                    axes[i*2+1].imshow(recon.permute(1, 2, 0))\n",
        "                    axes[i*2+1].set_title(f'Reconstrucción {i+1}')\n",
        "                    axes[i*2+1].axis('off')\n",
        "\n",
        "                    # Preparar para wandb\n",
        "                    img_np = (img.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
        "                    recon_np = (recon.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
        "                    val_reconstruction_images.append(wandb.Image(img_np, caption=f\"Original Val {i+1}\"))\n",
        "                    val_reconstruction_images.append(wandb.Image(recon_np, caption=f\"Reconstrucción Val {i+1}\"))\n",
        "\n",
        "                plt.tight_layout()\n",
        "\n",
        "                # Loggear reconstrucciones en wandb\n",
        "                if trainer.logger:\n",
        "                    trainer.logger.experiment.log({\n",
        "                        f\"val/reconstructions_epoch_{current_epoch}\": val_reconstruction_images,\n",
        "                    })\n",
        "\n",
        "                plt.close(fig)\n",
        "\n",
        "                # Visualización t-SNE del espacio latente\n",
        "                if val_latent_vectors:\n",
        "                    all_latent_vectors = np.concatenate(val_latent_vectors, axis=0)\n",
        "\n",
        "                    # Aplicar t-SNE (reducir a 2D para visualización)\n",
        "                    perplexity_val = min(30, len(all_latent_vectors) - 1)\n",
        "                    if perplexity_val > 0:\n",
        "                        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_val)\n",
        "                        latent_2d = tsne.fit_transform(all_latent_vectors)\n",
        "\n",
        "                        # Visualizar t-SNE\n",
        "                        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "                        ax.scatter(latent_2d[:, 0], latent_2d[:, 1], alpha=0.6, s=50)\n",
        "                        ax.set_title(f't-SNE del Espacio Latente (Época {current_epoch})')\n",
        "                        ax.set_xlabel('Componente t-SNE 1')\n",
        "                        ax.set_ylabel('Componente t-SNE 2')\n",
        "                        ax.grid(True, alpha=0.3)\n",
        "                        plt.tight_layout()\n",
        "\n",
        "                        # Loggear t-SNE en wandb\n",
        "                        if trainer.logger:\n",
        "                            trainer.logger.experiment.log({\n",
        "                                f\"val/tsne_latent_epoch_{current_epoch}\": wandb.Image(fig),\n",
        "                            })\n",
        "\n",
        "                        plt.close(fig)\n",
        "\n",
        "            pl_module.train()\n",
        "\n",
        "print(\"✓ Callback de visualización definido correctamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_TYqV4ecq1m"
      },
      "source": [
        "## 0.3. Configuración con Hydra\n",
        "\n",
        "En esta sección se configura el proyecto usando **Hydra** para la gestión modular de configuraciones, cumpliendo con los requisitos del enunciado.\n",
        "\n",
        "**Estructura de configuración (según enunciado)**:\n",
        "```\n",
        "conf/\n",
        "├── config.yaml          # Configuración principal\n",
        "├── model/               # Configuraciones de modelos\n",
        "│   ├── autoencoder_classic.yaml\n",
        "│   └── unet.yaml\n",
        "├── trainer/             # Configuración del entrenamiento\n",
        "│   └── default.yaml\n",
        "└── logger/              # Configuración de WandB\n",
        "    └── wandb.yaml\n",
        "```\n",
        "\n",
        "**Características**:\n",
        "- ✅ Configuración centralizada en archivos YAML\n",
        "- ✅ Separación modular de hiperparámetros (modelo, entrenamiento, logger)\n",
        "- ✅ Permite cambiar hiperparámetros fácilmente usando overrides\n",
        "- ✅ Integración con WandB para tracking de experimentos\n",
        "- ✅ Permite ejecutar experimentos con distintos parámetros (dimensión latente, épocas, batch size, etc.)\n",
        "\n",
        "**Ubicación de archivos**:\n",
        "- Los archivos de configuración pueden estar en el directorio actual (`/content/conf`) o en Google Drive\n",
        "- El código busca automáticamente en ambas ubicaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I2DMIBNg9rh"
      },
      "source": [
        "\n",
        "### 0.3.1. Inicialización de Hydra y Configuración\n",
        "La siguiente celda:\n",
        "- Inicializa Hydra con la configuración del proyecto\n",
        "- Crea los archivos de configuración si no existen\n",
        "- Registra las clases del notebook para que Hydra pueda instanciarlas\n",
        "- Carga la configuración base del proyecto\n",
        "- Configura la autenticación de WandB\n",
        "\n",
        "**Importante**: La siguiente celda debe ejecutarse antes de cualquier otra que use la configuración de Hydra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAsZHklaRfDe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n",
        "import cv2\n",
        "import hydra\n",
        "from omegaconf import DictConfig, OmegaConf\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "%pip show hydra-core\n",
        "%pip install --upgrade hydra-core\n",
        "\n",
        "# Los módulos ya están definidos en las celdas anteriores\n",
        "# No es necesario importarlos desde archivos externos\n",
        "\n",
        "# Autenticación de Weights & Biases\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "# Inicializar Hydra para ejecutar múltiples configuraciones\n",
        "# En Colab, usamos hydra.initialize() en lugar del decorador @hydra.main\n",
        "# IMPORTANTE: Hydra requiere que config_path sea una ruta RELATIVA, no absoluta\n",
        "# Los archivos de configuración deben estar en el directorio de trabajo o en Google Drive\n",
        "\n",
        "# Configuración: Usar solo Google Drive\n",
        "# Definir ruta base de Google Drive\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Tarea5-IA'\n",
        "drive_conf_dir = os.path.join(DRIVE_BASE_PATH, 'conf')\n",
        "\n",
        "# Crear directorio base si no existe\n",
        "os.makedirs(DRIVE_BASE_PATH, exist_ok=True)\n",
        "\n",
        "print(f\"🔍 Configurando para usar solo Google Drive...\")\n",
        "print(f\"   Ruta base: {DRIVE_BASE_PATH}\")\n",
        "print(f\"   Configuración: {drive_conf_dir}\")\n",
        "\n",
        "# Verificar si existe en Google Drive\n",
        "config_file_drive = os.path.join(drive_conf_dir, 'config.yaml')\n",
        "\n",
        "if os.path.exists(config_file_drive):\n",
        "    # Usar configuración de Google Drive\n",
        "    print(f\"✓ Usando configuración de Google Drive: {drive_conf_dir}\")\n",
        "    # Copiar temporalmente al directorio actual para Hydra (requiere ruta relativa)\n",
        "    import shutil\n",
        "    current_dir = os.getcwd()\n",
        "    conf_dir_temp = os.path.join(current_dir, 'conf')\n",
        "\n",
        "    # Limpiar directorio temporal si existe\n",
        "    if os.path.exists(conf_dir_temp):\n",
        "        shutil.rmtree(conf_dir_temp)\n",
        "\n",
        "    # Copiar configuración de Google Drive al directorio temporal\n",
        "    shutil.copytree(drive_conf_dir, conf_dir_temp)\n",
        "    print(f\"  ✓ Configuración copiada temporalmente a: {conf_dir_temp}\")\n",
        "    config_path_for_hydra = 'conf'  # Ruta relativa\n",
        "else:\n",
        "    # Si no existe, crear los archivos de configuración básicos en Google Drive\n",
        "    print(f\"⚠️ No se encontraron archivos de configuración en Google Drive\")\n",
        "    print(f\"   Creando estructura y archivos de configuración básicos en: {drive_conf_dir}\")\n",
        "\n",
        "    # Crear estructura de directorios en Google Drive\n",
        "    os.makedirs(drive_conf_dir, exist_ok=True)\n",
        "    os.makedirs(os.path.join(drive_conf_dir, 'model'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(drive_conf_dir, 'trainer'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(drive_conf_dir, 'logger'), exist_ok=True)\n",
        "\n",
        "    # Crear archivo config.yaml básico\n",
        "    config_yaml_content = f\"\"\"# Configuración principal del proyecto\n",
        "defaults:\n",
        "  - model: autoencoder_classic\n",
        "  - trainer: default\n",
        "  - logger: wandb\n",
        "\n",
        "# Configuración del dataset\n",
        "dataset:\n",
        "  path: \"{DRIVE_BASE_PATH}/dataset\"\n",
        "  categories: [\"cable\", \"capsule\", \"screw\", \"transistor\"]\n",
        "  selected_category: \"cable\"\n",
        "  image_size: 128\n",
        "  batch_size: 32\n",
        "  num_workers: 2\n",
        "\n",
        "# Configuración del modelo\n",
        "# Los parámetros específicos del modelo están en conf/model/\n",
        "\n",
        "# Configuración del entrenamiento\n",
        "trainer:\n",
        "  max_epochs: 20\n",
        "  learning_rate: 0.001\n",
        "  loss_function: \"L2\"\n",
        "  scheduler:\n",
        "    step_size: 10\n",
        "    gamma: 0.5\n",
        "\n",
        "# Configuración del logger\n",
        "logger:\n",
        "  project: \"tarea05-anomaly-detection\"\n",
        "  name: null\n",
        "\n",
        "# Configuración del dispositivo\n",
        "device: \"cuda\"\n",
        "\"\"\"\n",
        "\n",
        "    with open(config_file_drive, 'w', encoding='utf-8') as f:\n",
        "        f.write(config_yaml_content)\n",
        "    print(f\"  ✓ Creado: config.yaml\")\n",
        "\n",
        "    # Crear archivo autoencoder_classic.yaml\n",
        "    # NOTA: Las clases están definidas en el notebook y se registran como notebook_models\n",
        "    autoencoder_yaml = \"\"\"# Configuración del autoencoder clásico\n",
        "# NOTA: _target_ apunta a la clase registrada en notebook_models\n",
        "_target_: notebook_models.AutoencoderClassic\n",
        "\n",
        "architecture: \"autoencoder_classic\"\n",
        "\n",
        "input_channels: 3\n",
        "latent_dim: 128\n",
        "\n",
        "encoder_channels: [64, 128, 256, 512]\n",
        "decoder_channels: [512, 256, 128, 64]\n",
        "\"\"\"\n",
        "    with open(os.path.join(drive_conf_dir, 'model', 'autoencoder_classic.yaml'), 'w', encoding='utf-8') as f:\n",
        "        f.write(autoencoder_yaml)\n",
        "    print(f\"  ✓ Creado: model/autoencoder_classic.yaml\")\n",
        "\n",
        "    # Crear archivo unet.yaml\n",
        "    unet_yaml = \"\"\"# Configuración del autoencoder U-net con skip connections\n",
        "# NOTA: _target_ apunta a la clase registrada en notebook_models\n",
        "_target_: notebook_models.UNetAutoencoder\n",
        "\n",
        "architecture: \"unet\"\n",
        "\n",
        "input_channels: 3\n",
        "latent_dim: 128\n",
        "\n",
        "encoder_channels: [64, 128, 256, 512]\n",
        "decoder_channels: [512, 256, 128, 64]\n",
        "\"\"\"\n",
        "    with open(os.path.join(drive_conf_dir, 'model', 'unet.yaml'), 'w', encoding='utf-8') as f:\n",
        "        f.write(unet_yaml)\n",
        "    print(f\"  ✓ Creado: model/unet.yaml\")\n",
        "\n",
        "    # Crear archivo default.yaml para trainer\n",
        "    trainer_yaml = \"\"\"# Configuración del entrenador\n",
        "max_epochs: 20\n",
        "learning_rate: 0.001\n",
        "loss_function: \"L2\"\n",
        "scheduler:\n",
        "  step_size: 10\n",
        "  gamma: 0.5\n",
        "\"\"\"\n",
        "    with open(os.path.join(drive_conf_dir, 'trainer', 'default.yaml'), 'w', encoding='utf-8') as f:\n",
        "        f.write(trainer_yaml)\n",
        "    print(f\"  ✓ Creado: trainer/default.yaml\")\n",
        "\n",
        "    # Crear archivo wandb.yaml\n",
        "    wandb_yaml = f\"\"\"# Configuración de WandB\n",
        "project: \"tarea05-anomaly-detection\"\n",
        "name: null\n",
        "save_dir: \"{DRIVE_BASE_PATH}/wandb_logs\"\n",
        "\"\"\"\n",
        "    with open(os.path.join(drive_conf_dir, 'logger', 'wandb.yaml'), 'w', encoding='utf-8') as f:\n",
        "        f.write(wandb_yaml)\n",
        "    print(f\"  ✓ Creado: logger/wandb.yaml\")\n",
        "\n",
        "    print(f\"✓ Estructura de configuración creada en: {drive_conf_dir}\")\n",
        "\n",
        "    # Copiar temporalmente al directorio actual para Hydra\n",
        "    import shutil\n",
        "    current_dir = os.getcwd()\n",
        "    conf_dir_temp = os.path.join(current_dir, 'conf')\n",
        "    if os.path.exists(conf_dir_temp):\n",
        "        shutil.rmtree(conf_dir_temp)\n",
        "    shutil.copytree(drive_conf_dir, conf_dir_temp)\n",
        "    print(f\"  ✓ Configuración copiada temporalmente a: {conf_dir_temp}\")\n",
        "    config_path_for_hydra = 'conf'  # Ruta relativa\n",
        "\n",
        "# Verificar que el archivo config.yaml existe antes de inicializar Hydra\n",
        "# Usar el archivo temporal para Hydra\n",
        "# IMPORTANTE: Definir current_dir ANTES de usarlo\n",
        "current_dir = os.getcwd()\n",
        "conf_dir_temp = os.path.join(current_dir, 'conf')\n",
        "config_file = os.path.join(conf_dir_temp, 'config.yaml')\n",
        "\n",
        "if not os.path.exists(config_file):\n",
        "    raise FileNotFoundError(\n",
        "        f\"❌ ERROR: No se pudo crear o encontrar config.yaml\\n\"\n",
        "        f\"Se esperaba en: {config_file}\\n\"\n",
        "        f\"Directorio actual: {current_dir}\\n\"\n",
        "        f\"Por favor, verifica que los archivos estén en Google Drive: {drive_conf_dir}\"\n",
        "    )\n",
        "\n",
        "print(f\"✓ Archivo config.yaml verificado: {config_file}\")\n",
        "\n",
        "# Limpiar Hydra si ya está inicializado (útil si se ejecuta la celda múltiples veces)\n",
        "from hydra.core.global_hydra import GlobalHydra\n",
        "if GlobalHydra.instance().is_initialized():\n",
        "    GlobalHydra.instance().clear()\n",
        "    print(\"✓ Limpiando instancia previa de Hydra\")\n",
        "\n",
        "# IMPORTANTE: Asegurarnos de estar en el directorio correcto ANTES de inicializar Hydra\n",
        "# Hydra usa el directorio de trabajo actual para buscar la configuración\n",
        "# Si no estamos en el directorio correcto, Hydra buscará en /tmp\n",
        "os.chdir(current_dir)\n",
        "actual_dir = os.getcwd()\n",
        "print(f\"✓ Directorio de trabajo configurado: {actual_dir}\")\n",
        "\n",
        "# Verificar que el directorio conf existe en el directorio actual\n",
        "if not os.path.exists(conf_dir_temp):\n",
        "    raise FileNotFoundError(\n",
        "        f\"❌ ERROR: No se encontró el directorio conf en {actual_dir}\\n\"\n",
        "        f\"Se esperaba: {conf_dir_temp}\\n\"\n",
        "        f\"Por favor, verifica que los archivos de configuración estén en Google Drive: {drive_conf_dir}\"\n",
        "    )\n",
        "\n",
        "# Verificar que config.yaml existe\n",
        "if not os.path.exists(config_file):\n",
        "    raise FileNotFoundError(\n",
        "        f\"❌ ERROR: No se encontró config.yaml en {conf_dir_temp}\\n\"\n",
        "        f\"Por favor, verifica que los archivos de configuración estén en Google Drive: {drive_conf_dir}\"\n",
        "    )\n",
        "\n",
        "print(f\"✓ Verificado: {config_file} existe\")\n",
        "\n",
        "# Inicializar Hydra con ruta relativa\n",
        "# Hydra requiere que config_path sea relativa al directorio de trabajo actual\n",
        "# IMPORTANTE:\n",
        "# - config_path debe ser 'conf' (sin barra inicial, sin ruta absoluta)\n",
        "# - El directorio conf debe estar en el directorio de trabajo actual\n",
        "# - Hydra buscará en: <directorio_actual>/conf/\n",
        "\n",
        "# Verificar una vez más que estamos en el directorio correcto\n",
        "final_dir = os.getcwd()\n",
        "print(f\"✓ Directorio final antes de inicializar Hydra: {final_dir}\")\n",
        "print(f\"✓ Verificando que conf existe: {os.path.exists(conf_dir_temp)}\")\n",
        "print(f\"✓ Verificando que config.yaml existe: {os.path.exists(config_file)}\")\n",
        "\n",
        "# Inicializar Hydra\n",
        "# IMPORTANTE: Hydra puede cambiar el directorio de trabajo cuando se inicializa\n",
        "# Para evitar que busque en /tmp, necesitamos asegurarnos de que:\n",
        "# 1. El directorio de trabajo actual sea /content (donde está conf/)\n",
        "# 2. El directorio conf/ esté en el directorio de trabajo actual\n",
        "# 3. Usar job_name para evitar que Hydra cree directorios temporales\n",
        "\n",
        "# Verificar que estamos en /content\n",
        "if os.getcwd() != '/content':\n",
        "    print(f\"⚠️ ADVERTENCIA: El directorio de trabajo es {os.getcwd()}, no /content\")\n",
        "    print(f\"   Cambiando a /content...\")\n",
        "    os.chdir('/content')\n",
        "    print(f\"   Nuevo directorio: {os.getcwd()}\")\n",
        "\n",
        "# Verificar nuevamente que conf existe\n",
        "if not os.path.exists('/content/conf'):\n",
        "    print(f\"❌ ERROR: No se encontró /content/conf\")\n",
        "    print(f\"   Copiando desde Google Drive...\")\n",
        "    import shutil\n",
        "    if os.path.exists(drive_conf_dir):\n",
        "        if os.path.exists('/content/conf'):\n",
        "            shutil.rmtree('/content/conf')\n",
        "        shutil.copytree(drive_conf_dir, '/content/conf')\n",
        "        print(f\"   ✓ Copiado a /content/conf\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"No se encontró {drive_conf_dir}\")\n",
        "\n",
        "try:\n",
        "    print(f\"✓ Inicializando Hydra con config_path='{config_path_for_hydra}' desde directorio: {os.getcwd()}\")\n",
        "    # Usar job_name para evitar que Hydra cree directorios temporales\n",
        "    hydra.initialize(config_path=config_path_for_hydra, version_base=None, job_name=\"notebook\")\n",
        "    print(\"✓ Hydra inicializado correctamente\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error al inicializar Hydra: {e}\")\n",
        "    print(f\"   Directorio actual: {os.getcwd()}\")\n",
        "    print(f\"   Conf dir esperado: /content/conf\")\n",
        "    print(f\"   Conf dir existe: {os.path.exists('/content/conf')}\")\n",
        "    if os.path.exists('/content/conf'):\n",
        "        print(f\"   Contenido de /content/conf: {os.listdir('/content/conf')}\")\n",
        "    raise\n",
        "\n",
        "# IMPORTANTE: Registrar las clases del notebook en el resolver de Hydra\n",
        "# Esto permite que instantiate() encuentre las clases definidas en el notebook\n",
        "from hydra.core.global_hydra import GlobalHydra\n",
        "from hydra.core.config_store import ConfigStore\n",
        "\n",
        "# Crear un módulo temporal para registrar las clases\n",
        "import sys\n",
        "import types\n",
        "\n",
        "# Crear un módulo temporal 'notebook_models' que contenga las clases\n",
        "notebook_models = types.ModuleType('notebook_models')\n",
        "notebook_models.AutoencoderClassic = AutoencoderClassic\n",
        "notebook_models.UNetAutoencoder = UNetAutoencoder\n",
        "sys.modules['notebook_models'] = notebook_models\n",
        "\n",
        "# Actualizar los archivos YAML para usar notebook_models\n",
        "# Actualizar tanto en Google Drive como en el directorio temporal\n",
        "for yaml_file in [os.path.join(drive_conf_dir, 'model', 'autoencoder_classic.yaml'),\n",
        "                  os.path.join(drive_conf_dir, 'model', 'unet.yaml'),\n",
        "                  os.path.join(conf_dir_temp, 'model', 'autoencoder_classic.yaml'),\n",
        "                  os.path.join(conf_dir_temp, 'model', 'unet.yaml')]:\n",
        "    if os.path.exists(yaml_file):\n",
        "        with open(yaml_file, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "        # Reemplazar _target_ si apunta a AutoencoderClassic o UNetAutoencoder directamente\n",
        "        if '_target_: AutoencoderClassic' in content or '_target_: UNetAutoencoder' in content:\n",
        "            content = content.replace('_target_: AutoencoderClassic', '_target_: notebook_models.AutoencoderClassic')\n",
        "            content = content.replace('_target_: UNetAutoencoder', '_target_: notebook_models.UNetAutoencoder')\n",
        "            with open(yaml_file, 'w', encoding='utf-8') as f:\n",
        "                f.write(content)\n",
        "            print(f\"✓ Actualizado: {os.path.basename(yaml_file)}\")\n",
        "\n",
        "print(\"✓ Clases del notebook registradas para Hydra\")\n",
        "\n",
        "# Componer la configuración (permite override desde parámetros)\n",
        "\n",
        "print(os.getcwd())\n",
        "try:\n",
        "  print(os.path.exists('/content/conf/config.yaml'))\n",
        "  os.chdir('/content/conf')\n",
        "  print(os.getcwd())\n",
        "  cfg = hydra.compose(config_name=\"config\")\n",
        "except Exception as e:\n",
        "  print(f\"❌ Error al cargar la configuración: {e}\")\n",
        "  raise\n",
        "print(cfg)\n",
        "\n",
        "# Ejemplos de cómo usar overrides para cambiar configuraciones:\n",
        "#\n",
        "# 1. Cambiar arquitectura a U-net:\n",
        "#    cfg = hydra.compose(config_name=\"config\", overrides=[\"model=unet\"])\n",
        "#\n",
        "# 2. Cambiar función de pérdida:\n",
        "#    cfg = hydra.compose(config_name=\"config\", overrides=[\"trainer.loss_function=SSIM\"])\n",
        "#\n",
        "# 3. Cambiar learning rate:\n",
        "#    cfg = hydra.compose(config_name=\"config\", overrides=[\"trainer.learning_rate=0.0005\"])\n",
        "#\n",
        "# 4. Cambiar dimensión del espacio latente:\n",
        "#    cfg = hydra.compose(config_name=\"config\", overrides=[\"model.latent_dim=256\"])\n",
        "#\n",
        "# 5. Cambiar categoría del dataset:\n",
        "#    cfg = hydra.compose(config_name=\"config\", overrides=[\"dataset.selected_category=capsule\"])\n",
        "#\n",
        "# 6. Múltiples overrides:\n",
        "#    cfg = hydra.compose(config_name=\"config\", overrides=[\"model=unet\", \"trainer.loss_function=SSIM_L1\", \"trainer.max_epochs=30\"])\n",
        "\n",
        "print(\"Configuración cargada con Hydra:\")\n",
        "print(OmegaConf.to_yaml(cfg))\n",
        "\n",
        "# Extraer valores de configuración\n",
        "DATASET_PATH = cfg.dataset.path\n",
        "SELECTED_CATEGORY = cfg.dataset.selected_category\n",
        "IMAGE_SIZE = cfg.dataset.image_size\n",
        "BATCH_SIZE = cfg.dataset.batch_size\n",
        "NUM_WORKERS = cfg.dataset.num_workers\n",
        "\n",
        "# Configurar dispositivo\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Usando dispositivo: {device}')\n",
        "\n",
        "# La variable DRIVE_BASE_PATH está disponible para todo el notebook\n",
        "# Se usa para guardar modelos, imágenes, resultados, etc. en Google Drive\n",
        "print(f'✓ Ruta base de Google Drive configurada: {DRIVE_BASE_PATH}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-Ylf5VU4XJ3"
      },
      "outputs": [],
      "source": [
        "# Esta celda está duplicada - el montaje de Google Drive ya se hizo en la celda 2\n",
        "# Puedes omitir esta celda si ya ejecutaste la celda 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7St3OVrvRfDe"
      },
      "source": [
        "## 1. Carga y Preprocesamiento de Datos\n",
        "\n",
        "En esta sección se realiza:\n",
        "- **Carga del dataset MVTec AD**: Se cargan las rutas de las imágenes de entrenamiento y prueba\n",
        "- **Definición de transformaciones**: Resize a 128x128, normalización y conversión a tensores\n",
        "- **Creación de DataLoaders**: Preparación de los datos para el entrenamiento con división train/validation (80-20)\n",
        "\n",
        "### Nota Importante sobre Normalización\n",
        "\n",
        "**Corrección aplicada**: Las transformaciones utilizan normalización a **[-1, 1]** en lugar de las estadísticas de ImageNet. Esto es necesario porque:\n",
        "- El decoder del modelo utiliza `Tanh()` como función de activación final, que produce valores en el rango **[-1, 1]**\n",
        "- Si las imágenes de entrada están normalizadas con estadísticas de ImageNet (rango aproximado [-2.5, 2.5]), hay una incompatibilidad de rangos\n",
        "- La normalización a **[-1, 1]** se logra con `Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])` después de `ToTensor()` que convierte a [0, 1]\n",
        "- Esta corrección permite que el modelo reconstruya correctamente las imágenes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HieyVehhJPn"
      },
      "source": [
        "### 1.1. Definición del Dataset y Carga de Rutas\n",
        "La siguiente celda:\n",
        "- Define la clase `AnomalyDataset` para cargar imágenes del dataset MVTec AD\n",
        "- Define la función `load_dataset_paths` para obtener las rutas de las imágenes\n",
        "- Carga las rutas de entrenamiento (solo imágenes 'good') y prueba (todas las clases)\n",
        "- Muestra el número de imágenes en cada conjunto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pFqCaYDRfDf"
      },
      "outputs": [],
      "source": [
        "class AnomalyDataset(Dataset):\n",
        "    \"\"\"Dataset para cargar imágenes de entrenamiento y prueba\"\"\"\n",
        "\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "def load_dataset_paths(category_path, split='train'):\n",
        "    \"\"\"Carga las rutas de las imágenes del dataset\"\"\"\n",
        "    paths = []\n",
        "    split_path = os.path.join(category_path, split)\n",
        "\n",
        "    if split == 'train':\n",
        "        # Solo imágenes 'good' en entrenamiento\n",
        "        good_path = os.path.join(split_path, 'good')\n",
        "        if os.path.exists(good_path):\n",
        "            for img_file in os.listdir(good_path):\n",
        "                if img_file.endswith('.png'):\n",
        "                    paths.append(os.path.join(good_path, img_file))\n",
        "    else:\n",
        "        # En test, cargar todas las clases (good y defectos)\n",
        "        if os.path.exists(split_path):\n",
        "            for class_name in os.listdir(split_path):\n",
        "                class_path = os.path.join(split_path, class_name)\n",
        "                if os.path.isdir(class_path):\n",
        "                    for img_file in os.listdir(class_path):\n",
        "                        if img_file.endswith('.png'):\n",
        "                            paths.append(os.path.join(class_path, img_file))\n",
        "\n",
        "    return paths\n",
        "\n",
        "# Cargar rutas del dataset\n",
        "category_path = os.path.join(DATASET_PATH, SELECTED_CATEGORY)\n",
        "train_paths = load_dataset_paths(category_path, split='train')\n",
        "test_paths = load_dataset_paths(category_path, split='test')\n",
        "\n",
        "print(f'Imágenes de entrenamiento: {len(train_paths)}')\n",
        "print(f'Imágenes de prueba: {len(test_paths)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IumSciYhTki"
      },
      "source": [
        "Esta celda:\n",
        "- Define las transformaciones de datos (resize, normalización a [-1, 1])\n",
        "- Crea los datasets de entrenamiento y prueba\n",
        "- Divide el set de entrenamiento en train/validation (80-20)\n",
        "- Crea los DataLoaders para cada conjunto\n",
        "\n",
        "**Nota sobre normalización**: Se usa normalización a [-1, 1] para compatibilidad con `Tanh()` en el decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsUYNRV6RfDf"
      },
      "outputs": [],
      "source": [
        "# Transformaciones de datos (usando configuración de Hydra)\n",
        "# IMPORTANTE: Normalización a [-1, 1] para compatibilidad con Tanh() en el decoder\n",
        "# ToTensor() convierte a [0, 1], luego normalizamos a [-1, 1] con (x - 0.5) / 0.5\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normaliza a [-1, 1]\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normaliza a [-1, 1]\n",
        "])\n",
        "\n",
        "# Crear datasets\n",
        "train_dataset = AnomalyDataset(train_paths, transform=train_transform)\n",
        "test_dataset = AnomalyDataset(test_paths, transform=test_transform)\n",
        "\n",
        "# Crear dataloaders (usando configuración de Hydra)\n",
        "# Dividir el set de entrenamiento en train y validation (80-20) para WandB\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset_split, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset_split, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "print(f'Batches de entrenamiento: {len(train_loader)}')\n",
        "print(f'Batches de validación: {len(val_loader)}')\n",
        "print(f'Batches de prueba: {len(test_loader)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUxrqKI8RfDf"
      },
      "source": [
        "## 2. Entrenamiento de Modelos\n",
        "\n",
        "Esta sección contiene dos opciones para entrenar los modelos:\n",
        "\n",
        "### 2.1. Ejecución Automatizada de Experimentos (Recomendado)\n",
        "\n",
        "Esta opción ejecuta automáticamente **todos los experimentos requeridos**:\n",
        "- **2 arquitecturas**: Autoencoder Clásico y U-net\n",
        "- **4 funciones de pérdida**: L1, L2, SSIM, SSIM_L1\n",
        "- **Total: 8 experimentos**\n",
        "\n",
        "**Proceso automatizado**:\n",
        "1. Itera sobre todas las combinaciones de arquitectura y función de pérdida\n",
        "2. Entrena cada modelo usando Pytorch Lightning con callbacks de visualización\n",
        "3. Evalúa el modelo y calcula métricas (AUC-ROC, Average Precision)\n",
        "4. Registra todo en WandB\n",
        "5. Genera un resumen comparativo al final\n",
        "\n",
        "** IMPORTANTE**: Esta celda debe ejecutarse DESPUÉS de haber definido las funciones `AnomalyDataset`, `load_dataset_paths` y `get_anomaly_labels` en las celdas anteriores.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7Ql9WeQcq1p"
      },
      "source": [
        "### 2.2. Experimento Individual (Opcional)\n",
        "\n",
        "Esta sección permite ejecutar **un solo experimento** de forma manual, útil para:\n",
        "- Probar configuraciones específicas\n",
        "- Depurar problemas\n",
        "- Realizar experimentos personalizados\n",
        "\n",
        "**Pasos**:\n",
        "1. Instanciar el modelo usando Hydra (permite cambiar arquitectura con overrides)\n",
        "2. Crear el módulo Lightning con la función de pérdida especificada\n",
        "3. Configurar WandB logger y callbacks\n",
        "4. Entrenar el modelo\n",
        "\n",
        "**Nota**: Si ejecutaste la celda de automatización (2.1), puedes saltar esta sección.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEOFWvvfcq1p"
      },
      "outputs": [],
      "source": [
        "# Ejecución automatizada de todos los experimentos\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from hydra.utils import instantiate\n",
        "\n",
        "# Definir función auxiliar para obtener etiquetas de anomalía\n",
        "def get_anomaly_labels(test_paths, category_path):\n",
        "    \"\"\"Obtiene las etiquetas de anomalía (0=normal, 1=anomalía)\"\"\"\n",
        "    labels = []\n",
        "    good_path = os.path.join(category_path, 'test', 'good')\n",
        "\n",
        "    for path in test_paths:\n",
        "        if good_path in path:\n",
        "            labels.append(0)  # Normal\n",
        "        else:\n",
        "            labels.append(1)  # Anomalía\n",
        "\n",
        "    return np.array(labels)\n",
        "\n",
        "# Configuración de experimentos\n",
        "ARCHITECTURES = [\"autoencoder_classic\", \"unet\"]\n",
        "LOSS_FUNCTIONS = [\"L1\", \"L2\", \"SSIM\", \"SSIM_L1\"]\n",
        "\n",
        "# Almacenar resultados de todos los experimentos\n",
        "experiment_results = []\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"INICIANDO EJECUCIÓN AUTOMATIZADA DE EXPERIMENTOS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total de experimentos: {len(ARCHITECTURES) * len(LOSS_FUNCTIONS)}\")\n",
        "print(f\"Arquitecturas: {ARCHITECTURES}\")\n",
        "print(f\"Funciones de pérdida: {LOSS_FUNCTIONS}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Iterar sobre todas las combinaciones\n",
        "for arch_idx, architecture in enumerate(ARCHITECTURES):\n",
        "    for loss_idx, loss_function in enumerate(LOSS_FUNCTIONS):\n",
        "        exp_num = arch_idx * len(LOSS_FUNCTIONS) + loss_idx + 1\n",
        "        total_exps = len(ARCHITECTURES) * len(LOSS_FUNCTIONS)\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"EXPERIMENTO {exp_num}/{total_exps}\")\n",
        "        print(f\"Arquitectura: {architecture}\")\n",
        "        print(f\"Función de pérdida: {loss_function}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        # Componer configuración con overrides\n",
        "        cfg = hydra.compose(\n",
        "            config_name=\"config\",\n",
        "            overrides=[\n",
        "                f\"model={architecture}\",\n",
        "                f\"trainer.loss_function={loss_function}\"\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Extraer valores de configuración\n",
        "        DATASET_PATH = cfg.dataset.path\n",
        "        SELECTED_CATEGORY = cfg.dataset.selected_category\n",
        "        IMAGE_SIZE = cfg.dataset.image_size\n",
        "        BATCH_SIZE = cfg.dataset.batch_size\n",
        "        NUM_WORKERS = cfg.dataset.num_workers\n",
        "\n",
        "        # Cargar rutas del dataset\n",
        "        category_path = os.path.join(DATASET_PATH, SELECTED_CATEGORY)\n",
        "        train_paths = load_dataset_paths(category_path, split='train')\n",
        "        test_paths = load_dataset_paths(category_path, split='test')\n",
        "\n",
        "        # Crear transformaciones\n",
        "        # IMPORTANTE: Normalización a [-1, 1] para compatibilidad con Tanh() en el decoder\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normaliza a [-1, 1]\n",
        "        ])\n",
        "\n",
        "        test_transform = transforms.Compose([\n",
        "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normaliza a [-1, 1]\n",
        "        ])\n",
        "\n",
        "        # Crear datasets y dataloaders\n",
        "        train_dataset = AnomalyDataset(train_paths, transform=train_transform)\n",
        "        test_dataset = AnomalyDataset(test_paths, transform=test_transform)\n",
        "\n",
        "        # Dividir train en train y validation (80-20)\n",
        "        from torch.utils.data import random_split\n",
        "        train_size = int(0.8 * len(train_dataset))\n",
        "        val_size = len(train_dataset) - train_size\n",
        "        train_dataset_split, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "        train_loader = DataLoader(train_dataset_split, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "        # Instanciar modelo\n",
        "        base_model = instantiate(cfg.model)\n",
        "        base_model = base_model.to(device)\n",
        "\n",
        "        # Crear módulo Lightning\n",
        "        lightning_model = AutoencoderLightning(\n",
        "            model=base_model,\n",
        "            learning_rate=cfg.trainer.learning_rate,\n",
        "            loss_function=cfg.trainer.loss_function,\n",
        "            scheduler_config=cfg.trainer.scheduler\n",
        "        )\n",
        "\n",
        "        # Configurar logger de WandB para este experimento\n",
        "        wandb_logger = WandbLogger(\n",
        "            project=cfg.logger.project,\n",
        "            name=f\"{architecture}-{loss_function}-{SELECTED_CATEGORY}\",\n",
        "            config=OmegaConf.to_container(cfg, resolve=True),\n",
        "            reinit=True\n",
        "        )\n",
        "\n",
        "        # Callbacks\n",
        "        checkpoint_dir = os.path.join(DRIVE_BASE_PATH, 'checkpoints', f'{architecture}-{loss_function}')\n",
        "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "        checkpoint_callback = ModelCheckpoint(\n",
        "            monitor='train/loss_epoch',\n",
        "            dirpath=checkpoint_dir,\n",
        "            filename=f'{architecture}-{loss_function}-{{epoch:02d}}-{{train/loss_epoch:.4f}}',\n",
        "            save_top_k=1,\n",
        "            mode='min'\n",
        "        )\n",
        "\n",
        "        lr_monitor = LearningRateMonitor(logging_interval='step')\n",
        "\n",
        "        # Callback de visualización periódica (cada 5 épocas)\n",
        "        viz_callback = VisualizationCallback(\n",
        "            val_loader=val_loader,\n",
        "            device=device,\n",
        "            log_every_n_epochs=5,\n",
        "            num_samples=16\n",
        "        )\n",
        "\n",
        "        # Crear Trainer\n",
        "        trainer = Trainer(\n",
        "            max_epochs=cfg.trainer.max_epochs,\n",
        "            accelerator='auto',\n",
        "            devices=1,\n",
        "            logger=wandb_logger,\n",
        "            callbacks=[checkpoint_callback, lr_monitor, viz_callback],\n",
        "            log_every_n_steps=10,\n",
        "            enable_progress_bar=True\n",
        "        )\n",
        "\n",
        "        # Entrenar con validación\n",
        "        print(f\"\\nIniciando entrenamiento...\")\n",
        "        trainer.fit(lightning_model, train_loader, val_loader)\n",
        "\n",
        "        # Obtener pérdidas de entrenamiento y validación\n",
        "        train_losses = lightning_model.train_losses if hasattr(lightning_model, 'train_losses') else []\n",
        "\n",
        "        # Obtener pérdidas de validación del trainer\n",
        "        val_losses = []\n",
        "        if hasattr(trainer, 'callback_metrics') and 'val/loss_epoch' in trainer.callback_metrics:\n",
        "            # Si hay una pérdida de validación en las métricas del callback\n",
        "            val_loss_epoch = trainer.callback_metrics.get('val/loss_epoch')\n",
        "            if val_loss_epoch is not None:\n",
        "                # Si solo hay un valor, crear una lista con ese valor repetido para cada época\n",
        "                if len(train_losses) > 0:\n",
        "                    val_losses = [val_loss_epoch.item() if hasattr(val_loss_epoch, 'item') else val_loss_epoch] * len(train_losses)\n",
        "\n",
        "        # Si no se pudieron obtener las pérdidas de validación, intentar obtenerlas del historial del trainer\n",
        "        if len(val_losses) == 0 and hasattr(trainer, 'logged_metrics'):\n",
        "            # Intentar obtener del historial de métricas\n",
        "            logged_metrics = trainer.logged_metrics\n",
        "            if 'val/loss' in logged_metrics:\n",
        "                val_loss_val = logged_metrics['val/loss']\n",
        "                if hasattr(val_loss_val, 'item'):\n",
        "                    val_losses = [val_loss_val.item()] * len(train_losses) if len(train_losses) > 0 else []\n",
        "\n",
        "        final_train_loss = train_losses[-1] if train_losses else None\n",
        "        final_val_loss = val_losses[-1] if val_losses and len(val_losses) > 0 else None\n",
        "\n",
        "        # Evaluar\n",
        "        print(f\"\\nEvaluando modelo...\")\n",
        "        lightning_model.model.eval()\n",
        "        anomaly_scores = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images in test_loader:\n",
        "                images = images.to(device)\n",
        "                reconstructed = lightning_model.model(images)\n",
        "                mse = torch.mean((images - reconstructed) ** 2, dim=(1, 2, 3))\n",
        "                anomaly_scores.extend(mse.cpu().numpy())\n",
        "\n",
        "        anomaly_scores = np.array(anomaly_scores)\n",
        "\n",
        "        # Obtener etiquetas\n",
        "        test_labels = get_anomaly_labels(test_paths, category_path)\n",
        "\n",
        "        # Calcular métricas\n",
        "        auc_score = roc_auc_score(test_labels, anomaly_scores)\n",
        "        ap_score = average_precision_score(test_labels, anomaly_scores)\n",
        "\n",
        "        # Guardar resultados\n",
        "        result = {\n",
        "            'experiment': exp_num,\n",
        "            'architecture': architecture,\n",
        "            'loss_function': loss_function,\n",
        "            'category': SELECTED_CATEGORY,\n",
        "            'final_train_loss': final_train_loss,\n",
        "            'final_val_loss': final_val_loss,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'auc_roc': auc_score,\n",
        "            'average_precision': ap_score,\n",
        "            'min_score': float(anomaly_scores.min()),\n",
        "            'max_score': float(anomaly_scores.max()),\n",
        "            'mean_score': float(anomaly_scores.mean()),\n",
        "            'std_score': float(anomaly_scores.std())\n",
        "        }\n",
        "        experiment_results.append(result)\n",
        "\n",
        "        # Loggear métricas finales en wandb usando el logger del experimento\n",
        "        # NOTA: Usamos wandb_logger.experiment.log() en lugar de wandb.log() para mantener consistencia\n",
        "        # El WandbLogger finaliza automáticamente el run cuando el Trainer termina\n",
        "        try:\n",
        "            if wandb_logger is not None:\n",
        "                wandb_logger.experiment.log({\n",
        "                    \"eval/auc_roc\": auc_score,\n",
        "                    \"eval/average_precision\": ap_score,\n",
        "                    \"eval/final_train_loss\": final_train_loss if final_train_loss else 0.0,\n",
        "                    \"eval/final_val_loss\": final_val_loss if final_val_loss else 0.0\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error al loggear métricas finales en wandb: {e}\")\n",
        "\n",
        "        # NOTA: No finalizamos WandB aquí porque:\n",
        "        # 1. El WandbLogger de PyTorch Lightning finaliza automáticamente el run cuando el Trainer termina\n",
        "        # 2. Cada experimento tiene su propio run con WandbLogger(reinit=True), así que se finaliza automáticamente\n",
        "        # 3. Si necesitamos finalizar manualmente, se hará al final de todos los experimentos (ver celda 6.2)\n",
        "        # wandb.finish()  # Comentado: el logger finaliza automáticamente\n",
        "\n",
        "        print(f\"\\n✓ Experimento {exp_num}/{total_exps} completado\")\n",
        "        print(f\"  AUC-ROC: {auc_score:.4f}\")\n",
        "        print(f\"  Average Precision: {ap_score:.4f}\")\n",
        "        train_loss_str = f\"{final_train_loss:.4f}\" if final_train_loss is not None else \"N/A\"\n",
        "        val_loss_str = f\"{final_val_loss:.4f}\" if final_val_loss is not None else \"N/A\"\n",
        "        print(f\"  Pérdida final train: {train_loss_str}\")\n",
        "        print(f\"  Pérdida final val: {val_loss_str}\")\n",
        "\n",
        "        # Limpiar memoria\n",
        "        del lightning_model\n",
        "        del base_model\n",
        "        del trainer\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "# Crear DataFrame con resultados\n",
        "results_df = pd.DataFrame(experiment_results)\n",
        "\n",
        "# Guardar resultados\n",
        "results_path = os.path.join(DRIVE_BASE_PATH, 'resultados_experimentos.csv')\n",
        "results_df.to_csv(results_path, index=False)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"TODOS LOS EXPERIMENTOS COMPLETADOS\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"\\nResultados guardados en: {results_path}\")\n",
        "print(\"\\nResumen de resultados:\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Crear tabla comparativa\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"TABLA COMPARATIVA DE RESULTADOS\")\n",
        "print(f\"{'='*80}\")\n",
        "comparison = results_df.pivot_table(\n",
        "    index='architecture',\n",
        "    columns='loss_function',\n",
        "    values=['auc_roc', 'average_precision'],\n",
        "    aggfunc='first'\n",
        ")\n",
        "print(comparison)\n",
        "\n",
        "# Guardar tabla comparativa\n",
        "comparison_path = os.path.join(DRIVE_BASE_PATH, 'comparacion_experimentos.csv')\n",
        "comparison.to_csv(comparison_path)\n",
        "print(f\"\\nTabla comparativa guardada en: {comparison_path}\")\n",
        "\n",
        "# Visualizar comparación\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gráfico de AUC-ROC\n",
        "auc_pivot = results_df.pivot_table(\n",
        "    index='architecture',\n",
        "    columns='loss_function',\n",
        "    values='auc_roc',\n",
        "    aggfunc='first'\n",
        ")\n",
        "auc_pivot.plot(kind='bar', ax=axes[0], rot=0)\n",
        "axes[0].set_title('Comparación de AUC-ROC por Arquitectura y Función de Pérdida')\n",
        "axes[0].set_ylabel('AUC-ROC')\n",
        "axes[0].legend(title='Función de Pérdida')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Gráfico de Average Precision\n",
        "ap_pivot = results_df.pivot_table(\n",
        "    index='architecture',\n",
        "    columns='loss_function',\n",
        "    values='average_precision',\n",
        "    aggfunc='first'\n",
        ")\n",
        "ap_pivot.plot(kind='bar', ax=axes[1], rot=0)\n",
        "axes[1].set_title('Comparación de Average Precision por Arquitectura y Función de Pérdida')\n",
        "axes[1].set_ylabel('Average Precision')\n",
        "axes[1].legend(title='Función de Pérdida')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "comparison_plot_path = os.path.join(DRIVE_BASE_PATH, 'comparacion_grafica.png')\n",
        "plt.savefig(comparison_plot_path)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ANÁLISIS COMPLETO\")\n",
        "print(f\"{'='*80}\")\n",
        "print(\"\\nMejor resultado por métrica:\")\n",
        "        best_auc = results_df.loc[results_df['auc_roc'].idxmax()]\n",
        "        print(f\"Mejor AUC-ROC: Arquitectura={best_auc['architecture']}, Loss={best_auc['loss_function']}, AUC={best_auc['auc_roc']:.4f}\")\n",
        "        best_ap = results_df.loc[results_df['average_precision'].idxmax()]\n",
        "        print(f\"Mejor Average Precision: Arquitectura={best_ap['architecture']}, Loss={best_ap['loss_function']}, AP={best_ap['average_precision']:.4f}\")\n",
        "\n",
        "        # Finalizar cualquier run de WandB que quede activo después de todos los experimentos\n",
        "        # NOTA: Cada experimento tiene su propio run que se finaliza automáticamente cuando el Trainer termina\n",
        "        # Sin embargo, si hay algún run activo, lo finalizamos aquí\n",
        "        try:\n",
        "            if 'wandb' in globals() and wandb.run is not None:\n",
        "                if not wandb.run.settings._disable:\n",
        "                    wandb.finish()\n",
        "                    print(\"\\n✓ Todos los runs de WandB finalizados\")\n",
        "        except Exception as e:\n",
        "            if \"finished\" not in str(e).lower() and \"is finished\" not in str(e).lower():\n",
        "                print(f\"⚠️ Error al finalizar WandB: {e}\")\n",
        "            # Si ya está finalizado, no hay problema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBDNN52khfjR"
      },
      "source": [
        "### 2.2.1. Instanciación del Modelo\n",
        "\n",
        "En esta celda se instancia el modelo usando Hydra. El modelo se crea automáticamente según la configuración definida en `conf/model/`.\n",
        "\n",
        "**Uso de overrides**:\n",
        "- Para cambiar a U-net: `cfg = hydra.compose(config_name=\"config\", overrides=[\"model=unet\"])`\n",
        "- Para cambiar función de pérdida: `cfg = hydra.compose(config_name=\"config\", overrides=[\"trainer.loss_function=SSIM\"])`\n",
        "\n",
        "Hydra instanciará automáticamente el modelo según la configuración en `conf/model/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJVBWiAgRfDg"
      },
      "outputs": [],
      "source": [
        "\n",
        "from hydra.utils import instantiate\n",
        "\n",
        "# Instanciar el modelo automáticamente desde la configuración\n",
        "# Esto usa el _target_ y parámetros definidos en conf/model/autoencoder_classic.yaml o conf/model/unet.yaml\n",
        "base_model = instantiate(cfg.model)\n",
        "\n",
        "print(f\"Modelo instanciado: {cfg.model._target_}\")\n",
        "print(f\"Parámetros del modelo: {sum(p.numel() for p in base_model.parameters())}\")\n",
        "\n",
        "# Crear módulo Lightning con la función de pérdida especificada\n",
        "lightning_model = AutoencoderLightning(\n",
        "    model=base_model,\n",
        "    learning_rate=cfg.trainer.learning_rate,\n",
        "    loss_function=cfg.trainer.loss_function,\n",
        "    scheduler_config=cfg.trainer.scheduler\n",
        ")\n",
        "\n",
        "print(f\"\\nArquitectura: {cfg.model.architecture}\")\n",
        "print(f\"Función de pérdida: {cfg.trainer.loss_function}\")\n",
        "print(f\"Learning rate: {cfg.trainer.learning_rate}\")\n",
        "print(f\"Latent dim: {cfg.model.latent_dim}\")\n",
        "print(f\"Input channels: {cfg.model.input_channels}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idmV7qlxcq1q"
      },
      "source": [
        "## 3. Entrenamiento del Modelo\n",
        "\n",
        "En esta sección se entrena el modelo usando Pytorch Lightning.\n",
        "\n",
        "**Características del entrenamiento**:\n",
        "- **Callbacks automáticos**:\n",
        "  - `ModelCheckpoint`: Guarda el mejor modelo según la pérdida de entrenamiento\n",
        "  - `LearningRateMonitor`: Monitorea el learning rate durante el entrenamiento\n",
        "  - `VisualizationCallback`: Genera visualizaciones periódicas cada 5 épocas (reconstrucciones y t-SNE del espacio latente)\n",
        "- **Logging en WandB**: Todas las métricas se registran automáticamente en Weights & Biases\n",
        "- **Visualizaciones periódicas**: Durante el entrenamiento, cada 5 épocas se generan y registran en WandB:\n",
        "  - Comparación de reconstrucciones del set de validación (16 imágenes)\n",
        "  - Visualización t-SNE del espacio latente del set de validación\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brbqx3ZwiDrN"
      },
      "source": [
        "### 3.1. Entrenamiento del Modelo (Experimento Individual)\n",
        "\n",
        "Esta celda entrena el modelo usando Pytorch Lightning para un experimento individual.\n",
        "\n",
        "**Nota**: Si ejecutaste la celda de automatización (2.1), puedes saltar esta sección ya que el entrenamiento ya se realizó automáticamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLFeRCsdiGr6"
      },
      "source": [
        "### 3.3.1. Preparación para Visualizaciones\n",
        "\n",
        "Esta celda define la función auxiliar `denormalize` necesaria para visualizar las imágenes correctamente.\n",
        "\n",
        "Las visualizaciones del set de validación se generan automáticamente durante el entrenamiento mediante el `VisualizationCallback` (cada 5 épocas).\n",
        "\n",
        "Si necesitas generar visualizaciones adicionales después del entrenamiento, puedes usar las celdas siguientes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función auxiliar para denormalizar imágenes\n",
        "def denormalize(tensor, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]):\n",
        "    \"\"\"Desnormaliza un tensor de imagen de [-1, 1] a [0, 1] para visualización\"\"\"\n",
        "    # Desnormalizar: x = (x_norm * std) + mean\n",
        "    # Para normalización [-1, 1]: x_norm = (x - 0.5) / 0.5, entonces x = (x_norm * 0.5) + 0.5\n",
        "    for t, m, s in zip(tensor, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "    return tensor.clamp_(0, 1)\n",
        "\n",
        "print(\"✓ Función denormalize definida correctamente\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjsj4NrEifaa"
      },
      "outputs": [],
      "source": [
        "# Visualización de reconstrucciones del set de validación (16 imágenes)\n",
        "lightning_model.model.eval()\n",
        "val_images = []\n",
        "val_reconstructions = []\n",
        "val_latent_vectors = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, images in enumerate(val_loader):\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Obtener reconstrucciones\n",
        "        reconstructed = lightning_model.model(images)\n",
        "\n",
        "        # Extraer vectores latentes\n",
        "        if hasattr(lightning_model.model, 'encode'):\n",
        "            latent = lightning_model.model.encode(images)\n",
        "            # Para UNet, encode devuelve (latent, skip_connections), solo necesitamos latent\n",
        "            if isinstance(latent, tuple):\n",
        "                latent = latent[0]\n",
        "            # Aplanar el vector latente para t-SNE\n",
        "            latent_flat = latent.view(latent.size(0), -1)\n",
        "            val_latent_vectors.append(latent_flat.cpu().numpy())\n",
        "\n",
        "        # Guardar imágenes para visualización (hasta 16)\n",
        "        if len(val_images) < 16:\n",
        "            remaining = 16 - len(val_images)\n",
        "            val_images.append(images[:remaining].cpu())\n",
        "            val_reconstructions.append(reconstructed[:remaining].cpu())\n",
        "\n",
        "        if len(val_images) >= 16:\n",
        "            break\n",
        "\n",
        "# Concatenar todas las imágenes\n",
        "val_images = torch.cat(val_images[:16])[:16]\n",
        "val_reconstructions = torch.cat(val_reconstructions[:16])[:16]\n",
        "\n",
        "# Visualizar 16 reconstrucciones del set de validación\n",
        "fig, axes = plt.subplots(4, 8, figsize=(20, 10))\n",
        "axes = axes.flatten()\n",
        "val_reconstruction_images = []\n",
        "\n",
        "for i in range(16):\n",
        "    # Imagen original\n",
        "    img = denormalize(val_images[i].clone())\n",
        "    axes[i*2].imshow(img.permute(1, 2, 0))\n",
        "    axes[i*2].set_title(f'Original {i+1}')\n",
        "    axes[i*2].axis('off')\n",
        "\n",
        "    # Reconstrucción\n",
        "    recon = denormalize(val_reconstructions[i].clone())\n",
        "    axes[i*2+1].imshow(recon.permute(1, 2, 0))\n",
        "    axes[i*2+1].set_title(f'Reconstrucción {i+1}')\n",
        "    axes[i*2+1].axis('off')\n",
        "\n",
        "    # Preparar para wandb\n",
        "    img_np = (img.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
        "    recon_np = (recon.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
        "    val_reconstruction_images.append(wandb.Image(img_np, caption=f\"Original Val {i+1}\"))\n",
        "    val_reconstruction_images.append(wandb.Image(recon_np, caption=f\"Reconstrucción Val {i+1}\"))\n",
        "\n",
        "plt.tight_layout()\n",
        "validation_recon_path = os.path.join(DRIVE_BASE_PATH, 'validation_reconstructions.png')\n",
        "plt.savefig(validation_recon_path)\n",
        "plt.show()\n",
        "\n",
        "# Loggear en wandb (usar wandb_logger si está disponible y el run está activo)\n",
        "# Profe Steven ve esto, deje estas condicionales para hacer pruebas con la sesion cerrada y abierta de w&b\n",
        "try:\n",
        "    if 'wandb_logger' in globals() and wandb_logger is not None:\n",
        "        # Verificar si el experimento está activo\n",
        "        if hasattr(wandb_logger.experiment, 'id') and wandb_logger.experiment.id is not None:\n",
        "            try:\n",
        "                wandb_logger.experiment.log({\n",
        "                    \"val/reconstructions\": val_reconstruction_images,\n",
        "                    \"val/reconstructions_grid\": wandb.Image(validation_recon_path)\n",
        "                })\n",
        "                print(\"✓ Reconstrucciones del set de validación guardadas en wandb\")\n",
        "            except Exception as e:\n",
        "                # Si el run está finalizado, solo guardar localmente\n",
        "                if \"finished\" in str(e).lower() or \"is finished\" in str(e).lower():\n",
        "                    print(\"ℹ️ El run de WandB ya está finalizado. Las visualizaciones se guardaron localmente.\")\n",
        "                    print(f\"   Ruta: {validation_recon_path}\")\n",
        "                    print(\"   Nota: Estas visualizaciones ya se generaron automáticamente durante el entrenamiento.\")\n",
        "                else:\n",
        "                    raise\n",
        "        else:\n",
        "            print(f\"ℹ️ Reconstrucciones guardadas en: {validation_recon_path}\")\n",
        "            print(\"   Nota: El run de WandB no está activo. Las visualizaciones ya se generaron durante el entrenamiento.\")\n",
        "    elif 'wandb' in globals() and wandb.run is not None and not wandb.run.settings._disable:\n",
        "        # Si wandb está inicializado directamente y el run está activo\n",
        "        try:\n",
        "            wandb.log({\n",
        "                \"val/reconstructions\": val_reconstruction_images,\n",
        "                \"val/reconstructions_grid\": wandb.Image(validation_recon_path)\n",
        "            })\n",
        "            print(\"✓ Reconstrucciones del set de validación guardadas en wandb\")\n",
        "        except Exception as e:\n",
        "            if \"finished\" in str(e).lower() or \"is finished\" in str(e).lower():\n",
        "                print(\"ℹ️ El run de WandB ya está finalizado. Las visualizaciones se guardaron localmente.\")\n",
        "                print(f\"   Ruta: {validation_recon_path}\")\n",
        "            else:\n",
        "                raise\n",
        "    else:\n",
        "        print(f\"ℹ️ Reconstrucciones guardadas en: {validation_recon_path}\")\n",
        "        print(\"   Nota: wandb_logger no está definido o el run está finalizado.\")\n",
        "        print(\"   Las visualizaciones ya se generaron automáticamente durante el entrenamiento mediante el VisualizationCallback.\")\n",
        "except Exception as e:\n",
        "    # Manejar cualquier otro error silenciosamente\n",
        "    print(f\"ℹ️ Reconstrucciones guardadas en: {validation_recon_path}\")\n",
        "    if \"finished\" not in str(e).lower() and \"is finished\" not in str(e).lower():\n",
        "        print(f\"   (Error menor al loggear en wandb: {e})\")\n",
        "    print(\"   Nota: Las visualizaciones ya se generaron automáticamente durante el entrenamiento.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojs3NVbykQYk"
      },
      "source": [
        "### 3.3.2. Reconstrucciones del Set de Validación\n",
        "\n",
        "Esta celda genera visualizaciones de reconstrucciones del set de validación (16 imágenes) **después del entrenamiento**.\n",
        "\n",
        "**Nota**: Estas visualizaciones también se generan automáticamente durante el entrenamiento cada 5 épocas mediante el `VisualizationCallback`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3.3. Visualización t-SNE del Espacio Latente\n",
        "\n",
        "Esta celda genera la visualización t-SNE del espacio latente del set de validación **después del entrenamiento**.\n",
        "\n",
        "**Nota**: Esta visualización también se genera automáticamente durante el entrenamiento cada 5 épocas mediante el `VisualizationCallback`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CjOVtTJcq1r"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Verificar si val_latent_vectors existe, si no, extraerlos\n",
        "if 'val_latent_vectors' not in globals() or not val_latent_vectors:\n",
        "    print(\"Extrayendo vectores latentes del set de validación...\")\n",
        "    val_latent_vectors = []\n",
        "\n",
        "    # Verificar que el modelo y val_loader estén disponibles\n",
        "    if 'lightning_model' in globals() and 'val_loader' in globals():\n",
        "        lightning_model.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, images in enumerate(val_loader):\n",
        "                images = images.to(device)\n",
        "\n",
        "                # Extraer vectores latentes\n",
        "                if hasattr(lightning_model.model, 'encode'):\n",
        "                    latent = lightning_model.model.encode(images)\n",
        "                    # Para UNet, encode devuelve (latent, skip_connections), solo necesitamos latent\n",
        "                    if isinstance(latent, tuple):\n",
        "                        latent = latent[0]\n",
        "                    # Aplanar el vector latente para t-SNE\n",
        "                    latent_flat = latent.view(latent.size(0), -1)\n",
        "                    val_latent_vectors.append(latent_flat.cpu().numpy())\n",
        "        print(f\"✓ Vectores latentes extraídos: {len(val_latent_vectors)} batches\")\n",
        "    else:\n",
        "        print(\"Error: lightning_model o val_loader no están disponibles.\")\n",
        "        print(\"Por favor, ejecuta primero las celdas de entrenamiento y carga de datos.\")\n",
        "\n",
        "# Concatenar todos los vectores latentes\n",
        "if val_latent_vectors:\n",
        "    all_latent_vectors = np.concatenate(val_latent_vectors, axis=0)\n",
        "\n",
        "    print(f\"Forma de los vectores latentes: {all_latent_vectors.shape}\")\n",
        "    print(\"Aplicando t-SNE...\")\n",
        "\n",
        "    # Aplicar t-SNE (reducir a 2D para visualización)\n",
        "    # Asegurarse de que perplexity sea menor que el número de muestras\n",
        "    perplexity_val = min(30, len(all_latent_vectors) - 1)\n",
        "    if perplexity_val <= 0: # Handle case with very few samples\n",
        "        print(\"No hay suficientes muestras para t-SNE con perplexity > 0.\")\n",
        "    else:\n",
        "        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_val)\n",
        "        latent_2d = tsne.fit_transform(all_latent_vectors)\n",
        "\n",
        "        # Visualizar t-SNE\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.scatter(latent_2d[:, 0], latent_2d[:, 1], alpha=0.6, s=50)\n",
        "        plt.title('t-SNE del Espacio Latente (Set de Validación)')\n",
        "        plt.xlabel('Componente t-SNE 1')\n",
        "        plt.ylabel('Componente t-SNE 2')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        tsne_path = os.path.join(DRIVE_BASE_PATH, 'tsne_latent_validation.png')\n",
        "        plt.savefig(tsne_path)\n",
        "        plt.show()\n",
        "\n",
        "        # Loggear en wandb (verificar si wandb_logger está definido)\n",
        "        try:\n",
        "            if 'wandb_logger' in globals() and wandb_logger is not None:\n",
        "                wandb_logger.experiment.log({\n",
        "                    \"val/tsne_latent\": wandb.Image(tsne_path)\n",
        "                })\n",
        "                print(\"t-SNE del espacio latente guardado en wandb\")\n",
        "            elif 'wandb' in globals() and wandb.run is not None:\n",
        "                # Verificar que wandb está inicializado\n",
        "                wandb.log({\n",
        "                    \"val/tsne_latent\": wandb.Image(tsne_path)\n",
        "                })\n",
        "                print(\"t-SNE del espacio latente guardado en wandb\")\n",
        "            else:\n",
        "                print(\"t-SNE del espacio latente guardado en:\", tsne_path)\n",
        "                print(\"Nota: wandb_logger no está definido. Ejecuta primero la celda de entrenamiento para habilitar el logging.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al loggear en wandb: {e}\")\n",
        "            print(\"t-SNE del espacio latente guardado en:\", tsne_path)\n",
        "else:\n",
        "    print(\"No se pudieron extraer vectores latentes. Verifica que el modelo tenga método encode().\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTAR15LgRfDg"
      },
      "source": [
        "## 3.3. Visualizaciones del Set de Validación (Requisitos WandB)\n",
        "\n",
        "Esta sección genera las visualizaciones requeridas para WandB del set de validación **después del entrenamiento**:\n",
        "\n",
        "1. **Reconstrucciones del set de validación (16 imágenes)**: Comparación lado a lado de imágenes originales vs reconstruidas\n",
        "2. **t-SNE del espacio latente**: Visualización 2D de cómo el modelo organiza las imágenes en el espacio latente\n",
        "\n",
        "**Nota**:\n",
        "- Estas visualizaciones también se generan **automáticamente durante el entrenamiento** cada 5 épocas mediante el `VisualizationCallback`\n",
        "- Las visualizaciones se registran automáticamente en WandB con el nombre del experimento correspondiente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsk-Pr-fRfDg"
      },
      "outputs": [],
      "source": [
        "# Entrenar modelo usando Pytorch Lightning Trainer\n",
        "# Verificar e instalar dependencias si es necesario\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "    from pytorch_lightning import Trainer\n",
        "    from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
        "except ImportError:\n",
        "    print(\"pytorch-lightning no encontrado. Instalando...\")\n",
        "    try:\n",
        "        # Intentar usar %pip (compatible con Colab)\n",
        "        get_ipython().run_line_magic('pip', 'install pytorch-lightning')\n",
        "    except:\n",
        "        # Si no funciona, usar subprocess\n",
        "        import subprocess\n",
        "        import sys\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pytorch-lightning\"])\n",
        "    from pytorch_lightning import Trainer\n",
        "    from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
        "    print(\"✓ pytorch-lightning instalado correctamente\")\n",
        "\n",
        "# Configurar logger de WandB para Lightning\n",
        "wandb_logger = WandbLogger(\n",
        "    project=cfg.logger.project,\n",
        "    name=f\"{cfg.model.architecture}-{cfg.trainer.loss_function}-{SELECTED_CATEGORY}\",\n",
        "    config=OmegaConf.to_container(cfg, resolve=True)\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "checkpoint_dir = os.path.join(DRIVE_BASE_PATH, 'checkpoints')\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='train/loss_epoch',\n",
        "    dirpath=checkpoint_dir,\n",
        "    filename=f'{cfg.model.architecture}-{cfg.trainer.loss_function}-{{epoch:02d}}-{{train/loss_epoch:.4f}}',\n",
        "    save_top_k=1,\n",
        "    mode='min'\n",
        ")\n",
        "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
        "\n",
        "# Callback de visualización periódica (cada 5 épocas)\n",
        "# Genera visualizaciones de reconstrucciones y t-SNE durante el entrenamiento\n",
        "viz_callback = VisualizationCallback(\n",
        "    val_loader=val_loader,\n",
        "    device=device,\n",
        "    log_every_n_epochs=5,\n",
        "    num_samples=16\n",
        ")\n",
        "\n",
        "# Crear Trainer de Pytorch Lightning\n",
        "trainer = Trainer(\n",
        "    max_epochs=cfg.trainer.max_epochs,\n",
        "    accelerator='auto',\n",
        "    devices=1,\n",
        "    logger=wandb_logger,\n",
        "    callbacks=[checkpoint_callback, lr_monitor, viz_callback],\n",
        "    log_every_n_steps=10,\n",
        "    enable_progress_bar=True\n",
        ")\n",
        "\n",
        "# Entrenar modelo\n",
        "print(f'Iniciando entrenamiento con Pytorch Lightning...')\n",
        "print(f'Arquitectura: {cfg.model.architecture}')\n",
        "print(f'Función de pérdida: {cfg.trainer.loss_function}')\n",
        "print(f'Learning rate: {cfg.trainer.learning_rate}')\n",
        "trainer.fit(lightning_model, train_loader, val_loader) # Añadir val_loader para que el logger monitoree 'val/loss'\n",
        "print('\\nEntrenamiento completado!')\n",
        "\n",
        "# Obtener pérdidas de entrenamiento del modelo Lightning\n",
        "train_losses = lightning_model.train_losses if hasattr(lightning_model, 'train_losses') else []\n",
        "\n",
        "# Obtener el modelo base entrenado para evaluación\n",
        "model = lightning_model.model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLjpADYzkuSm"
      },
      "source": [
        "### 3.2. Visualización de Curva de Pérdida\n",
        "\n",
        "Esta celda genera la gráfica de la curva de pérdida de entrenamiento a lo largo de las épocas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mazT5RORfDh"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses)\n",
        "plt.title('Curva de Pérdida de Entrenamiento')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Pérdida (MSE)')\n",
        "plt.grid(True)\n",
        "train_loss_path = os.path.join(DRIVE_BASE_PATH, 'train_loss_curve.png')\n",
        "plt.savefig(train_loss_path)\n",
        "plt.show()\n",
        "\n",
        "# Loggear la curva de pérdida en wandb\n",
        "wandb_logger.experiment.log({\"train/loss_curve\": wandb.Image(train_loss_path)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXSHxPgWRfDh"
      },
      "source": [
        "## 4. Evaluación y Detección de Anomalías\n",
        "\n",
        "En esta sección se evalúa el modelo entrenado en el set de prueba:\n",
        "\n",
        "1. **Cálculo de scores de anomalía**: Se usa el error de reconstrucción (MSE) como métrica\n",
        "2. **Selección de imágenes**: Se seleccionan 8 imágenes buenas y 8 con anomalías para visualización\n",
        "3. **Cálculo de métricas**: AUC-ROC y Average Precision\n",
        "4. **Visualización de curva ROC**: Gráfico de la curva ROC para evaluar el rendimiento\n",
        "\n",
        "**Métricas calculadas**:\n",
        "- **AUC-ROC**: Área bajo la curva ROC (mientras más cercano a 1.0, mejor)\n",
        "- **Average Precision**: Precisión promedio (mientras más cercano a 1.0, mejor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODBK4hPSkk6Y"
      },
      "source": [
        "### 4.1. Obtención de Etiquetas de Anomalía\n",
        "\n",
        "Esta celda define la función para obtener las etiquetas de anomalía (0=normal, 1=anomalía) del set de prueba y muestra la distribución de clases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCx-UKUaRfDh"
      },
      "outputs": [],
      "source": [
        "def get_anomaly_labels(test_paths, category_path):\n",
        "    \"\"\"Obtiene las etiquetas de anomalía (0=normal, 1=anomalía)\"\"\"\n",
        "    labels = []\n",
        "    good_path = os.path.join(category_path, 'test', 'good')\n",
        "\n",
        "    for path in test_paths:\n",
        "        if good_path in path:\n",
        "            labels.append(0)  # Normal\n",
        "        else:\n",
        "            labels.append(1)  # Anomalía\n",
        "\n",
        "    return np.array(labels)\n",
        "\n",
        "# Obtener etiquetas de prueba\n",
        "test_labels = get_anomaly_labels(test_paths, category_path)\n",
        "print(f'Imágenes normales: {np.sum(test_labels == 0)}')\n",
        "print(f'Imágenes con anomalías: {np.sum(test_labels == 1)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtCj39TulPhT"
      },
      "source": [
        "### 4.2. Evaluación del Modelo en Set de Prueba\n",
        "\n",
        "Esta celda evalúa el modelo entrenado en el set de prueba:\n",
        "- Calcula scores de anomalía usando el error de reconstrucción (MSE)\n",
        "- Selecciona 8 imágenes buenas y 8 con anomalías para visualización\n",
        "- Prepara los datos para el cálculo de métricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVKJlE0TRfDh"
      },
      "outputs": [],
      "source": [
        "lightning_model.model.eval()\n",
        "anomaly_scores = []\n",
        "test_images_all = []\n",
        "test_reconstructions_all = []\n",
        "test_paths_all = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, images in enumerate(test_loader):\n",
        "        images = images.to(device)\n",
        "        reconstructed = lightning_model.model(images)\n",
        "\n",
        "        # Calcular error de reconstrucción (MSE) como score de anomalía\n",
        "        mse = torch.mean((images - reconstructed) ** 2, dim=(1, 2, 3))\n",
        "        anomaly_scores.extend(mse.cpu().numpy())\n",
        "\n",
        "        # Guardar todas las imágenes y reconstrucciones con sus rutas\n",
        "        batch_start = batch_idx * BATCH_SIZE\n",
        "        batch_paths = test_paths[batch_start:batch_start + len(images)]\n",
        "        test_images_all.append(images.cpu())\n",
        "        test_reconstructions_all.append(reconstructed.cpu())\n",
        "        test_paths_all.extend(batch_paths)\n",
        "\n",
        "anomaly_scores = np.array(anomaly_scores)\n",
        "print(f'Rango de scores: [{anomaly_scores.min():.4f}, {anomaly_scores.max():.4f}]')\n",
        "\n",
        "# Obtener etiquetas\n",
        "category_path = os.path.join(DATASET_PATH, SELECTED_CATEGORY)\n",
        "test_labels = get_anomaly_labels(test_paths_all, category_path)\n",
        "\n",
        "# Separar imágenes buenas y con anomalías\n",
        "good_indices = np.where(test_labels == 0)[0]\n",
        "anomaly_indices = np.where(test_labels == 1)[0]\n",
        "\n",
        "# Seleccionar 8 imágenes buenas y 8 con anomalías\n",
        "num_samples = 8\n",
        "selected_good_indices = good_indices[:num_samples] if len(good_indices) >= num_samples else good_indices\n",
        "selected_anomaly_indices = anomaly_indices[:num_samples] if len(anomaly_indices) >= num_samples else anomaly_indices\n",
        "\n",
        "# Concatenar todas las imágenes\n",
        "all_test_images = torch.cat(test_images_all)\n",
        "all_test_reconstructions = torch.cat(test_reconstructions_all)\n",
        "\n",
        "# Seleccionar las 16 imágenes (8 buenas + 8 con anomalías)\n",
        "selected_indices = np.concatenate([selected_good_indices, selected_anomaly_indices])\n",
        "sample_images = all_test_images[selected_indices]\n",
        "sample_reconstructions = all_test_reconstructions[selected_indices]\n",
        "sample_labels = test_labels[selected_indices]\n",
        "\n",
        "print(f'Imágenes seleccionadas: {len(selected_good_indices)} buenas + {len(selected_anomaly_indices)} con anomalías = {len(sample_images)} total')\n",
        "\n",
        "# Loggear estadísticas de scores en wandb\n",
        "wandb_logger.experiment.log({\n",
        "    \"eval/min_score\": float(anomaly_scores.min()),\n",
        "    \"eval/max_score\": float(anomaly_scores.max()),\n",
        "    \"eval/mean_score\": float(anomaly_scores.mean()),\n",
        "    \"eval/std_score\": float(anomaly_scores.std())\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9FSWyailUJ1"
      },
      "source": [
        "### 4.3. Cálculo de Métricas y Curva ROC\n",
        "\n",
        "Esta celda calcula las métricas de evaluación:\n",
        "- **AUC-ROC**: Área bajo la curva ROC (mientras más cercano a 1.0, mejor)\n",
        "- **Average Precision**: Precisión promedio (mientras más cercano a 1.0, mejor)\n",
        "- Genera la curva ROC para visualización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awWbAD11RfDh"
      },
      "outputs": [],
      "source": [
        "auc_score = roc_auc_score(test_labels, anomaly_scores)\n",
        "ap_score = average_precision_score(test_labels, anomaly_scores)\n",
        "print(f'AUC-ROC: {auc_score:.4f}')\n",
        "print(f'Average Precision: {ap_score:.4f}')\n",
        "\n",
        "# Curva ROC\n",
        "fpr, tpr, thresholds = roc_curve(test_labels, anomaly_scores)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.4f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "roc_curve_path = os.path.join(DRIVE_BASE_PATH, 'roc_curve.png')\n",
        "plt.savefig(roc_curve_path)\n",
        "plt.show()\n",
        "\n",
        "# Loggear métricas y curva ROC en wandb\n",
        "wandb_logger.experiment.log({\n",
        "    \"eval/auc_roc\": auc_score,\n",
        "    \"eval/average_precision\": ap_score,\n",
        "    \"eval/roc_curve\": wandb.Image(roc_curve_path)\n",
        "})\n",
        "\n",
        "# Crear curva ROC interactiva en wandb\n",
        "# Nota: wandb.plot.roc_curve requiere que los scores sean probabilidades\n",
        "# Normalizamos los scores para que estén en [0, 1]\n",
        "normalized_scores = (anomaly_scores - anomaly_scores.min()) / (anomaly_scores.max() - anomaly_scores.min() + 1e-8)\n",
        "\n",
        "# wandb.plot.roc_curve necesita un formato específico: y_probas debe ser un array 2D de forma (n_samples, n_classes)\n",
        "# Para binario, necesitamos [prob_clase_0, prob_clase_1] donde prob_clase_1 = normalized_scores\n",
        "try:\n",
        "    # Crear array de probabilidades con forma (n_samples, 2)\n",
        "    y_probas = np.column_stack([1 - normalized_scores, normalized_scores])\n",
        "    wandb_logger.experiment.log({\n",
        "        \"eval/roc_curve_interactive\": wandb.plot.roc_curve(test_labels,\n",
        "                                                           y_probas,\n",
        "                                                           labels=[\"Normal\", \"Anomalía\"],\n",
        "                                                           classes_to_plot=[1])\n",
        "    })\n",
        "except Exception as e:\n",
        "    print(f\"Error al crear curva ROC interactiva en wandb: {e}\")\n",
        "    print(\"La curva ROC estática ya fue guardada correctamente.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hsPPd5fRfDh"
      },
      "source": [
        "## 5. Visualización de Resultados\n",
        "\n",
        "Esta sección contiene todas las visualizaciones de los resultados del modelo entrenado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqiJwionldLe"
      },
      "source": [
        "### 5.1. Reconstrucciones del Set de Prueba\n",
        "\n",
        "Esta celda visualiza las reconstrucciones del set de prueba:\n",
        "- **8 imágenes buenas**: Originales y sus reconstrucciones\n",
        "- **8 imágenes con anomalías**: Originales y sus reconstrucciones\n",
        "- Total: 16 imágenes (32 paneles: original + reconstrucción)\n",
        "\n",
        "**Función `denormalize`**: Convierte las imágenes normalizadas de [-1, 1] a [0, 1] para visualización correcta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXM-O50-RfDi"
      },
      "outputs": [],
      "source": [
        "def denormalize(tensor, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]):\n",
        "    \"\"\"Desnormaliza un tensor de imagen de [-1, 1] a [0, 1] para visualización\"\"\"\n",
        "    # Desnormalizar: x = (x_norm * std) + mean\n",
        "    # Para normalización [-1, 1]: x_norm = (x - 0.5) / 0.5, entonces x = (x_norm * 0.5) + 0.5\n",
        "    for t, m, s in zip(tensor, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "    return tensor.clamp_(0, 1)\n",
        "\n",
        "# Visualizar reconstrucciones del set de prueba (16 imágenes: 8 buenas + 8 con anomalías)\n",
        "fig, axes = plt.subplots(4, 8, figsize=(20, 10))\n",
        "axes = axes.flatten()\n",
        "test_reconstruction_images = []\n",
        "\n",
        "# Visualizar 8 imágenes buenas primero\n",
        "for i in range(8):\n",
        "    if i < len(sample_images):\n",
        "        idx = i\n",
        "        img = denormalize(sample_images[idx].clone())\n",
        "        recon = denormalize(sample_reconstructions[idx].clone())\n",
        "\n",
        "        # Imagen original\n",
        "        axes[i*2].imshow(img.permute(1, 2, 0))\n",
        "        axes[i*2].set_title(f'Buenas - Original {i+1}')\n",
        "        axes[i*2].axis('off')\n",
        "\n",
        "        # Reconstrucción\n",
        "        axes[i*2+1].imshow(recon.permute(1, 2, 0))\n",
        "        axes[i*2+1].set_title(f'Buenas - Reconstrucción {i+1}')\n",
        "        axes[i*2+1].axis('off')\n",
        "\n",
        "        # Preparar para wandb\n",
        "        img_np = (img.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
        "        recon_np = (recon.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
        "        test_reconstruction_images.append(wandb.Image(img_np, caption=f\"Buenas - Original {i+1}\"))\n",
        "        test_reconstruction_images.append(wandb.Image(recon_np, caption=f\"Buenas - Reconstrucción {i+1}\"))\n",
        "\n",
        "# Visualizar 8 imágenes con anomalías\n",
        "for i in range(8):\n",
        "    idx = 8 + i\n",
        "    if idx < len(sample_images):\n",
        "        img = denormalize(sample_images[idx].clone())\n",
        "        recon = denormalize(sample_reconstructions[idx].clone())\n",
        "\n",
        "        # Imagen original\n",
        "        axes[16 + i*2].imshow(img.permute(1, 2, 0))\n",
        "        axes[16 + i*2].set_title(f'Anomalía - Original {i+1}')\n",
        "        axes[16 + i*2].axis('off')\n",
        "\n",
        "        # Reconstrucción\n",
        "        axes[16 + i*2+1].imshow(recon.permute(1, 2, 0))\n",
        "        axes[16 + i*2+1].set_title(f'Anomalía - Reconstrucción {i+1}')\n",
        "        axes[16 + i*2+1].axis('off')\n",
        "\n",
        "        # Preparar para wandb\n",
        "        img_np = (img.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
        "        recon_np = (recon.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
        "        test_reconstruction_images.append(wandb.Image(img_np, caption=f\"Anomalía - Original {i+1}\"))\n",
        "        test_reconstruction_images.append(wandb.Image(recon_np, caption=f\"Anomalía - Reconstrucción {i+1}\"))\n",
        "\n",
        "plt.tight_layout()\n",
        "test_recon_path = os.path.join(DRIVE_BASE_PATH, 'test_reconstructions.png')\n",
        "plt.savefig(test_recon_path)\n",
        "plt.show()\n",
        "\n",
        "# Loggear reconstrucciones del set de prueba en wandb\n",
        "wandb_logger.experiment.log({\n",
        "    \"test/reconstructions\": test_reconstruction_images,\n",
        "    \"test/reconstructions_grid\": wandb.Image(test_recon_path)\n",
        "})\n",
        "print(\"Reconstrucciones del set de prueba (16 imágenes: 8 buenas + 8 con anomalías) guardadas en wandb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKzeJ9KcllNf"
      },
      "source": [
        "### 5.2. Distribución de Scores de Anomalía\n",
        "\n",
        "Esta celda visualiza la distribución de los scores de anomalía:\n",
        "- Histograma comparativo entre imágenes normales y con anomalías\n",
        "- Boxplot de scores por clase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RTI1Qy9RfDi"
      },
      "outputs": [],
      "source": [
        "normal_scores = anomaly_scores[test_labels == 0]\n",
        "anomaly_scores_plot = anomaly_scores[test_labels == 1]\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(normal_scores, bins=50, alpha=0.7, label='Normal', color='green')\n",
        "plt.hist(anomaly_scores_plot, bins=50, alpha=0.7, label='Anomalía', color='red')\n",
        "plt.xlabel('Anomaly Score (MSE)')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.title('Distribución de Scores de Anomalía')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.boxplot([normal_scores, anomaly_scores_plot], tick_labels=['Normal', 'Anomalía'])\n",
        "plt.ylabel('Anomaly Score (MSE)')\n",
        "plt.title('Boxplot de Scores')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "\n",
        "score_dist_path = os.path.join(DRIVE_BASE_PATH, 'score_distribution.png')\n",
        "plt.savefig(score_dist_path)\n",
        "plt.show()\n",
        "\n",
        "# Loggear distribución de scores en wandb\n",
        "wandb_logger.experiment.log({\n",
        "    \"eval/score_distribution\": wandb.Image(score_dist_path),\n",
        "    \"eval/normal_scores_mean\": float(normal_scores.mean()),\n",
        "    \"eval/normal_scores_std\": float(normal_scores.std()),\n",
        "    \"eval/anomaly_scores_mean\": float(anomaly_scores_plot.mean()),\n",
        "    \"eval/anomaly_scores_std\": float(anomaly_scores_plot.std())\n",
        "})\n",
        "\n",
        "# Crear histograma interactivo en wandb\n",
        "wandb_logger.experiment.log({\n",
        "    \"eval/scores_histogram\": wandb.Histogram(anomaly_scores, num_bins=50)\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts1VUwK1ltMO"
      },
      "source": [
        "### 5.3. Análisis de Mejores y Peores Reconstrucciones\n",
        "\n",
        "Esta celda identifica las imágenes con menor y mayor error de reconstrucción, lo que ayuda a entender el comportamiento del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNlxgWWIRfDi"
      },
      "outputs": [],
      "source": [
        "\n",
        "sorted_indices = np.argsort(anomaly_scores)\n",
        "\n",
        "print(\"Top 5 imágenes con menor error (más normales):\")\n",
        "for idx in sorted_indices[:5]:\n",
        "    print(f\"  {test_paths[idx]} - Score: {anomaly_scores[idx]:.4f}\")\n",
        "\n",
        "print(\"\\nTop 5 imágenes con mayor error (más anómalas):\")\n",
        "for idx in sorted_indices[-5:]:\n",
        "    print(f\"  {test_paths[idx]} - Score: {anomaly_scores[idx]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnLR2PTBOl9b"
      },
      "source": [
        "### 5.4. Histogramas de Error de Reconstrucción por Subclase de Defecto\n",
        "\n",
        "Esta celda genera histogramas del error de reconstrucción entre clases normales y defectuosas para cada subclase de defecto del set de pruebas, según los requisitos del enunciado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7oLmTHDOl9b"
      },
      "outputs": [],
      "source": [
        "# Función para obtener la subclase de defecto de una ruta\n",
        "def get_defect_subclass(path, category_path):\n",
        "    \"\"\"Extrae la subclase de defecto de la ruta de la imagen\"\"\"\n",
        "    test_path = os.path.join(category_path, 'test')\n",
        "    if 'good' in path:\n",
        "        return 'good'\n",
        "    # Extraer el nombre de la subclase (directorio después de 'test/')\n",
        "    relative_path = os.path.relpath(path, test_path)\n",
        "    subclass = os.path.dirname(relative_path)\n",
        "    return subclass if subclass else 'unknown'\n",
        "\n",
        "# Obtener subclases de defecto para todas las imágenes de prueba\n",
        "defect_subclasses = [get_defect_subclass(path, category_path) for path in test_paths_all]\n",
        "\n",
        "# Crear diccionario con scores por subclase\n",
        "scores_by_subclass = {}\n",
        "for subclass, score in zip(defect_subclasses, anomaly_scores):\n",
        "    if subclass not in scores_by_subclass:\n",
        "        scores_by_subclass[subclass] = []\n",
        "    scores_by_subclass[subclass].append(score)\n",
        "\n",
        "# Crear histogramas para cada subclase\n",
        "n_subclasses = len(scores_by_subclass)\n",
        "n_cols = 3\n",
        "n_rows = (n_subclasses + n_cols - 1) // n_cols\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6 * n_rows))\n",
        "axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
        "\n",
        "for idx, (subclass, scores) in enumerate(sorted(scores_by_subclass.items())):\n",
        "    ax = axes[idx]\n",
        "\n",
        "    # Separar scores de 'good' y defectos\n",
        "    if subclass == 'good':\n",
        "        ax.hist(scores, bins=30, alpha=0.7, label='Normal (good)', color='green', edgecolor='black')\n",
        "        ax.set_title(f'Histograma de Error - {subclass.capitalize()}\\n(n={len(scores)} imágenes)')\n",
        "    else:\n",
        "        # Comparar con 'good' si existe\n",
        "        if 'good' in scores_by_subclass:\n",
        "            good_scores = scores_by_subclass['good']\n",
        "            ax.hist(good_scores, bins=30, alpha=0.5, label='Normal (good)', color='green', edgecolor='black')\n",
        "            ax.hist(scores, bins=30, alpha=0.7, label=f'Defecto ({subclass})', color='red', edgecolor='black')\n",
        "            ax.set_title(f'Histograma de Error - {subclass}\\n(n={len(scores)} imágenes)')\n",
        "        else:\n",
        "            ax.hist(scores, bins=30, alpha=0.7, label=f'Defecto ({subclass})', color='red', edgecolor='black')\n",
        "            ax.set_title(f'Histograma de Error - {subclass}\\n(n={len(scores)} imágenes)')\n",
        "\n",
        "    ax.set_xlabel('Error de Reconstrucción (MSE)')\n",
        "    ax.set_ylabel('Frecuencia')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Ocultar ejes no utilizados\n",
        "for idx in range(len(scores_by_subclass), len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "histograms_path = os.path.join(DRIVE_BASE_PATH, 'histogramas_por_subclase.png')\n",
        "plt.savefig(histograms_path, dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Loggear histogramas en wandb\n",
        "try:\n",
        "    if 'wandb_logger' in globals() and wandb_logger is not None:\n",
        "        wandb_logger.experiment.log({\n",
        "            \"eval/histograms_by_subclass\": wandb.Image(histograms_path)\n",
        "        })\n",
        "        print(\"Histogramas por subclase guardados en wandb\")\n",
        "    else:\n",
        "        print(f\"Histogramas guardados en: {histograms_path}\")\n",
        "        print(\"Nota: wandb_logger no está definido. Ejecuta primero la celda de entrenamiento para habilitar el logging.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al loggear en wandb: {e}\")\n",
        "    print(f\"Histogramas guardados en: {histograms_path}\")\n",
        "\n",
        "# Mostrar estadísticas por subclase\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ESTADÍSTICAS DE ERROR POR SUBCLASE\")\n",
        "print(\"=\"*80)\n",
        "for subclass in sorted(scores_by_subclass.keys()):\n",
        "    scores = scores_by_subclass[subclass]\n",
        "    print(f\"\\n{subclass}:\")\n",
        "    print(f\"  Número de imágenes: {len(scores)}\")\n",
        "    print(f\"  Error medio: {np.mean(scores):.4f}\")\n",
        "    print(f\"  Error mediano: {np.median(scores):.4f}\")\n",
        "    print(f\"  Error std: {np.std(scores):.4f}\")\n",
        "    print(f\"  Error mínimo: {np.min(scores):.4f}\")\n",
        "    print(f\"  Error máximo: {np.max(scores):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwDdYpjcOl9b"
      },
      "source": [
        "### 5.5. Comparación de Funciones de Pérdida (Train/Val Loss)\n",
        "\n",
        "Esta celda compara las funciones de pérdida mostrando las gráficas de pérdida de entrenamiento y validación entre las diferentes configuraciones, según los requisitos del enunciado.\n",
        "\n",
        "**Nota**: Esta visualización requiere que se hayan ejecutado múltiples experimentos (celda 2.1) o que se hayan guardado los resultados de entrenamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jnv3yp3Ol9b"
      },
      "outputs": [],
      "source": [
        "# Comparación de funciones de pérdida\n",
        "# Si se ejecutó la celda de experimentos automatizados, usar esos resultados\n",
        "# Si no, usar los resultados del experimento actual\n",
        "\n",
        "if 'experiment_results' in globals() and len(experiment_results) > 0:\n",
        "    # Usar resultados de experimentos automatizados\n",
        "    print(\"Usando resultados de experimentos automatizados...\")\n",
        "\n",
        "    # Crear gráficas de comparación de pérdidas train/val por arquitectura\n",
        "    architectures = results_df['architecture'].unique()\n",
        "\n",
        "    for arch in architectures:\n",
        "        arch_data = results_df[results_df['architecture'] == arch]\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "        # Gráfica de pérdidas de entrenamiento\n",
        "        ax1 = axes[0]\n",
        "        for loss_func in LOSS_FUNCTIONS:\n",
        "            exp_data = arch_data[arch_data['loss_function'] == loss_func]\n",
        "            if len(exp_data) > 0 and exp_data.iloc[0]['train_losses']:\n",
        "                train_losses = exp_data.iloc[0]['train_losses']\n",
        "                epochs = range(1, len(train_losses) + 1)\n",
        "                ax1.plot(epochs, train_losses, label=loss_func, linewidth=2, marker='o', markersize=4)\n",
        "\n",
        "        ax1.set_xlabel('Época')\n",
        "        ax1.set_ylabel('Pérdida de Entrenamiento')\n",
        "        ax1.set_title(f'Comparación de Pérdida de Entrenamiento - {arch}')\n",
        "        ax1.legend(title='Función de Pérdida')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Gráfica de pérdidas de validación\n",
        "        ax2 = axes[1]\n",
        "        for loss_func in LOSS_FUNCTIONS:\n",
        "            exp_data = arch_data[arch_data['loss_function'] == loss_func]\n",
        "            if len(exp_data) > 0 and exp_data.iloc[0]['val_losses']:\n",
        "                val_losses = exp_data.iloc[0]['val_losses']\n",
        "                # Filtrar valores None\n",
        "                val_losses_clean = [v for v in val_losses if v is not None]\n",
        "                if val_losses_clean:\n",
        "                    epochs = range(1, len(val_losses_clean) + 1)\n",
        "                    ax2.plot(epochs, val_losses_clean, label=loss_func, linewidth=2, marker='s', markersize=4)\n",
        "\n",
        "        ax2.set_xlabel('Época')\n",
        "        ax2.set_ylabel('Pérdida de Validación')\n",
        "        ax2.set_title(f'Comparación de Pérdida de Validación - {arch}')\n",
        "        ax2.legend(title='Función de Pérdida')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        comparison_loss_path = os.path.join(DRIVE_BASE_PATH, f'comparacion_loss_{arch}.png')\n",
        "        plt.savefig(comparison_loss_path, dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        # Loggear en wandb\n",
        "        try:\n",
        "            if 'wandb_logger' in globals() and wandb_logger is not None:\n",
        "                wandb_logger.experiment.log({\n",
        "                    f\"comparison/loss_comparison_{arch}\": wandb.Image(comparison_loss_path)\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Error al loggear en wandb: {e}\")\n",
        "\n",
        "    print(\"\\n✓ Comparación de funciones de pérdida completada\")\n",
        "\n",
        "else:\n",
        "    # Si no hay experimentos automatizados, mostrar solo el experimento actual\n",
        "    print(\"No se encontraron resultados de experimentos automatizados.\")\n",
        "    print(\"Para ver la comparación completa, ejecuta la celda 2.1 (Ejecución Automatizada de Experimentos).\")\n",
        "    print(\"\\nMostrando solo el experimento actual:\")\n",
        "\n",
        "    if 'train_losses' in globals() and train_losses:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "        # Gráfica de pérdida de entrenamiento\n",
        "        epochs = range(1, len(train_losses) + 1)\n",
        "        axes[0].plot(epochs, train_losses, label=cfg.trainer.loss_function, linewidth=2, marker='o', markersize=4)\n",
        "        axes[0].set_xlabel('Época')\n",
        "        axes[0].set_ylabel('Pérdida de Entrenamiento')\n",
        "        axes[0].set_title(f'Pérdida de Entrenamiento - {cfg.model.architecture}')\n",
        "        axes[0].legend(title='Función de Pérdida')\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Gráfica de pérdida de validación (si está disponible)\n",
        "        if 'val_losses' in globals() and val_losses:\n",
        "            val_losses_clean = [v for v in val_losses if v is not None]\n",
        "            if val_losses_clean:\n",
        "                epochs_val = range(1, len(val_losses_clean) + 1)\n",
        "                axes[1].plot(epochs_val, val_losses_clean, label=cfg.trainer.loss_function, linewidth=2, marker='s', markersize=4)\n",
        "                axes[1].set_xlabel('Época')\n",
        "                axes[1].set_ylabel('Pérdida de Validación')\n",
        "                axes[1].set_title(f'Pérdida de Validación - {cfg.model.architecture}')\n",
        "                axes[1].legend(title='Función de Pérdida')\n",
        "                axes[1].grid(True, alpha=0.3)\n",
        "        else:\n",
        "            axes[1].text(0.5, 0.5, 'Pérdidas de validación no disponibles',\n",
        "                        ha='center', va='center', transform=axes[1].transAxes)\n",
        "            axes[1].set_title(f'Pérdida de Validación - {cfg.model.architecture}')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        current_loss_path = os.path.join(DRIVE_BASE_PATH, f'loss_current_{cfg.model.architecture}_{cfg.trainer.loss_function}.png')\n",
        "        plt.savefig(current_loss_path, dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        # Loggear en wandb\n",
        "        try:\n",
        "            if 'wandb_logger' in globals() and wandb_logger is not None:\n",
        "                wandb_logger.experiment.log({\n",
        "                    f\"train/loss_comparison_current\": wandb.Image(current_loss_path)\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Error al loggear en wandb: {e}\")\n",
        "    else:\n",
        "        print(\"No hay datos de pérdidas disponibles para visualizar.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZXwe2LSRfDi"
      },
      "source": [
        "## 6. Guardar Modelo\n",
        "\n",
        "Esta sección guarda el modelo entrenado y finaliza la sesión de WandB.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC-aUmRgl3sh"
      },
      "source": [
        "### 6.1. Guardado del Modelo y Finalización\n",
        "\n",
        "Esta celda:\n",
        "- Guarda el estado del modelo entrenado (pesos, optimizador, métricas)\n",
        "- Guarda el modelo en WandB para acceso posterior\n",
        "- Finaliza la sesión de WandB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kH3w1fURfDi"
      },
      "outputs": [],
      "source": [
        "model_save_path = os.path.join(DRIVE_BASE_PATH, 'modelo_autoencoder.pth')\n",
        "torch.save({\n",
        "    'model_state_dict': lightning_model.model.state_dict(),\n",
        "    'optimizer_state_dict': lightning_model.optimizers().state_dict(),\n",
        "    'train_losses': train_losses,\n",
        "    'auc_score': auc_score,\n",
        "    'ap_score': ap_score\n",
        "}, model_save_path)\n",
        "print(f'Modelo guardado en: {model_save_path}')\n",
        "\n",
        "# Guardar modelo en wandb\n",
        "try:\n",
        "    if 'wandb_logger' in globals() and wandb_logger is not None:\n",
        "        wandb_logger.experiment.save(model_save_path)\n",
        "        print(\"✓ Modelo guardado en WandB\")\n",
        "    else:\n",
        "        print(\"ℹ️ wandb_logger no está disponible. El modelo se guardó localmente.\")\n",
        "except Exception as e:\n",
        "    print(f\"ℹ️ Error al guardar modelo en WandB: {e}\")\n",
        "    print(\"   El modelo se guardó localmente.\")\n",
        "\n",
        "# NOTA: No finalizamos WandB aquí para permitir que las celdas de visualización posteriores puedan loggear\n",
        "# WandB se finalizará al final del notebook (ver celda final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRXYbGMNRfDi"
      },
      "source": [
        "## Notas Finales\n",
        "\n",
        "- El modelo ha sido entrenado solo con imágenes normales (good)\n",
        "- Las anomalías se detectan mediante el error de reconstrucción\n",
        "- Puedes ajustar el umbral de detección según tus necesidades\n",
        "- Para cargar el modelo guardado, usa: `torch.load(model_save_path)`\n",
        "\n",
        "### Weights & Biases\n",
        "\n",
        "- Todas las métricas de entrenamiento y evaluación se registran automáticamente en wandb\n",
        "- Revisa tus resultados en: https://wandb.ai\n",
        "- El proyecto se llama: `tarea05-anomaly-detection`\n",
        "- Cada run se identifica por la categoría utilizada\n",
        "\n",
        "### Métricas Registradas\n",
        "\n",
        "- **Entrenamiento**: pérdida por batch, pérdida por época, learning rate\n",
        "- **Evaluación**: AUC-ROC, Average Precision, estadísticas de scores\n",
        "- **Visualizaciones**: curva ROC, reconstrucciones, distribución de scores\n",
        "- **Modelo**: se guarda automáticamente en wandb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2. Finalización de WandB\n",
        "\n",
        "Esta celda finaliza la sesión de WandB después de que todas las visualizaciones y métricas hayan sido registradas.\n",
        "\n",
        "**Nota**: Esta celda debe ejecutarse al final de todo el proceso para cerrar correctamente la sesión de WandB.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finalizar WandB run (solo si está activo)\n",
        "try:\n",
        "    # Intentar finalizar usando wandb_logger\n",
        "    if 'wandb_logger' in globals() and wandb_logger is not None:\n",
        "        try:\n",
        "            wandb_logger.experiment.finish()\n",
        "            print(\"✓ WandB run finalizado correctamente\")\n",
        "        except Exception as e:\n",
        "            if \"finished\" in str(e).lower() or \"is finished\" in str(e).lower():\n",
        "                print(\"ℹ️ El run de WandB ya estaba finalizado\")\n",
        "            else:\n",
        "                print(f\"⚠️ Error al finalizar wandb_logger: {e}\")\n",
        "    \n",
        "    # Intentar finalizar usando wandb directamente (para experimentos automatizados)\n",
        "    if 'wandb' in globals() and wandb.run is not None:\n",
        "        try:\n",
        "            if not wandb.run.settings._disable:\n",
        "                wandb.finish()\n",
        "                print(\"✓ WandB run finalizado correctamente\")\n",
        "        except Exception as e:\n",
        "            if \"finished\" in str(e).lower() or \"is finished\" in str(e).lower():\n",
        "                print(\"ℹ️ El run de WandB ya estaba finalizado\")\n",
        "            else:\n",
        "                print(f\"⚠️ Error al finalizar wandb: {e}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PROCESO COMPLETADO\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"Revisa tus métricas y visualizaciones en: https://wandb.ai\")\n",
        "    print(f\"Proyecto: tarea05-anomaly-detection\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Error al finalizar WandB: {e}\")\n",
        "    print(\"   Puedes revisar tus métricas en: https://wandb.ai\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
